{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869c9052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1812, 51)\n",
      "Test set shape: (453, 45)\n",
      "'W' column in train dataset: True\n",
      "'W' column in test dataset: False\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Build robust path to data folder (notebooks and data are siblings)\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "SUB_DIR = Path.cwd().parent / 'submissions'\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)  # This is for final predictions (no 'W' column)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"'W' column in train dataset: {'W' in train_df.columns}\")\n",
    "print(f\"'W' column in test dataset: {'W' in test_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6ffd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform analysis for outlier detection and perform outlier handling\n",
    "\n",
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "\n",
    "# print(\"COMPREHENSIVE OUTLIER ANALYSIS AND HANDLING\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # Get numerical columns but exclude categorical/engineered features that shouldn't be treated as numerical\n",
    "# numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# # Exclude target, ID, and categorical/engineered columns\n",
    "# exclude_cols = ['W', 'ID', 'yearID', 'year_label', 'decade_label', 'win_bins']\n",
    "# numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "# print(f\"Excluding from outlier analysis: {exclude_cols}\")\n",
    "# print(f\"Remaining numerical features for outlier analysis: {len(numerical_cols)}\")\n",
    "\n",
    "# # 1. Identify outliers using multiple methods\n",
    "# print(f\"\\n1. OUTLIER DETECTION ON {len(numerical_cols)} FEATURES\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# outlier_summary = {}\n",
    "# for col in numerical_cols:\n",
    "#     if col in train_df.columns and train_df[col].std() > 0:\n",
    "#         # IQR method\n",
    "#         Q1 = train_df[col].quantile(0.25)\n",
    "#         Q3 = train_df[col].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - 1.5 * IQR\n",
    "#         upper_bound = Q3 + 1.5 * IQR\n",
    "#         iqr_outliers = ((train_df[col] < lower_bound) | (train_df[col] > upper_bound)).sum()\n",
    "        \n",
    "#         # Z-score method  \n",
    "#         z_scores = np.abs(stats.zscore(train_df[col].dropna()))\n",
    "#         z_outliers = (z_scores > 3).sum()\n",
    "        \n",
    "#         outlier_summary[col] = {\n",
    "#             'iqr_outliers': iqr_outliers,\n",
    "#             'z_outliers': z_outliers,\n",
    "#             'outlier_rate': max(iqr_outliers, z_outliers) / len(train_df)\n",
    "#         }\n",
    "\n",
    "# # Sort by outlier rate\n",
    "# sorted_outliers = sorted(outlier_summary.items(), \n",
    "#                         key=lambda x: x[1]['outlier_rate'], reverse=True)\n",
    "\n",
    "# print(\"Top 10 features with highest outlier rates:\")\n",
    "# for col, stats_dict in sorted_outliers[:10]:\n",
    "#     rate = stats_dict['outlier_rate']\n",
    "#     iqr_count = stats_dict['iqr_outliers'] \n",
    "#     z_count = stats_dict['z_outliers']\n",
    "#     print(f\"  {col:>15}: {rate*100:.1f}% (IQR: {iqr_count}, Z-score: {z_count})\")\n",
    "\n",
    "# # 2. Select features for outlier handling (>5% outlier rate)\n",
    "# features_to_handle = [col for col, stats_dict in sorted_outliers \n",
    "#                      if stats_dict['outlier_rate'] > 0.05]\n",
    "\n",
    "# print(f\"\\n2. FEATURES SELECTED FOR OUTLIER HANDLING\")\n",
    "# print(\"-\" * 50)\n",
    "# print(f\"Features with >5% outlier rate: {len(features_to_handle)}\")\n",
    "# for feature in features_to_handle:\n",
    "#     rate = outlier_summary[feature]['outlier_rate']\n",
    "#     print(f\"  {feature}: {rate*100:.1f}%\")\n",
    "\n",
    "# if features_to_handle:\n",
    "#     print(f\"\\n3. APPLYING OUTLIER HANDLING\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # Store original for comparison\n",
    "#     train_df_original = train_df.copy()\n",
    "    \n",
    "#     # Handle each problematic feature\n",
    "#     bounds_applied = {}\n",
    "    \n",
    "#     # Filter to only handle columns that exist in both train AND test datasets\n",
    "#     valid_features_to_handle = [col for col in features_to_handle \n",
    "#                                if col in train_df.columns and col in test_df.columns]\n",
    "    \n",
    "#     print(f\"Processing {len(valid_features_to_handle)} features that exist in both train and test sets\")\n",
    "    \n",
    "#     for col in valid_features_to_handle:\n",
    "#         # Calculate bounds on TRAINING data only\n",
    "#         lower_bound = train_df[col].quantile(0.01)\n",
    "#         upper_bound = train_df[col].quantile(0.99)\n",
    "        \n",
    "#         # Store bounds for reference\n",
    "#         bounds_applied[col] = {\n",
    "#             'lower': lower_bound,\n",
    "#             'upper': upper_bound,\n",
    "#             'original_range': f\"{train_df[col].min():.2f} to {train_df[col].max():.2f}\"\n",
    "#         }\n",
    "        \n",
    "#         # Apply SAME bounds to both datasets\n",
    "#         train_df[col] = train_df[col].clip(lower_bound, upper_bound)\n",
    "#         test_df[col] = test_df[col].clip(lower_bound, upper_bound)  # Same bounds!\n",
    "        \n",
    "#         bounds_applied[col]['new_range'] = f\"{train_df[col].min():.2f} to {train_df[col].max():.2f}\"\n",
    "    \n",
    "#     # Report on any features that were excluded from processing\n",
    "#     excluded_features = [col for col in features_to_handle if col not in valid_features_to_handle]\n",
    "#     if excluded_features:\n",
    "#         print(f\"\\n‚ö†Ô∏è  Skipped {len(excluded_features)} features not present in test set:\")\n",
    "#         for col in excluded_features:\n",
    "#             print(f\"    - {col} (train-only feature)\")\n",
    "    \n",
    "#     print(f\"\\n‚úÖ Outlier handling complete!\")\n",
    "#     print(f\"Features processed: {len(bounds_applied)}\")\n",
    "#     print(f\"All rows preserved: Train {train_df.shape[0]}, Test {test_df.shape[0]}\")\n",
    "    \n",
    "#     print(f\"\\n4. IMPACT SUMMARY\")\n",
    "#     print(\"-\" * 50)\n",
    "#     for col, bounds in bounds_applied.items():\n",
    "#         print(f\"{col}:\")\n",
    "#         print(f\"  Bounds applied: {bounds['lower']:.2f} to {bounds['upper']:.2f}\")\n",
    "#         print(f\"  Before: {bounds['original_range']}\")\n",
    "#         print(f\"  After:  {bounds['new_range']}\")\n",
    "#         print()\n",
    "\n",
    "# else:\n",
    "#     print(f\"\\n‚úÖ No features require outlier handling (all <5% outlier rate)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d476af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created derived features: R_per_game, RA_per_game\n",
      "Train - R_per_game range: 2.409 to 6.884\n",
      "Train - RA_per_game range: 2.458 to 7.686\n",
      "Test - R_per_game range: 2.783 to 6.896\n",
      "Test - RA_per_game range: 2.867 to 6.865\n",
      "\n",
      "Created derived feature: Expected_Wins\n",
      "Train - Expected_Wins range: 35.860 to 119.963\n",
      "Test - Expected_Wins range: 40.352 to 107.111\n",
      "\n",
      "Created derived feature: Times_On_Base\n",
      "Train - Times_On_Base range: 1367.000 to 2415.000\n",
      "Test - Times_On_Base range: 1453.000 to 2327.000\n",
      "\n",
      "Created derived feature: BB_Rate\n",
      "Train - BB_Rate range: 0.051 to 0.136\n",
      "Test - BB_Rate range: 0.052 to 0.123\n",
      "\n",
      "Created derived feature: HR_Rate\n",
      "Train - HR_Rate range: 0.001 to 0.047\n",
      "Test - HR_Rate range: 0.001 to 0.045\n",
      "\n",
      "Created derived feature: OBP\n",
      "Train - OBP range: 0.262 to 0.382\n",
      "Test - OBP range: 0.267 to 0.382\n",
      "\n",
      "Created derived feature: SLG\n",
      "Train - SLG range: 0.274 to 0.491\n",
      "Test - SLG range: 0.261 to 0.488\n",
      "\n",
      "Created derived feature: OPS\n",
      "Train - OPS range: 0.539 to 0.870\n",
      "Test - OPS range: 0.530 to 0.870\n",
      "\n",
      "Created derived feature: Times_On_Base_Allowed\n",
      "Train - Times_On_Base_Allowed range: 1441.000 to 2536.000\n",
      "Test - Times_On_Base_Allowed range: 1454.000 to 2421.000\n",
      "\n",
      "Created derived feature: WHIP\n",
      "Train - WHIP range: 1.025 to 1.848\n",
      "Test - WHIP range: 1.028 to 1.776\n",
      "\n",
      "Created derived feature: K_per_9\n",
      "Train - K_per_9 range: 2.102 to 9.353\n",
      "Test - K_per_9 range: 2.064 to 8.704\n",
      "\n",
      "Created derived feature: HR_per_9\n",
      "Train - HR_per_9 range: 0.032 to 1.610\n",
      "Test - HR_per_9 range: 0.040 to 1.429\n",
      "\n",
      "Created derived feature: REI\n",
      "Train - REI range: 1.475 to 2.545\n",
      "Test - REI range: 1.554 to 2.402\n",
      "\n",
      "Created derived feature: PEI\n",
      "Train - PEI range: 0.975 to 28.452\n",
      "Test - PEI range: 1.215 to 27.720\n",
      "\n",
      "Created derived feature: Era_Adjusted_OBP\n",
      "Train - Era_Adjusted_OBP range: 0.246 to 0.434\n",
      "Test - Era_Adjusted_OBP range: 0.254 to 0.405\n",
      "\n",
      "Created derived feature: Era_Adjusted_SLG\n",
      "Train - Era_Adjusted_SLG range: 0.289 to 0.501\n",
      "Test - Era_Adjusted_SLG range: 0.298 to 0.482\n",
      "\n",
      "Created derived feature: Era_Adjusted_OPS\n",
      "Train - Era_Adjusted_OPS range: 0.536 to 0.910\n",
      "Test - Era_Adjusted_OPS range: 0.564 to 0.850\n",
      "\n",
      "Created derived feature: Era_Adjusted_WHIP\n",
      "Train - Era_Adjusted_WHIP range: 1.084 to 1.882\n",
      "Test - Era_Adjusted_WHIP range: 1.127 to 1.707\n",
      "\n",
      "Created derived feature: Era_Adjusted_K_per_9\n",
      "Train - Era_Adjusted_K_per_9 range: 1.847 to 9.626\n",
      "Test - Era_Adjusted_K_per_9 range: 1.795 to 8.963\n",
      "\n",
      "Created derived feature: Era_Adjusted_HR_per_9\n",
      "Train - Era_Adjusted_HR_per_9 range: 0.040 to 1.584\n",
      "Test - Era_Adjusted_HR_per_9 range: 0.043 to 1.347\n",
      "\n",
      "Created derived feature: Era_Adjusted_BB_Rate\n",
      "Train - Era_Adjusted_BB_Rate range: 0.049 to 0.136\n",
      "Test - Era_Adjusted_BB_Rate range: 0.046 to 0.133\n",
      "\n",
      "Created derived feature: Era_Adjusted_HR_Rate\n",
      "Train - Era_Adjusted_HR_Rate range: 0.001 to 0.047\n",
      "Test - Era_Adjusted_HR_Rate range: 0.001 to 0.043\n"
     ]
    }
   ],
   "source": [
    "# Create derived features for both train and test sets\n",
    "\n",
    "# R_per_game: Runs per game\n",
    "# RA_per_game: Runs allowed per game\n",
    "train_df['R_per_game'] = train_df['R'] / train_df['G']\n",
    "train_df['RA_per_game'] = train_df['RA'] / train_df['G']\n",
    "test_df['R_per_game'] = test_df['R'] / test_df['G']\n",
    "test_df['RA_per_game'] = test_df['RA'] / test_df['G']\n",
    "\n",
    "print(f\"\\nCreated derived features: R_per_game, RA_per_game\")\n",
    "print(f\"Train - R_per_game range: {train_df['R_per_game'].min():.3f} to {train_df['R_per_game'].max():.3f}\")\n",
    "print(f\"Train - RA_per_game range: {train_df['RA_per_game'].min():.3f} to {train_df['RA_per_game'].max():.3f}\")\n",
    "print(f\"Test - R_per_game range: {test_df['R_per_game'].min():.3f} to {test_df['R_per_game'].max():.3f}\")\n",
    "print(f\"Test - RA_per_game range: {test_df['RA_per_game'].min():.3f} to {test_df['RA_per_game'].max():.3f}\")\n",
    "\n",
    "# Expected Wins of Season = G √ó (R¬≤) / (R¬≤ + RA¬≤)\n",
    "train_df['Expected_Wins'] = train_df['G'] * (train_df['R_per_game'] ** 2) / ((train_df['R_per_game'] ** 2) + (train_df['RA_per_game'] ** 2))\n",
    "test_df['Expected_Wins'] = test_df['G'] * (test_df['R_per_game'] ** 2) / ((test_df['R_per_game'] ** 2) + (test_df['RA_per_game'] ** 2))\n",
    "# train_df['Expected_Wins'] = train_df['G'] * (train_df['R'] ** 2) / ((train_df['R'] ** 2) + (train_df['RA'] ** 2))\n",
    "# test_df['Expected_Wins'] = test_df['G'] * (test_df['R'] ** 2) / ((test_df['R'] ** 2) + (test_df['RA'] ** 2))\n",
    "print(f\"\\nCreated derived feature: Expected_Wins\")   \n",
    "print(f\"Train - Expected_Wins range: {train_df['Expected_Wins'].min():.3f} to {train_df['Expected_Wins'].max():.3f}\")\n",
    "print(f\"Test - Expected_Wins range: {test_df['Expected_Wins'].min():.3f} to {test_df['Expected_Wins'].max():.3f}\")\n",
    "\n",
    "# Times getting on base\n",
    "train_df['Times_On_Base'] = train_df['H'] + train_df['BB']\n",
    "test_df['Times_On_Base'] = test_df['H'] + test_df['BB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: Times_On_Base\")\n",
    "print(f\"Train - Times_On_Base range: {train_df['Times_On_Base'].min():.3f} to {train_df['Times_On_Base'].max():.3f}\")\n",
    "print(f\"Test - Times_On_Base range: {test_df['Times_On_Base'].min():.3f} to {test_df['Times_On_Base'].max():.3f}\")\n",
    "\n",
    "# BB Rate (Walk Percentage) - BB / AB + BB\n",
    "train_df['BB_Rate'] = train_df['BB'] / (train_df['AB'] + train_df['BB'])\n",
    "test_df['BB_Rate'] = test_df['BB'] / (test_df['AB'] + test_df['BB'])\n",
    "\n",
    "print(f\"\\nCreated derived feature: BB_Rate\")\n",
    "print(f\"Train - BB_Rate range: {train_df['BB_Rate'].min():.3f} to {train_df['BB_Rate'].max():.3f}\") \n",
    "print(f\"Test - BB_Rate range: {test_df['BB_Rate'].min():.3f} to {test_df['BB_Rate'].max():.3f}\")\n",
    "\n",
    "# Home Run Rate - HR / AB\n",
    "train_df['HR_Rate'] = train_df['HR'] / train_df['AB']\n",
    "test_df['HR_Rate'] = test_df['HR'] / test_df['AB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: HR_Rate\")\n",
    "print(f\"Train - HR_Rate range: {train_df['HR_Rate'].min():.3f} to {train_df['HR_Rate'].max():.3f}\")\n",
    "print(f\"Test - HR_Rate range: {test_df['HR_Rate'].min():.3f} to {test_df['HR_Rate'].max():.3f}\")\n",
    "\n",
    "# On-Base Percentage (OBP) - (H + BB) / (AB + BB)\n",
    "train_df['OBP'] = (train_df['H'] + train_df['BB']) / (train_df['AB'] + train_df['BB'])\n",
    "test_df['OBP'] = (test_df['H'] + test_df['BB']) / (test_df['AB'] + test_df['BB'])\n",
    "\n",
    "print(f\"\\nCreated derived feature: OBP\")\n",
    "print(f\"Train - OBP range: {train_df['OBP'].min():.3f} to {train_df['OBP'].max():.3f}\") \n",
    "print(f\"Test - OBP range: {test_df['OBP'].min():.3f} to {test_df['OBP'].max():.3f}\")\n",
    "\n",
    "# Slugging Percentage (SLG)\n",
    "# Singles = H - (2B + 3B + HR)\n",
    "# Total Bases = Singles + (2 * 2B) + (3 * 3B) + (4 * HR)\n",
    "# SLG = Total Bases / AB\n",
    "Singles_train = train_df['H'] - (train_df['2B'] + train_df['3B'] + train_df['HR'])\n",
    "Total_Bases_train = Singles_train + (2 * train_df['2B']) + (3 * train_df['3B']) + (4 * train_df['HR'])\n",
    "train_df['SLG'] = Total_Bases_train / train_df['AB']  \n",
    "\n",
    "Singles_test = test_df['H'] - (test_df['2B'] + test_df['3B'] + test_df['HR'])\n",
    "Total_Bases_test = Singles_test + (2 * test_df['2B']) + (3 * test_df['3B']) + (4 * test_df['HR'])\n",
    "test_df['SLG'] = Total_Bases_test / test_df['AB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: SLG\")\n",
    "print(f\"Train - SLG range: {train_df['SLG'].min():.3f} to {train_df['SLG'].max():.3f}\") \n",
    "print(f\"Test - SLG range: {test_df['SLG'].min():.3f} to {test_df['SLG'].max():.3f}\")    \n",
    "\n",
    "# Combined On-Base Plus Slugging (OPS) - OBP + SLG\n",
    "train_df['OPS'] = train_df['OBP'] + train_df['SLG']\n",
    "test_df['OPS'] = test_df['OBP'] + test_df['SLG']\n",
    "\n",
    "print(f\"\\nCreated derived feature: OPS\")\n",
    "print(f\"Train - OPS range: {train_df['OPS'].min():.3f} to {train_df['OPS'].max():.3f}\") \n",
    "print(f\"Test - OPS range: {test_df['OPS'].min():.3f} to {test_df['OPS'].max():.3f}\")\n",
    "\n",
    "# Time on Base Allowed - HA + BBA\n",
    "train_df['Times_On_Base_Allowed'] = train_df['HA'] + train_df['BBA']\n",
    "test_df['Times_On_Base_Allowed'] = test_df['HA'] + test_df['BBA']\n",
    "\n",
    "print(f\"\\nCreated derived feature: Times_On_Base_Allowed\")\n",
    "print(f\"Train - Times_On_Base_Allowed range: {train_df['Times_On_Base_Allowed'].min():.3f} to {train_df['Times_On_Base_Allowed'].max():.3f}\")\n",
    "print(f\"Test - Times_On_Base_Allowed range: {test_df['Times_On_Base_Allowed'].min():.3f} to {test_df['Times_On_Base_Allowed'].max():.3f}\")\n",
    "\n",
    "# WHIP (Walks plus Hits per Inning Pitched)\n",
    "# Inings Pitched = IPouts / 3\n",
    "# Times_On_Base_Per_Inning = Times_On_Base_Allowed / Inings_Pitched\n",
    "train_df['Innings_Pitched'] = train_df['IPouts'] / 3\n",
    "train_df['WHIP'] = train_df['Times_On_Base_Allowed'] / train_df['Innings_Pitched']\n",
    "test_df['Innings_Pitched'] = test_df['IPouts'] / 3\n",
    "test_df['WHIP'] = test_df['Times_On_Base_Allowed'] / test_df['Innings_Pitched']\n",
    "\n",
    "print(f\"\\nCreated derived feature: WHIP\")\n",
    "print(f\"Train - WHIP range: {train_df['WHIP'].min():.3f} to {train_df['WHIP'].max():.3f}\")\n",
    "print(f\"Test - WHIP range: {test_df['WHIP'].min():.3f} to {test_df['WHIP'].max():.3f}\")\n",
    "\n",
    "# K/9 (Strikeouts per 9 Innings) - SOA / Innings_Pitched * 9\n",
    "train_df['K_per_9'] = (train_df['SOA'] / train_df['Innings_Pitched']) * 9\n",
    "test_df['K_per_9'] = (test_df['SOA'] / test_df['Innings_Pitched']) * 9  \n",
    "\n",
    "print(f\"\\nCreated derived feature: K_per_9\")\n",
    "print(f\"Train - K_per_9 range: {train_df['K_per_9'].min():.3f} to {train_df['K_per_9'].max():.3f}\")\n",
    "print(f\"Test - K_per_9 range: {test_df['K_per_9'].min():.3f} to {test_df['K_per_9'].max():.3f}\")\n",
    "\n",
    "# HR/9 (Home Runs Allowed per 9 Innings) - HRA / Innings_Pitched * 9\n",
    "train_df['HR_per_9'] = (train_df['HRA'] / train_df['Innings_Pitched']) * 9\n",
    "test_df['HR_per_9'] = (test_df['HRA'] / test_df['Innings_Pitched']) * 9\n",
    "\n",
    "print(f\"\\nCreated derived feature: HR_per_9\")\n",
    "print(f\"Train - HR_per_9 range: {train_df['HR_per_9'].min():.3f} to {train_df['HR_per_9'].max():.3f}\")\n",
    "print(f\"Test - HR_per_9 range: {test_df['HR_per_9'].min():.3f} to {test_df['HR_per_9'].max():.3f}\")\n",
    "\n",
    "# Run Environment Idex (REI) - (R + RA) / G / mlb_rpg\n",
    "train_df['REI'] = (train_df['R'] + train_df['RA']) / train_df['G'] / train_df['mlb_rpg']\n",
    "test_df['REI'] = (test_df['R'] + test_df['RA']) / test_df['G'] / test_df['mlb_rpg']\n",
    "print(f\"\\nCreated derived feature: REI\")\n",
    "print(f\"Train - REI range: {train_df['REI'].min():.3f} to {train_df['REI'].max():.3f}\")\n",
    "print(f\"Test - REI range: {test_df['REI'].min():.3f} to {test_df['REI'].max():.3f}\")    \n",
    "\n",
    "# Power Environement Index (PEI) -  (HR + HRA) / G / (mlb_rpg * avg_hr_rate)\n",
    "avg_hr_rate = train_df['HR_Rate'].mean()\n",
    "train_df['PEI'] = (train_df['HR'] + train_df['HRA']) / train_df['G'] / (train_df['mlb_rpg'] * avg_hr_rate)\n",
    "test_df['PEI'] = (test_df['HR'] + test_df['HRA']) / test_df['G'] / (test_df['mlb_rpg'] * avg_hr_rate)\n",
    "print(f\"\\nCreated derived feature: PEI\")\n",
    "print(f\"Train - PEI range: {train_df['PEI'].min():.3f} to {train_df['PEI'].max():.3f}\")\n",
    "print(f\"Test - PEI range: {test_df['PEI'].min():.3f} to {test_df['PEI'].max():.3f}\") \n",
    "\n",
    "# Era adjusted OBP, SLG, OPS, WHIP, K_per_9, HR_per_9, BB_Rate, HR_Rate\n",
    "# Historical average runs per game (RPG) for MLB\n",
    "historical_avg_rpg_train = train_df['mlb_rpg'].mean()\n",
    "historical_avg_rpg_test = test_df['mlb_rpg'].mean()\n",
    "# historical_avg_rpg_train = 4.4\n",
    "# historical_avg_rpg_test = 4.4\n",
    "\n",
    "# Era adjusted OBP\n",
    "train_df['Era_Adjusted_OBP'] = train_df['OBP'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_OBP'] = test_df['OBP'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_OBP\")\n",
    "print(f\"Train - Era_Adjusted_OBP range: {train_df['Era_Adjusted_OBP'].min():.3f} to {train_df['Era_Adjusted_OBP'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_OBP range: {test_df['Era_Adjusted_OBP'].min():.3f} to {test_df['Era_Adjusted_OBP'].max():.3f}\") \n",
    "\n",
    "# Era adjusted SLG\n",
    "train_df['Era_Adjusted_SLG'] = train_df['SLG'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_SLG'] = test_df['SLG'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_SLG\")\n",
    "print(f\"Train - Era_Adjusted_SLG range: {train_df['Era_Adjusted_SLG'].min():.3f} to {train_df['Era_Adjusted_SLG'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_SLG range: {test_df['Era_Adjusted_SLG'].min():.3f} to {test_df['Era_Adjusted_SLG'].max():.3f}\") \n",
    "\n",
    "# Era adjusted OPS\n",
    "train_df['Era_Adjusted_OPS'] = train_df['OPS'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_OPS'] = test_df['OPS'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_OPS\")\n",
    "print(f\"Train - Era_Adjusted_OPS range: {train_df['Era_Adjusted_OPS'].min():.3f} to {train_df['Era_Adjusted_OPS'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_OPS range: {test_df['Era_Adjusted_OPS'].min():.3f} to {test_df['Era_Adjusted_OPS'].max():.3f}\")\n",
    "\n",
    "# Era adjusted WHIP\n",
    "train_df['Era_Adjusted_WHIP'] = train_df['WHIP'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_WHIP'] = test_df['WHIP'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_WHIP\")\n",
    "print(f\"Train - Era_Adjusted_WHIP range: {train_df['Era_Adjusted_WHIP'].min():.3f} to {train_df['Era_Adjusted_WHIP'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_WHIP range: {test_df['Era_Adjusted_WHIP'].min():.3f} to {test_df['Era_Adjusted_WHIP'].max():.3f}\")\n",
    "\n",
    "# Era adjusted K_per_9\n",
    "train_df['Era_Adjusted_K_per_9'] = train_df['K_per_9'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_K_per_9'] = test_df['K_per_9'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_K_per_9\")\n",
    "print(f\"Train - Era_Adjusted_K_per_9 range: {train_df['Era_Adjusted_K_per_9'].min():.3f} to {train_df['Era_Adjusted_K_per_9'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_K_per_9 range: {test_df['Era_Adjusted_K_per_9'].min():.3f} to {test_df['Era_Adjusted_K_per_9'].max():.3f}\") \n",
    "\n",
    "# Era adjusted HR_per_9\n",
    "train_df['Era_Adjusted_HR_per_9'] = train_df['HR_per_9'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_HR_per_9'] = test_df['HR_per_9'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_HR_per_9\")\n",
    "print(f\"Train - Era_Adjusted_HR_per_9 range: {train_df['Era_Adjusted_HR_per_9'].min():.3f} to {train_df['Era_Adjusted_HR_per_9'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_HR_per_9 range: {test_df['Era_Adjusted_HR_per_9'].min():.3f} to {test_df['Era_Adjusted_HR_per_9'].max():.3f}\")\n",
    "\n",
    "# Era adjusted BB_Rate\n",
    "train_df['Era_Adjusted_BB_Rate'] = train_df['BB_Rate'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_BB_Rate'] = test_df['BB_Rate'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_BB_Rate\")\n",
    "print(f\"Train - Era_Adjusted_BB_Rate range: {train_df['Era_Adjusted_BB_Rate'].min():.3f} to {train_df['Era_Adjusted_BB_Rate'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_BB_Rate range: {test_df['Era_Adjusted_BB_Rate'].min():.3f} to {test_df['Era_Adjusted_BB_Rate'].max():.3f}\") \n",
    "\n",
    "# Era adjusted HR_Rate\n",
    "train_df['Era_Adjusted_HR_Rate'] = train_df['HR_Rate'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_HR_Rate'] = test_df['HR_Rate'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_HR_Rate\")\n",
    "print(f\"Train - Era_Adjusted_HR_Rate range: {train_df['Era_Adjusted_HR_Rate'].min():.3f} to {train_df['Era_Adjusted_HR_Rate'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_HR_Rate range: {test_df['Era_Adjusted_HR_Rate'].min():.3f} to {test_df['Era_Adjusted_HR_Rate'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71b7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available default features: 65\n",
      "Available features:\n",
      "G\n",
      "R\n",
      "AB\n",
      "H\n",
      "2B\n",
      "3B\n",
      "HR\n",
      "BB\n",
      "SO\n",
      "SB\n",
      "RA\n",
      "ER\n",
      "ERA\n",
      "CG\n",
      "SHO\n",
      "SV\n",
      "IPouts\n",
      "HA\n",
      "HRA\n",
      "BBA\n",
      "SOA\n",
      "E\n",
      "DP\n",
      "FP\n",
      "Expected_Wins\n",
      "Times_On_Base\n",
      "Times_On_Base_Allowed\n",
      "mlb_rpg\n",
      "Era_Adjusted_OBP\n",
      "Era_Adjusted_SLG\n",
      "Era_Adjusted_OPS\n",
      "Era_Adjusted_WHIP\n",
      "Era_Adjusted_K_per_9\n",
      "Era_Adjusted_HR_per_9\n",
      "Era_Adjusted_BB_Rate\n",
      "Era_Adjusted_HR_Rate\n",
      "OBP\n",
      "SLG\n",
      "OPS\n",
      "WHIP\n",
      "K_per_9\n",
      "HR_per_9\n",
      "BB_Rate\n",
      "HR_Rate\n",
      "PEI\n",
      "REI\n",
      "era_1\n",
      "era_2\n",
      "era_3\n",
      "era_4\n",
      "era_5\n",
      "era_6\n",
      "era_7\n",
      "era_8\n",
      "decade_1910\n",
      "decade_1920\n",
      "decade_1930\n",
      "decade_1940\n",
      "decade_1950\n",
      "decade_1960\n",
      "decade_1970\n",
      "decade_1980\n",
      "decade_1990\n",
      "decade_2000\n",
      "decade_2010\n"
     ]
    }
   ],
   "source": [
    "# Select only the default features from DATA_DESCRIPTION.md\n",
    "# default_features = [\n",
    "#     # Basic Statistics\n",
    "#     'G', 'HR', 'SHO', 'SV', 'IPouts', 'FP', 'ERA', 'ER', 'E',\n",
    "\n",
    "#     # Derived Features\n",
    "#     'Expected_Wins', 'Times_On_Base', 'BB_Rate', 'HR_Rate', 'OPS', 'Times_On_Base_Allowed', \n",
    "#     'WHIP', 'K_per_9', 'HR_per_9', 'mlb_rpg',\n",
    "    \n",
    "#     # Era Indicators\n",
    "#     'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8',\n",
    "    \n",
    "#     # Decade Indicators\n",
    "#     'decade_1910', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950',\n",
    "#     'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000', 'decade_2010'\n",
    "# ]\n",
    "\n",
    "# default_features = [\n",
    "#     # Basic Statistics\n",
    "#     'G', 'R', 'AB', 'H', '2B', '3B', 'HR', 'BB', 'SO', 'SB', 'CS', 'HBP', 'SF',\n",
    "#     'RA', 'ER', 'ERA', 'CG', 'SHO', 'SV', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA',\n",
    "#     'E', 'DP', 'FP', 'attendance', 'BPF', 'PPF',\n",
    "    \n",
    "#     # Derived Features\n",
    "#     'Expected_Wins', 'Times_On_Base', 'Times_On_Base_Allowed', 'mlb_rpg',\n",
    "\n",
    "#     # 'Era_Adjusted_OBP', 'Era_Adjusted_SLG', 'Era_Adjusted_OPS', 'Era_Adjusted_WHIP',\n",
    "#     # 'Era_Adjusted_K_per_9', 'Era_Adjusted_HR_per_9', 'Era_Adjusted_BB_Rate', 'Era_Adjusted_HR_Rate',\n",
    "#     'BB_Rate', 'HR_Rate', 'OBP', 'SLG', 'OPS', 'WHIP', 'K_per_9', 'HR_per_9',\n",
    "    \n",
    "#     # 'PEI', 'REI',\n",
    "    \n",
    "#     # # Era Indicators\n",
    "#     # 'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8',\n",
    "    \n",
    "#     # Decade Indicators\n",
    "#     'decade_1910', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950',\n",
    "#     'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000', 'decade_2010'\n",
    "#  ]\n",
    "\n",
    "default_features = [\n",
    "    # Basic Statistics\n",
    "    'G', 'R', 'AB', 'H', '2B', '3B', 'HR', 'BB', 'SO', 'SB', 'CS', 'HBP', 'SF',\n",
    "    'RA', 'ER', 'ERA', 'CG', 'SHO', 'SV', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA',\n",
    "    'E', 'DP', 'FP', 'attendance', 'BPF', 'PPF',\n",
    "    \n",
    "    # Derived Features\n",
    "    'Expected_Wins', 'Times_On_Base', 'Times_On_Base_Allowed', 'mlb_rpg',\n",
    "\n",
    "    'Era_Adjusted_OBP', 'Era_Adjusted_SLG', 'Era_Adjusted_OPS', 'Era_Adjusted_WHIP',\n",
    "    'Era_Adjusted_K_per_9', 'Era_Adjusted_HR_per_9', 'Era_Adjusted_BB_Rate', 'Era_Adjusted_HR_Rate',\n",
    "    \n",
    "    'OBP', 'SLG', 'OPS', 'WHIP', 'K_per_9', 'HR_per_9', 'BB_Rate', 'HR_Rate', \n",
    "    \n",
    "    'PEI', 'REI',\n",
    "    \n",
    "    # Era Indicators\n",
    "    'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8',\n",
    "    \n",
    "    # Decade Indicators\n",
    "    'decade_1910', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950',\n",
    "    'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000', 'decade_2010'\n",
    " ]\n",
    "\n",
    "# Filter features that exist in both training data AND test data\n",
    "available_features = [col for col in default_features \n",
    "                     if col in train_df.columns and col in test_df.columns]\n",
    "print(f\"Number of available default features: {len(available_features)}\")\n",
    "\n",
    "# Print available features in a column\n",
    "print(\"Available features:\")\n",
    "for feature in available_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edd8c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1449, 65)\n",
      "Validation set shape: (363, 65)\n",
      "Final test set shape: (453, 65)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data (split the train.csv for model evaluation)\n",
    "X_full = train_df[available_features]\n",
    "y_full = train_df['W']\n",
    "\n",
    "# Split training data into train/validation sets for model evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare final test data for predictions (this has no target variable)\n",
    "X_test_final = test_df[available_features]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Final test set shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a238e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CORRELATION ANALYSIS\n",
      "==================================================\n",
      "Correlation threshold: 0.95\n",
      "Original features: 65\n",
      "Features to remove: 13\n",
      "\n",
      "Highly correlated features to remove:\n",
      "  ‚Ä¢ ERA (corr=0.959 with RA)\n",
      "  ‚Ä¢ FP (corr=0.996 with E)\n",
      "  ‚Ä¢ Era_Adjusted_K_per_9 (corr=0.953 with SOA)\n",
      "  ‚Ä¢ Era_Adjusted_HR_per_9 (corr=0.981 with HRA)\n",
      "  ‚Ä¢ Era_Adjusted_HR_Rate (corr=0.979 with HR)\n",
      "  ‚Ä¢ OPS (corr=0.969 with SLG)\n",
      "  ‚Ä¢ K_per_9 (corr=0.999 with SOA)\n",
      "  ‚Ä¢ HR_per_9 (corr=0.999 with HRA)\n",
      "  ‚Ä¢ BB_Rate (corr=0.982 with BB)\n",
      "  ‚Ä¢ HR_Rate (corr=0.999 with HR)\n",
      "  ‚Ä¢ PEI (corr=0.959 with Era_Adjusted_HR_Rate)\n",
      "  ‚Ä¢ decade_1910 (corr=1.000 with era_1)\n",
      "  ‚Ä¢ decade_2010 (corr=1.000 with era_8)\n",
      "\n",
      "Features after removal: 52\n",
      "Features removed: 13\n",
      "Dimensionality reduction: 20.0%\n",
      "\n",
      "üìä UPDATED DATASET INFO\n",
      "==================================================\n",
      "X_full shape: (1812, 52)\n",
      "X_test_final shape: (453, 52)\n",
      "Available features updated: 52\n",
      "‚úÖ Feature alignment verified between train and test sets\n",
      "\n",
      "üîÑ Variables updated for downstream compatibility:\n",
      "  ‚Ä¢ X_full: (1812, 52)\n",
      "  ‚Ä¢ X_test_final: (453, 52)\n",
      "  ‚Ä¢ available_features: 52 features\n",
      "\n",
      "üí° To disable correlation removal, simply comment out this entire cell\n"
     ]
    }
   ],
   "source": [
    "# Remove highly correlated features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_correlated_features(X_train, X_test, threshold=0.95, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features from training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature DataFrame\n",
    "    - X_test: Test feature DataFrame  \n",
    "    - threshold: Correlation threshold (default 0.95)\n",
    "    - verbose: Print information about removed features\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_filtered, X_test_filtered: DataFrames with correlated features removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    \n",
    "    # Find pairs of highly correlated features\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find features to remove (those with correlation > threshold)\n",
    "    features_to_remove = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üîç CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Correlation threshold: {threshold}\")\n",
    "        print(f\"Original features: {X_train.shape[1]}\")\n",
    "        print(f\"Features to remove: {len(features_to_remove)}\")\n",
    "        \n",
    "        if features_to_remove:\n",
    "            print(f\"\\nHighly correlated features to remove:\")\n",
    "            for feature in features_to_remove:\n",
    "                # Find what it's correlated with\n",
    "                high_corr = upper_tri[feature].dropna()\n",
    "                high_corr = high_corr[high_corr > threshold]\n",
    "                if len(high_corr) > 0:\n",
    "                    corr_with = high_corr.index[0]\n",
    "                    corr_value = high_corr.iloc[0]\n",
    "                    print(f\"  ‚Ä¢ {feature} (corr={corr_value:.3f} with {corr_with})\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No highly correlated features found above threshold {threshold}\")\n",
    "    \n",
    "    # Remove highly correlated features from both datasets\n",
    "    X_train_filtered = X_train.drop(columns=features_to_remove)\n",
    "    X_test_filtered = X_test.drop(columns=features_to_remove)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFeatures after removal: {X_train_filtered.shape[1]}\")\n",
    "        print(f\"Features removed: {len(features_to_remove)}\")\n",
    "        if len(features_to_remove) > 0:\n",
    "            improvement = len(features_to_remove) / X_train.shape[1] * 100\n",
    "            print(f\"Dimensionality reduction: {improvement:.1f}%\")\n",
    "    \n",
    "    return X_train_filtered, X_test_filtered\n",
    "\n",
    "# Apply correlation removal to our datasets\n",
    "# Store original datasets for backup\n",
    "X_full_original = X_full.copy()\n",
    "X_test_final_original = X_test_final.copy()\n",
    "\n",
    "# Remove correlated features\n",
    "X_full_filtered, X_test_final_filtered = remove_correlated_features(\n",
    "    X_full, X_test_final, \n",
    "    threshold=0.95, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Update the main datasets (so later cells use the filtered versions)\n",
    "X_full = X_full_filtered\n",
    "X_test_final = X_test_final_filtered\n",
    "\n",
    "# Update available_features list to match the filtered features\n",
    "available_features_filtered = list(X_full.columns)\n",
    "\n",
    "print(f\"\\nüìä UPDATED DATASET INFO\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"X_full shape: {X_full.shape}\")\n",
    "print(f\"X_test_final shape: {X_test_final.shape}\")\n",
    "print(f\"Available features updated: {len(available_features_filtered)}\")\n",
    "\n",
    "# Verify both datasets have the same features\n",
    "assert list(X_full.columns) == list(X_test_final.columns), \"Feature mismatch between train and test!\"\n",
    "print(f\"‚úÖ Feature alignment verified between train and test sets\")\n",
    "\n",
    "# Update available_features for downstream compatibility\n",
    "available_features = available_features_filtered\n",
    "\n",
    "print(f\"\\nüîÑ Variables updated for downstream compatibility:\")\n",
    "print(f\"  ‚Ä¢ X_full: {X_full.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test_final: {X_test_final.shape}\")  \n",
    "print(f\"  ‚Ä¢ available_features: {len(available_features)} features\")\n",
    "print(f\"\\nüí° To disable correlation removal, simply comment out this entire cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37fccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA-OPTIMIZED BOOSTING MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (1812, 52)\n",
      "Features being used: ['G', 'R', 'AB', 'H', '2B', '3B', 'HR', 'BB', 'SO', 'SB', 'RA', 'ER', 'CG', 'SHO', 'SV', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA', 'E', 'DP', 'Expected_Wins', 'Times_On_Base', 'Times_On_Base_Allowed', 'mlb_rpg', 'Era_Adjusted_OBP', 'Era_Adjusted_SLG', 'Era_Adjusted_OPS', 'Era_Adjusted_WHIP', 'Era_Adjusted_BB_Rate', 'OBP', 'SLG', 'WHIP', 'REI', 'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950', 'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000']\n",
      "\n",
      "üîç HYPERPARAMETER OPTIMIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Optimizing XGBoost hyperparameters...\n",
      "  Best MAE: 3.0262 (Time: 31.8s)\n",
      "\n",
      "Optimizing CatBoost hyperparameters...\n",
      "  Best MAE: 3.0262 (Time: 31.8s)\n",
      "\n",
      "Optimizing CatBoost hyperparameters...\n",
      "  Best MAE: 3.0056 (Time: 183.1s)\n",
      "\n",
      "üìã OPTIMIZATION SUMMARY\n",
      "--------------------------------------------------\n",
      "XGBoost:\n",
      "  Best CV MAE: 3.0262\n",
      "  Optimization time: 31.8s\n",
      "  Trials completed: 50\n",
      "\n",
      "CatBoost:\n",
      "  Best CV MAE: 3.0056\n",
      "  Optimization time: 183.1s\n",
      "  Trials completed: 50\n",
      "\n",
      "  Best MAE: 3.0056 (Time: 183.1s)\n",
      "\n",
      "üìã OPTIMIZATION SUMMARY\n",
      "--------------------------------------------------\n",
      "XGBoost:\n",
      "  Best CV MAE: 3.0262\n",
      "  Optimization time: 31.8s\n",
      "  Trials completed: 50\n",
      "\n",
      "CatBoost:\n",
      "  Best CV MAE: 3.0056\n",
      "  Optimization time: 183.1s\n",
      "  Trials completed: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import boosting libraries and Optuna\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"OPTUNA-OPTIMIZED BOOSTING MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Features being used: {list(X.columns)}\")\n",
    "\n",
    "# Define objective functions for Optuna hyperparameter optimization\n",
    "def xgboost_objective(trial):\n",
    "    \"\"\"Objective function for XGBoost hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    \n",
    "    # Try GPU first, fallback to CPU if needed\n",
    "    try:\n",
    "        params['device'] = 'cuda'\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\n",
    "    except:\n",
    "        params['device'] = 'cpu'\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    return -scores.mean()  # Optuna minimizes, so negate MAE\n",
    "\n",
    "def catboost_objective(trial):\n",
    "    \"\"\"Objective function for CatBoost hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    # Try GPU first, fallback to CPU if needed\n",
    "    try:\n",
    "        params['task_type'] = 'GPU'\n",
    "        model = CatBoostRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\n",
    "    except:\n",
    "        params['task_type'] = 'CPU'\n",
    "        model = CatBoostRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optimize hyperparameters for each model\n",
    "print(\"\\nüîç HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "optimized_params = {}\n",
    "optimization_results = {}\n",
    "\n",
    "# XGBoost optimization\n",
    "print(\"\\nOptimizing XGBoost hyperparameters...\")\n",
    "start_time = time.time()\n",
    "xgb_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "xgb_study.optimize(xgboost_objective, n_trials=50, show_progress_bar=False)\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "optimized_params['XGBoost'] = xgb_study.best_params\n",
    "optimization_results['XGBoost'] = {\n",
    "    'best_mae': xgb_study.best_value,\n",
    "    'optimization_time': xgb_time,\n",
    "    'n_trials': len(xgb_study.trials)\n",
    "}\n",
    "print(f\"  Best MAE: {xgb_study.best_value:.4f} (Time: {xgb_time:.1f}s)\")\n",
    "\n",
    "# CatBoost optimization\n",
    "print(\"\\nOptimizing CatBoost hyperparameters...\")\n",
    "start_time = time.time()\n",
    "cat_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "cat_study.optimize(catboost_objective, n_trials=50, show_progress_bar=False)\n",
    "cat_time = time.time() - start_time\n",
    "\n",
    "optimized_params['CatBoost'] = cat_study.best_params\n",
    "optimization_results['CatBoost'] = {\n",
    "    'best_mae': cat_study.best_value,\n",
    "    'optimization_time': cat_time,\n",
    "    'n_trials': len(cat_study.trials)\n",
    "}\n",
    "print(f\"  Best MAE: {cat_study.best_value:.4f} (Time: {cat_time:.1f}s)\")\n",
    "\n",
    "print(f\"\\nüìã OPTIMIZATION SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, result in optimization_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Best CV MAE: {result['best_mae']:.4f}\")\n",
    "    print(f\"  Optimization time: {result['optimization_time']:.1f}s\")\n",
    "    print(f\"  Trials completed: {result['n_trials']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bf2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è BUILDING OPTIMIZED MODELS\n",
      "--------------------------------------------------\n",
      "\n",
      "Building optimized XGBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0306\n",
      "\n",
      "Building optimized CatBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0306\n",
      "\n",
      "Building optimized CatBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0077\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)   GPU       \n",
      "------------------------------------------------------------------------------------------\n",
      "CatBoost               0.9149    3.0077     0.0350 ‚úì        4.3    üöÄ\n",
      "XGBoost                0.9146    3.0306     0.0324 ‚úì        1.5    üöÄ\n",
      "\n",
      "üéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Selected Model: CatBoost\n",
      "Selection Reason: Selected for low overfitting (0.0350) with minimal MAE penalty (0.0000)\n",
      "MAE: 3.0077, Overfitting: 0.0350\n",
      "\n",
      "üèÜ BEST OPTIMIZED MODEL: CatBoost\n",
      "   CV MAE: 3.0077 (¬±0.0861)\n",
      "   CV R¬≤: 0.9149 (¬±0.0054)\n",
      "   Overfitting: 0.0350 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR CatBoost:\n",
      "----------------------------------------\n",
      "  iterations: 170\n",
      "  depth: 6\n",
      "  learning_rate: 0.0714\n",
      "  l2_leaf_reg: 9.1746\n",
      "  bagging_temperature: 0.8502\n",
      "  random_strength: 0.0529\n",
      "  border_count: 71\n",
      "\n",
      "üìä FEATURE IMPORTANCE (CatBoost)\n",
      "----------------------------------------\n",
      "Training CatBoost on full dataset for feature importance...\n",
      "  ‚úÖ GPU | CV MAE: 3.0077\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)   GPU       \n",
      "------------------------------------------------------------------------------------------\n",
      "CatBoost               0.9149    3.0077     0.0350 ‚úì        4.3    üöÄ\n",
      "XGBoost                0.9146    3.0306     0.0324 ‚úì        1.5    üöÄ\n",
      "\n",
      "üéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Selected Model: CatBoost\n",
      "Selection Reason: Selected for low overfitting (0.0350) with minimal MAE penalty (0.0000)\n",
      "MAE: 3.0077, Overfitting: 0.0350\n",
      "\n",
      "üèÜ BEST OPTIMIZED MODEL: CatBoost\n",
      "   CV MAE: 3.0077 (¬±0.0861)\n",
      "   CV R¬≤: 0.9149 (¬±0.0054)\n",
      "   Overfitting: 0.0350 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR CatBoost:\n",
      "----------------------------------------\n",
      "  iterations: 170\n",
      "  depth: 6\n",
      "  learning_rate: 0.0714\n",
      "  l2_leaf_reg: 9.1746\n",
      "  bagging_temperature: 0.8502\n",
      "  random_strength: 0.0529\n",
      "  border_count: 71\n",
      "\n",
      "üìä FEATURE IMPORTANCE (CatBoost)\n",
      "----------------------------------------\n",
      "Training CatBoost on full dataset for feature importance...\n",
      "\n",
      "Top 15 Features:\n",
      "         Expected_Wins: 75.8354\n",
      "                    SV: 8.3463\n",
      "                    CG: 1.8056\n",
      "                   SHO: 1.6801\n",
      "                   OBP: 1.4831\n",
      "     Era_Adjusted_WHIP: 0.9315\n",
      "                IPouts: 0.9288\n",
      "                    3B: 0.7382\n",
      "                    AB: 0.5502\n",
      "                   BBA: 0.5125\n",
      "                    SB: 0.5060\n",
      "      Era_Adjusted_SLG: 0.4609\n",
      "                   REI: 0.4239\n",
      "  Era_Adjusted_BB_Rate: 0.4226\n",
      "                    SO: 0.4089\n",
      "\n",
      "‚ú® OPTIMIZATION COMPLETE!\n",
      "   Best model improved MAE through Optuna hyperparameter tuning\n",
      "   Ready for ensemble or final predictions\n",
      "\n",
      "Top 15 Features:\n",
      "         Expected_Wins: 75.8354\n",
      "                    SV: 8.3463\n",
      "                    CG: 1.8056\n",
      "                   SHO: 1.6801\n",
      "                   OBP: 1.4831\n",
      "     Era_Adjusted_WHIP: 0.9315\n",
      "                IPouts: 0.9288\n",
      "                    3B: 0.7382\n",
      "                    AB: 0.5502\n",
      "                   BBA: 0.5125\n",
      "                    SB: 0.5060\n",
      "      Era_Adjusted_SLG: 0.4609\n",
      "                   REI: 0.4239\n",
      "  Era_Adjusted_BB_Rate: 0.4226\n",
      "                    SO: 0.4089\n",
      "\n",
      "‚ú® OPTIMIZATION COMPLETE!\n",
      "   Best model improved MAE through Optuna hyperparameter tuning\n",
      "   Ready for ensemble or final predictions\n"
     ]
    }
   ],
   "source": [
    "# Build models with optimized parameters and perform detailed comparison\n",
    "print(\"\\nüèóÔ∏è BUILDING OPTIMIZED MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create models with optimized parameters\n",
    "def create_optimized_model(model_name, params):\n",
    "    \"\"\"Create a model instance with optimized parameters\"\"\"\n",
    "    if model_name == 'XGBoost':\n",
    "        # Try GPU first, fallback to CPU\n",
    "        try:\n",
    "            params_gpu = params.copy()\n",
    "            params_gpu['device'] = 'cuda'\n",
    "            params_gpu['tree_method'] = 'hist'\n",
    "            params_gpu['verbosity'] = 0\n",
    "            model = XGBRegressor(**params_gpu)\n",
    "            # Test if GPU works\n",
    "            model.fit(X[:100], y[:100])\n",
    "            return model, '‚úÖ GPU'\n",
    "        except:\n",
    "            params_cpu = params.copy()\n",
    "            params_cpu['device'] = 'cpu'\n",
    "            params_cpu['tree_method'] = 'hist' \n",
    "            params_cpu['verbosity'] = 0\n",
    "            return XGBRegressor(**params_cpu), '‚ö†Ô∏è CPU'\n",
    "            \n",
    "    elif model_name == 'CatBoost':\n",
    "        try:\n",
    "            params_gpu = params.copy()\n",
    "            params_gpu['task_type'] = 'GPU'\n",
    "            params_gpu['verbose'] = False\n",
    "            model = CatBoostRegressor(**params_gpu)\n",
    "            # Test if GPU works\n",
    "            model.fit(X[:100], y[:100])\n",
    "            return model, '‚úÖ GPU'\n",
    "        except:\n",
    "            params_cpu = params.copy()\n",
    "            params_cpu['task_type'] = 'CPU'\n",
    "            params_cpu['verbose'] = False\n",
    "            return CatBoostRegressor(**params_cpu), '‚ö†Ô∏è CPU'\n",
    "\n",
    "# Build optimized models\n",
    "optimized_models = {}\n",
    "cv_results_optimized = {}\n",
    "\n",
    "for name, params in optimized_params.items():\n",
    "    print(f\"\\nBuilding optimized {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, gpu_status = create_optimized_model(name, params)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        model, X, y,\n",
    "        cv=5,\n",
    "        scoring=['r2', 'neg_mean_absolute_error'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=1 if 'GPU' in gpu_status else -1\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    cv_results_optimized[name] = {\n",
    "        'test_r2': cv_scores['test_r2'].mean(),\n",
    "        'test_r2_std': cv_scores['test_r2'].std(),\n",
    "        'test_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "        'test_mae_std': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "        'train_r2': cv_scores['train_r2'].mean(),\n",
    "        'overfitting': cv_scores['train_r2'].mean() - cv_scores['test_r2'].mean(),\n",
    "        'time': end_time - start_time,\n",
    "        'gpu_status': gpu_status\n",
    "    }\n",
    "    \n",
    "    optimized_models[name] = model\n",
    "    print(f\"  {gpu_status} | CV MAE: {cv_results_optimized[name]['test_mae']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OPTIMIZED MODELS RESULTS SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<22} {'Test R¬≤':<10} {'Test MAE':<11} {'Overfitting':<13} {'Time (s)':<10} {'GPU':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Sort by Test MAE (lower is better) for initial ranking\n",
    "sorted_results = sorted(cv_results_optimized.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "for name, result in sorted_results:\n",
    "    overfit_warning = \"‚ö†Ô∏è\" if result['overfitting'] > 0.05 else \"‚úì\"\n",
    "    gpu_icon = \"üöÄ\" if \"GPU\" in result['gpu_status'] else \"üíª\"\n",
    "    print(f\"{name:<22} {result['test_r2']:.4f}    {result['test_mae']:.4f}     \"\n",
    "          f\"{result['overfitting']:>6.4f} {overfit_warning:<5} {result['time']:>6.1f}    {gpu_icon}\")\n",
    "\n",
    "print(f\"\\nüéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Select best model considering both performance and overfitting\n",
    "def select_best_model_with_overfitting_control(results, overfitting_threshold=0.05, mae_tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Select the best model balancing performance and overfitting.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of model results\n",
    "        overfitting_threshold: Maximum acceptable overfitting gap (train_r2 - test_r2)\n",
    "        mae_tolerance: MAE tolerance for accepting a less overfitting model over the best performer\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_model_name, reason)\n",
    "    \"\"\"\n",
    "    # Sort by MAE first\n",
    "    sorted_by_mae = sorted(results.items(), key=lambda x: x[1]['test_mae'])\n",
    "    \n",
    "    # Find models that don't overfit significantly\n",
    "    non_overfitting_models = [\n",
    "        (name, result) for name, result in sorted_by_mae \n",
    "        if result['overfitting'] <= overfitting_threshold\n",
    "    ]\n",
    "    \n",
    "    best_mae_model = sorted_by_mae[0]\n",
    "    best_mae = best_mae_model[1]['test_mae']\n",
    "    \n",
    "    if non_overfitting_models:\n",
    "        # Check if the best non-overfitting model is within acceptable MAE tolerance\n",
    "        best_non_overfit = non_overfitting_models[0]\n",
    "        mae_diff = best_non_overfit[1]['test_mae'] - best_mae\n",
    "        \n",
    "        if mae_diff <= mae_tolerance:\n",
    "            return best_non_overfit[0], f\"Selected for low overfitting ({best_non_overfit[1]['overfitting']:.4f}) with minimal MAE penalty ({mae_diff:.4f})\"\n",
    "        else:\n",
    "            # Check if the best MAE model overfits significantly\n",
    "            if best_mae_model[1]['overfitting'] > overfitting_threshold:\n",
    "                return best_non_overfit[0], f\"Selected to avoid overfitting. Best MAE model overfits by {best_mae_model[1]['overfitting']:.4f}\"\n",
    "            else:\n",
    "                return best_mae_model[0], f\"Selected for best MAE ({best_mae:.4f}) with acceptable overfitting ({best_mae_model[1]['overfitting']:.4f})\"\n",
    "    else:\n",
    "        # All models overfit, choose the one with least overfitting among top performers\n",
    "        print(\"  ‚ö†Ô∏è All models show overfitting. Selecting least overfitting among top 3 MAE performers.\")\n",
    "        top_3_mae = sorted_by_mae[:3]\n",
    "        least_overfit_of_top3 = min(top_3_mae, key=lambda x: x[1]['overfitting'])\n",
    "        return least_overfit_of_top3[0], f\"Least overfitting ({least_overfit_of_top3[1]['overfitting']:.4f}) among top 3 MAE performers\"\n",
    "\n",
    "# Apply intelligent model selection\n",
    "best_model_name, selection_reason = select_best_model_with_overfitting_control(cv_results_optimized)\n",
    "best_model = optimized_models[best_model_name]\n",
    "best_mae = cv_results_optimized[best_model_name]['test_mae']\n",
    "best_overfitting = cv_results_optimized[best_model_name]['overfitting']\n",
    "\n",
    "print(f\"\\nSelected Model: {best_model_name}\")\n",
    "print(f\"Selection Reason: {selection_reason}\")\n",
    "print(f\"MAE: {best_mae:.4f}, Overfitting: {best_overfitting:.4f}\")\n",
    "\n",
    "# Show comparison with pure MAE-based selection\n",
    "pure_mae_best = sorted_results[0][0]\n",
    "if pure_mae_best != best_model_name:\n",
    "    pure_mae_result = cv_results_optimized[pure_mae_best]\n",
    "    print(f\"\\nComparison with pure MAE selection:\")\n",
    "    print(f\"  Pure MAE Best: {pure_mae_best} (MAE: {pure_mae_result['test_mae']:.4f}, Overfitting: {pure_mae_result['overfitting']:.4f})\")\n",
    "    print(f\"  Selected Model: {best_model_name} (MAE: {best_mae:.4f}, Overfitting: {best_overfitting:.4f})\")\n",
    "    mae_diff = best_mae - pure_mae_result['test_mae']\n",
    "    overfit_improvement = pure_mae_result['overfitting'] - best_overfitting\n",
    "    print(f\"  Trade-off: +{mae_diff:.4f} MAE for -{overfit_improvement:.4f} overfitting reduction\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST OPTIMIZED MODEL: {best_model_name}\")\n",
    "print(f\"   CV MAE: {best_mae:.4f} (¬±{cv_results_optimized[best_model_name]['test_mae_std']:.4f})\")\n",
    "print(f\"   CV R¬≤: {cv_results_optimized[best_model_name]['test_r2']:.4f} (¬±{cv_results_optimized[best_model_name]['test_r2_std']:.4f})\")\n",
    "print(f\"   Overfitting: {best_overfitting:.4f} ({'‚ö†Ô∏è' if best_overfitting > 0.05 else '‚úì'} {'High' if best_overfitting > 0.05 else 'Acceptable'})\")\n",
    "\n",
    "# Display optimized parameters\n",
    "print(f\"\\nüîß OPTIMIZED PARAMETERS FOR {best_model_name}:\")\n",
    "print(\"-\" * 40)\n",
    "for param, value in optimized_params[best_model_name].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "# Feature importance for best model\n",
    "print(f\"\\nüìä FEATURE IMPORTANCE ({best_model_name})\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training {best_model_name} on full dataset for feature importance...\")\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 15 Features:\")\n",
    "    for i, row in importance_df.head(15).iterrows():\n",
    "        print(f\"  {row['feature']:>20}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® OPTIMIZATION COMPLETE!\")\n",
    "print(f\"   Best model improved MAE through Optuna hyperparameter tuning\")\n",
    "print(f\"   Ready for ensemble or final predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc9b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA-OPTIMIZED LINEAR MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (1812, 52)\n",
      "Using 52 engineered features\n",
      "\n",
      "üîç LINEAR MODELS HYPERPARAMETER OPTIMIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Optimizing Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7194 (Time: 2.5s)\n",
      "\n",
      "Optimizing Lasso hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7194 (Time: 2.5s)\n",
      "\n",
      "Optimizing Lasso hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7166 (Time: 0.6s)\n",
      "\n",
      "Optimizing ElasticNet hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7166 (Time: 0.6s)\n",
      "\n",
      "Optimizing ElasticNet hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7196 (Time: 0.7s)\n",
      "\n",
      "Optimizing Huber hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7196 (Time: 0.7s)\n",
      "\n",
      "Optimizing Huber hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7201 (Time: 2.3s)\n",
      "\n",
      "Optimizing Polynomial_Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7201 (Time: 2.3s)\n",
      "\n",
      "Optimizing Polynomial_Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7426 (Time: 13.7s)\n",
      "\n",
      "Testing Linear Regression (no hyperparameters)...\n",
      "  ‚úÖ MAE: 2.7312 (Time: 0.0s)\n",
      "\n",
      "üìã LINEAR MODELS OPTIMIZATION SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ridge:\n",
      "  Best CV MAE: 2.7194\n",
      "  Optimization time: 2.5s\n",
      "  Trials completed: 30\n",
      "\n",
      "Lasso:\n",
      "  Best CV MAE: 2.7166\n",
      "  Optimization time: 0.6s\n",
      "  Trials completed: 30\n",
      "\n",
      "ElasticNet:\n",
      "  Best CV MAE: 2.7196\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "Huber:\n",
      "  Best CV MAE: 2.7201\n",
      "  Optimization time: 2.3s\n",
      "  Trials completed: 30\n",
      "\n",
      "Polynomial_Ridge:\n",
      "  Best CV MAE: 2.7426\n",
      "  Optimization time: 13.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "LinearRegression:\n",
      "  Best CV MAE: 2.7312\n",
      "  Optimization time: 0.0s\n",
      "  Trials completed: 1\n",
      "\n",
      "  ‚úÖ Best MAE: 2.7426 (Time: 13.7s)\n",
      "\n",
      "Testing Linear Regression (no hyperparameters)...\n",
      "  ‚úÖ MAE: 2.7312 (Time: 0.0s)\n",
      "\n",
      "üìã LINEAR MODELS OPTIMIZATION SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ridge:\n",
      "  Best CV MAE: 2.7194\n",
      "  Optimization time: 2.5s\n",
      "  Trials completed: 30\n",
      "\n",
      "Lasso:\n",
      "  Best CV MAE: 2.7166\n",
      "  Optimization time: 0.6s\n",
      "  Trials completed: 30\n",
      "\n",
      "ElasticNet:\n",
      "  Best CV MAE: 2.7196\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "Huber:\n",
      "  Best CV MAE: 2.7201\n",
      "  Optimization time: 2.3s\n",
      "  Trials completed: 30\n",
      "\n",
      "Polynomial_Ridge:\n",
      "  Best CV MAE: 2.7426\n",
      "  Optimization time: 13.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "LinearRegression:\n",
      "  Best CV MAE: 2.7312\n",
      "  Optimization time: 0.0s\n",
      "  Trials completed: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import linear model libraries and continue using Optuna\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Comprehensive warning suppression for clean output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\") \n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # üîß Suppress all convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Objective did not converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*coordinate_descent.*\")  # üîß Suppress coordinate descent warnings\n",
    "\n",
    "print(\"OPTUNA-OPTIMIZED LINEAR MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "X_linear = X_full\n",
    "y_linear = y_full\n",
    "\n",
    "print(f\"\\nDataset shape: {X_linear.shape}\")\n",
    "print(f\"Using {len(available_features)} engineered features\")\n",
    "\n",
    "# Define objective functions for Optuna hyperparameter optimization\n",
    "def ridge_objective(trial):\n",
    "    \"\"\"Objective function for Ridge regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=alpha, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def lasso_objective(trial):\n",
    "    \"\"\"Objective function for Lasso regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)  # üîß Raised minimum alpha for better convergence\n",
    "    max_iter = trial.suggest_int('max_iter', 10000, 20000)  # üîß Much higher iteration range\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Lasso(alpha=alpha, max_iter=max_iter, tol=1e-3, random_state=42, warm_start=False))  # üîß Relaxed tolerance, no warm start\n",
    "    ])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # üîß Local warning suppression\n",
    "        scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def elasticnet_objective(trial):\n",
    "    \"\"\"Objective function for ElasticNet regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)  # üîß Raised minimum alpha\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "    max_iter = trial.suggest_int('max_iter', 10000, 20000)  # üîß Much higher iteration range\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, tol=1e-3, random_state=42, warm_start=False))  # üîß Relaxed tolerance\n",
    "    ])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # üîß Local warning suppression  \n",
    "        scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def huber_objective(trial):\n",
    "    \"\"\"Objective function for Huber regression hyperparameter tuning\"\"\"\n",
    "    epsilon = trial.suggest_float('epsilon', 1.1, 3.0)\n",
    "    alpha = trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    max_iter = trial.suggest_int('max_iter', 1000, 5000)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', HuberRegressor(epsilon=epsilon, alpha=alpha, max_iter=max_iter, tol=1e-05))\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def polynomial_ridge_objective(trial):\n",
    "    \"\"\"Objective function for Polynomial Ridge regression hyperparameter tuning\"\"\"\n",
    "    degree = trial.suggest_int('degree', 2, 3)\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 1000.0, log=True)\n",
    "    include_bias = trial.suggest_categorical('include_bias', [True, False])\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=alpha, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optimize hyperparameters for each linear model\n",
    "print(\"\\nüîç LINEAR MODELS HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "optimized_linear_params = {}\n",
    "linear_optimization_results = {}\n",
    "\n",
    "# List of models to optimize\n",
    "linear_models_to_optimize = [\n",
    "    ('Ridge', ridge_objective),\n",
    "    ('Lasso', lasso_objective), \n",
    "    ('ElasticNet', elasticnet_objective),\n",
    "    ('Huber', huber_objective),\n",
    "    ('Polynomial_Ridge', polynomial_ridge_objective)\n",
    "]\n",
    "\n",
    "for model_name, objective_func in linear_models_to_optimize:\n",
    "    print(f\"\\nOptimizing {model_name} hyperparameters...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create study for this model\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        study.optimize(objective_func, n_trials=30, show_progress_bar=False)\n",
    "        optimization_time = time.time() - start_time\n",
    "        \n",
    "        optimized_linear_params[model_name] = study.best_params\n",
    "        linear_optimization_results[model_name] = {\n",
    "            'best_mae': study.best_value,\n",
    "            'optimization_time': optimization_time,\n",
    "            'n_trials': len(study.trials),\n",
    "            'status': 'Success'\n",
    "        }\n",
    "        print(f\"  ‚úÖ Best MAE: {study.best_value:.4f} (Time: {optimization_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "        linear_optimization_results[model_name] = {\n",
    "            'status': 'Failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Also include basic Linear Regression (no hyperparameters to optimize)\n",
    "print(f\"\\nTesting Linear Regression (no hyperparameters)...\")\n",
    "start_time = time.time()\n",
    "linear_reg_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "scores = cross_val_score(linear_reg_model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "linear_reg_time = time.time() - start_time\n",
    "\n",
    "optimized_linear_params['LinearRegression'] = {}\n",
    "linear_optimization_results['LinearRegression'] = {\n",
    "    'best_mae': -scores.mean(),\n",
    "    'optimization_time': linear_reg_time,\n",
    "    'n_trials': 1,\n",
    "    'status': 'Success'\n",
    "}\n",
    "print(f\"  ‚úÖ MAE: {-scores.mean():.4f} (Time: {linear_reg_time:.1f}s)\")\n",
    "\n",
    "print(f\"\\nüìã LINEAR MODELS OPTIMIZATION SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "successful_linear = {k: v for k, v in linear_optimization_results.items() if v.get('status') == 'Success'}\n",
    "for model_name, result in successful_linear.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Best CV MAE: {result['best_mae']:.4f}\")\n",
    "    print(f\"  Optimization time: {result['optimization_time']:.1f}s\")\n",
    "    print(f\"  Trials completed: {result['n_trials']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca7aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è BUILDING OPTIMIZED LINEAR MODELS\n",
      "--------------------------------------------------\n",
      "\n",
      "Building optimized Ridge...\n",
      "  ‚úÖ CV MAE: 2.7263\n",
      "\n",
      "Building optimized Lasso...\n",
      "  ‚úÖ CV MAE: 2.7235\n",
      "\n",
      "Building optimized ElasticNet...\n",
      "  ‚úÖ CV MAE: 2.7259\n",
      "\n",
      "Building optimized Huber...\n",
      "  ‚úÖ CV MAE: 2.7261\n",
      "\n",
      "Building optimized Polynomial_Ridge...\n",
      "  ‚úÖ CV MAE: 2.7444\n",
      "\n",
      "Building optimized LinearRegression...\n",
      "  ‚úÖ CV MAE: 2.7448\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED LINEAR MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)  \n",
      "------------------------------------------------------------------------------------------\n",
      "Lasso                  0.9312    2.7235     0.0041 ‚úì        0.0\n",
      "ElasticNet             0.9311    2.7259     0.0041 ‚úì        0.0\n",
      "Huber                  0.9309    2.7261     0.0047 ‚úì        0.0\n",
      "Ridge                  0.9310    2.7263     0.0046 ‚úì        0.0\n",
      "Polynomial_Ridge       0.9300    2.7444     0.0116 ‚úì        0.2\n",
      "LinearRegression       0.9301    2.7448     0.0059 ‚úì        0.0\n",
      "\n",
      "üéØ INTELLIGENT LINEAR MODEL SELECTION\n",
      "--------------------------------------------------\n",
      "\n",
      "Selected Linear Model: Lasso\n",
      "Selection Reason: Selected for low overfitting (0.0041) with minimal MAE penalty (0.0000)\n",
      "MAE: 2.7235, Overfitting: 0.0041\n",
      "\n",
      "üèÜ BEST OPTIMIZED LINEAR MODEL: Lasso\n",
      "   CV MAE: 2.7235 (¬±0.0525)\n",
      "   CV R¬≤: 0.9312 (¬±0.0068)\n",
      "   Overfitting: 0.0041 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR Lasso:\n",
      "----------------------------------------\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "üîÑ COMPARISON WITH BOOSTING MODELS\n",
      "--------------------------------------------------\n",
      "Best Boosting Model: CatBoost (MAE: 3.0077)\n",
      "Best Linear Model: Lasso (MAE: 2.7235)\n",
      "‚úÖ Linear model outperforms boosting by 0.2843\n",
      "\n",
      "‚ú® LINEAR OPTIMIZATION COMPLETE!\n",
      "   All linear models optimized with Optuna hyperparameter tuning\n",
      "   Ready for enhanced ensemble with optimized linear models\n",
      "  ‚úÖ CV MAE: 2.7444\n",
      "\n",
      "Building optimized LinearRegression...\n",
      "  ‚úÖ CV MAE: 2.7448\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED LINEAR MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)  \n",
      "------------------------------------------------------------------------------------------\n",
      "Lasso                  0.9312    2.7235     0.0041 ‚úì        0.0\n",
      "ElasticNet             0.9311    2.7259     0.0041 ‚úì        0.0\n",
      "Huber                  0.9309    2.7261     0.0047 ‚úì        0.0\n",
      "Ridge                  0.9310    2.7263     0.0046 ‚úì        0.0\n",
      "Polynomial_Ridge       0.9300    2.7444     0.0116 ‚úì        0.2\n",
      "LinearRegression       0.9301    2.7448     0.0059 ‚úì        0.0\n",
      "\n",
      "üéØ INTELLIGENT LINEAR MODEL SELECTION\n",
      "--------------------------------------------------\n",
      "\n",
      "Selected Linear Model: Lasso\n",
      "Selection Reason: Selected for low overfitting (0.0041) with minimal MAE penalty (0.0000)\n",
      "MAE: 2.7235, Overfitting: 0.0041\n",
      "\n",
      "üèÜ BEST OPTIMIZED LINEAR MODEL: Lasso\n",
      "   CV MAE: 2.7235 (¬±0.0525)\n",
      "   CV R¬≤: 0.9312 (¬±0.0068)\n",
      "   Overfitting: 0.0041 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR Lasso:\n",
      "----------------------------------------\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "üîÑ COMPARISON WITH BOOSTING MODELS\n",
      "--------------------------------------------------\n",
      "Best Boosting Model: CatBoost (MAE: 3.0077)\n",
      "Best Linear Model: Lasso (MAE: 2.7235)\n",
      "‚úÖ Linear model outperforms boosting by 0.2843\n",
      "\n",
      "‚ú® LINEAR OPTIMIZATION COMPLETE!\n",
      "   All linear models optimized with Optuna hyperparameter tuning\n",
      "   Ready for enhanced ensemble with optimized linear models\n"
     ]
    }
   ],
   "source": [
    "# Build optimized linear models and perform detailed comparison\n",
    "print(\"\\nüèóÔ∏è BUILDING OPTIMIZED LINEAR MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create models with optimized parameters\n",
    "def create_optimized_linear_model(model_name, params):\n",
    "    \"\"\"Create a linear model instance with optimized parameters\"\"\"\n",
    "    if model_name == 'Ridge':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Ridge(alpha=params['alpha'], random_state=42))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Lasso':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Lasso(\n",
    "                alpha=params['alpha'], \n",
    "                max_iter=params['max_iter'], \n",
    "                tol=1e-4,  # üîß Improved tolerance\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'ElasticNet':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', ElasticNet(\n",
    "                alpha=params['alpha'], \n",
    "                l1_ratio=params['l1_ratio'], \n",
    "                max_iter=params['max_iter'],\n",
    "                tol=1e-4,  # üîß Improved tolerance \n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Huber':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', HuberRegressor(\n",
    "                epsilon=params['epsilon'],\n",
    "                alpha=params['alpha'],\n",
    "                max_iter=params['max_iter'],\n",
    "                tol=1e-05\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Polynomial_Ridge':\n",
    "        return Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=params['degree'], include_bias=params['include_bias'])),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Ridge(alpha=params['alpha'], random_state=42))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'LinearRegression':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LinearRegression())\n",
    "        ])\n",
    "\n",
    "# Build optimized linear models\n",
    "optimized_linear_models = {}\n",
    "cv_results_linear_optimized = {}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name in successful_linear.keys():\n",
    "    print(f\"\\nBuilding optimized {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = create_optimized_linear_model(name, optimized_linear_params[name])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        model, X_linear, y_linear,\n",
    "        cv=cv,\n",
    "        scoring=['r2', 'neg_mean_absolute_error'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    cv_results_linear_optimized[name] = {\n",
    "        'test_r2': cv_scores['test_r2'].mean(),\n",
    "        'test_r2_std': cv_scores['test_r2'].std(),\n",
    "        'test_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "        'test_mae_std': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "        'train_r2': cv_scores['train_r2'].mean(),\n",
    "        'overfitting': cv_scores['train_r2'].mean() - cv_scores['test_r2'].mean(),\n",
    "        'time': end_time - start_time\n",
    "    }\n",
    "    \n",
    "    optimized_linear_models[name] = model\n",
    "    print(f\"  ‚úÖ CV MAE: {cv_results_linear_optimized[name]['test_mae']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OPTIMIZED LINEAR MODELS RESULTS SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<22} {'Test R¬≤':<10} {'Test MAE':<11} {'Overfitting':<13} {'Time (s)':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Sort by Test MAE (lower is better)\n",
    "sorted_linear_results = sorted(cv_results_linear_optimized.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "for name, result in sorted_linear_results:\n",
    "    overfit_warning = \"‚ö†Ô∏è\" if result['overfitting'] > 0.05 else \"‚úì\"\n",
    "    print(f\"{name:<22} {result['test_r2']:.4f}    {result['test_mae']:.4f}     \"\n",
    "          f\"{result['overfitting']:>6.4f} {overfit_warning:<5} {result['time']:>6.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ INTELLIGENT LINEAR MODEL SELECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Apply intelligent model selection (similar to boosting models)\n",
    "best_linear_model_name, linear_selection_reason = select_best_model_with_overfitting_control(\n",
    "    cv_results_linear_optimized, \n",
    "    overfitting_threshold=0.05, \n",
    "    mae_tolerance=0.01\n",
    ")\n",
    "best_linear_model = optimized_linear_models[best_linear_model_name]\n",
    "best_linear_mae = cv_results_linear_optimized[best_linear_model_name]['test_mae']\n",
    "best_linear_overfitting = cv_results_linear_optimized[best_linear_model_name]['overfitting']\n",
    "\n",
    "print(f\"\\nSelected Linear Model: {best_linear_model_name}\")\n",
    "print(f\"Selection Reason: {linear_selection_reason}\")\n",
    "print(f\"MAE: {best_linear_mae:.4f}, Overfitting: {best_linear_overfitting:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST OPTIMIZED LINEAR MODEL: {best_linear_model_name}\")\n",
    "print(f\"   CV MAE: {best_linear_mae:.4f} (¬±{cv_results_linear_optimized[best_linear_model_name]['test_mae_std']:.4f})\")\n",
    "print(f\"   CV R¬≤: {cv_results_linear_optimized[best_linear_model_name]['test_r2']:.4f} (¬±{cv_results_linear_optimized[best_linear_model_name]['test_r2_std']:.4f})\")\n",
    "print(f\"   Overfitting: {best_linear_overfitting:.4f} ({'‚ö†Ô∏è' if best_linear_overfitting > 0.05 else '‚úì'} {'High' if best_linear_overfitting > 0.05 else 'Acceptable'})\")\n",
    "\n",
    "# Display optimized parameters\n",
    "if optimized_linear_params[best_linear_model_name]:  # Skip if empty (LinearRegression)\n",
    "    print(f\"\\nüîß OPTIMIZED PARAMETERS FOR {best_linear_model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for param, value in optimized_linear_params[best_linear_model_name].items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {param}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüîÑ COMPARISON WITH BOOSTING MODELS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Best Boosting Model: {best_model_name} (MAE: {best_mae:.4f})\")\n",
    "print(f\"Best Linear Model: {best_linear_model_name} (MAE: {best_linear_mae:.4f})\")\n",
    "\n",
    "if best_linear_mae < best_mae:\n",
    "    print(f\"‚úÖ Linear model outperforms boosting by {best_mae - best_linear_mae:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Boosting model outperforms linear by {best_linear_mae - best_mae:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® LINEAR OPTIMIZATION COMPLETE!\")\n",
    "print(f\"   All linear models optimized with Optuna hyperparameter tuning\")\n",
    "print(f\"   Ready for enhanced ensemble with optimized linear models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f31f9",
   "metadata": {},
   "source": [
    "# Key Differences Between Weighted and Stacked Ensemble Approaches\n",
    "\n",
    "| Aspect | Weighted Ensemble | Stacked Ensemble |\n",
    "|--------|-------------------|------------------|\n",
    "| **Meta-learning** | ‚ùå No | ‚úÖ Yes |\n",
    "| **Learning Method** | Mathematical optimization | Machine learning model |\n",
    "| **Validation Strategy** | Cross-validation for evaluation | Cross-validation for meta-training |\n",
    "| **Weight Determination** | Optuna optimization (scipy.optimize) | Meta-learner learns automatically |\n",
    "| **Model Complexity** | Medium | High |\n",
    "| **Training Speed** | Fast | Slower |\n",
    "| **Interpretability** | ‚úÖ Clear weights | ‚ùå Black-box meta-model |\n",
    "| **Overfitting Risk** | Lower | Medium |\n",
    "| **Feature Engineering** | Uses base predictions only | Can learn complex interactions |\n",
    "| **Hyperparameter Tuning** | Base models + weights | Base models + meta-learner |\n",
    "| **Implementation Complexity** | Simpler | More complex |\n",
    "| **Performance Potential** | Good | Potentially higher |\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Weighted Ensemble**: Combines predictions using mathematically optimized weights\n",
    "- **Stacked Ensemble**: Uses a meta-learner to learn how to combine base model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da0da9",
   "metadata": {},
   "source": [
    "# Advanced Ensemble Model Selection - Implementation Summary\n",
    "\n",
    "## üéØ **Key Improvements Made**\n",
    "\n",
    "### **1. Weighted Ensemble Enhancement**\n",
    "- **Before**: Pure performance-based selection (top 2 linear + top 1 boosting)\n",
    "- **After**: Balanced selection considering both performance (60%) and diversity (40%)\n",
    "- **Benefit**: Reduces redundancy, improves ensemble complementarity\n",
    "\n",
    "### **2. Stacked Ensemble Enhancement**  \n",
    "- **Before**: Parameter variations for diversity (good approach)\n",
    "- **After**: Added correlation analysis and automatic removal of highly correlated models (r > 0.95)\n",
    "- **Benefit**: Ensures base models provide unique information to meta-learner\n",
    "\n",
    "### **3. Selection Strategy Comparison**\n",
    "\n",
    "| Strategy | Weighted Ensemble | Stacked Ensemble |\n",
    "|----------|-------------------|------------------|\n",
    "| **Previous** | Top performers only | Parameter variations |\n",
    "| **Enhanced** | Balanced performance+diversity | Correlation filtering + variations |\n",
    "| **Key Metric** | Balanced score (60%/40% split) | Correlation threshold (r ‚â§ 0.95) |\n",
    "\n",
    "## üîç **How It Works**\n",
    "\n",
    "### **Balanced Selection Formula**\n",
    "```\n",
    "Balanced Score = 0.6 √ó (1/MAE) + 0.4 √ó (1 - avg_correlation)\n",
    "```\n",
    "- Higher performance score = Lower MAE\n",
    "- Higher diversity score = Lower correlation with other models\n",
    "\n",
    "### **Correlation Filtering**\n",
    "- Calculates pairwise correlations between all model predictions\n",
    "- Removes the worse-performing model from highly correlated pairs\n",
    "- Preserves ensemble diversity for better generalization\n",
    "\n",
    "## üìä **Expected Benefits**\n",
    "1. **Reduced Overfitting**: More diverse models generalize better\n",
    "2. **Better Ensemble Performance**: Complementary predictions combine more effectively\n",
    "3. **Automatic Optimization**: No manual model selection needed\n",
    "4. **Robust Validation**: Correlation analysis ensures real diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4f5b",
   "metadata": {},
   "source": [
    "# Weighted Ensemble Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb6133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: WEIGHTED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "1. ADVANCED MODEL SELECTION FOR WEIGHTED ENSEMBLE\n",
      "--------------------------------------------------\n",
      "Generating predictions for all Optuna-optimized models...\n",
      "  Processing linear model: Ridge\n",
      "  Processing linear model: Lasso\n",
      "  Processing linear model: ElasticNet\n",
      "  Processing linear model: Huber\n",
      "  Processing linear model: Polynomial_Ridge\n",
      "  Processing linear model: Huber\n",
      "  Processing linear model: Polynomial_Ridge\n",
      "  Processing linear model: LinearRegression\n",
      "  Processing boosting model: XGBoost\n",
      "  Processing linear model: LinearRegression\n",
      "  Processing boosting model: XGBoost\n",
      "  Processing boosting model: CatBoost\n",
      "  Processing boosting model: CatBoost\n",
      "\n",
      "Applying balanced selection strategy (performance + diversity)...\n",
      "\n",
      "üéØ BALANCED MODEL SELECTION RESULTS\n",
      "--------------------------------------------------\n",
      "Selected 4 models using balanced strategy:\n",
      "  1. Lasso:\n",
      "     MAE: 2.7235\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.222\n",
      "  2. Huber:\n",
      "     MAE: 2.7262\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "  3. Ridge:\n",
      "     MAE: 2.7264\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "  4. ElasticNet:\n",
      "     MAE: 2.7260\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "\n",
      "2. ENSEMBLE COMPOSITION (BALANCED SELECTION)\n",
      "--------------------------------------------------\n",
      "Selected 4 balanced models for ensemble:\n",
      "  ‚úÖ Lasso (Optuna-optimized)\n",
      "  ‚úÖ Huber (Optuna-optimized)\n",
      "  ‚úÖ Ridge (Optuna-optimized)\n",
      "  ‚úÖ ElasticNet (Optuna-optimized)\n",
      "\n",
      "Total ensemble models: 4\n",
      "\n",
      "3. SELECTED MODEL PERFORMANCE SUMMARY\n",
      "--------------------------------------------------\n",
      "Lasso: MAE = 2.7235, R¬≤ = 0.9312\n",
      "Huber: MAE = 2.7261, R¬≤ = 0.9309\n",
      "Ridge: MAE = 2.7263, R¬≤ = 0.9310\n",
      "ElasticNet: MAE = 2.7259, R¬≤ = 0.9311\n",
      "\n",
      "4. OPTUNA PARAMETERS IN USE\n",
      "--------------------------------------------------\n",
      "\n",
      "Lasso (Linear):\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "Huber (Linear):\n",
      "  epsilon: 2.9382\n",
      "  alpha: 0.9628\n",
      "  max_iter: 3990\n",
      "\n",
      "Ridge (Linear):\n",
      "  alpha: 4.8923\n",
      "\n",
      "ElasticNet (Linear):\n",
      "  alpha: 0.0111\n",
      "  l1_ratio: 0.7230\n",
      "  max_iter: 17330\n",
      "\n",
      "üöÄ Using balanced Optuna-optimized models for superior ensemble diversity!\n",
      "\n",
      "Applying balanced selection strategy (performance + diversity)...\n",
      "\n",
      "üéØ BALANCED MODEL SELECTION RESULTS\n",
      "--------------------------------------------------\n",
      "Selected 4 models using balanced strategy:\n",
      "  1. Lasso:\n",
      "     MAE: 2.7235\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.222\n",
      "  2. Huber:\n",
      "     MAE: 2.7262\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "  3. Ridge:\n",
      "     MAE: 2.7264\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "  4. ElasticNet:\n",
      "     MAE: 2.7260\n",
      "     Avg Correlation: 0.997\n",
      "     Diversity Score: 0.003\n",
      "     Performance Score: 0.4\n",
      "     Balanced Score: 0.221\n",
      "\n",
      "2. ENSEMBLE COMPOSITION (BALANCED SELECTION)\n",
      "--------------------------------------------------\n",
      "Selected 4 balanced models for ensemble:\n",
      "  ‚úÖ Lasso (Optuna-optimized)\n",
      "  ‚úÖ Huber (Optuna-optimized)\n",
      "  ‚úÖ Ridge (Optuna-optimized)\n",
      "  ‚úÖ ElasticNet (Optuna-optimized)\n",
      "\n",
      "Total ensemble models: 4\n",
      "\n",
      "3. SELECTED MODEL PERFORMANCE SUMMARY\n",
      "--------------------------------------------------\n",
      "Lasso: MAE = 2.7235, R¬≤ = 0.9312\n",
      "Huber: MAE = 2.7261, R¬≤ = 0.9309\n",
      "Ridge: MAE = 2.7263, R¬≤ = 0.9310\n",
      "ElasticNet: MAE = 2.7259, R¬≤ = 0.9311\n",
      "\n",
      "4. OPTUNA PARAMETERS IN USE\n",
      "--------------------------------------------------\n",
      "\n",
      "Lasso (Linear):\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "Huber (Linear):\n",
      "  epsilon: 2.9382\n",
      "  alpha: 0.9628\n",
      "  max_iter: 3990\n",
      "\n",
      "Ridge (Linear):\n",
      "  alpha: 4.8923\n",
      "\n",
      "ElasticNet (Linear):\n",
      "  alpha: 0.0111\n",
      "  l1_ratio: 0.7230\n",
      "  max_iter: 17330\n",
      "\n",
      "üöÄ Using balanced Optuna-optimized models for superior ensemble diversity!\n"
     ]
    }
   ],
   "source": [
    "print(\"PHASE 1: WEIGHTED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# First, let's identify our top performing models from Optuna optimization\n",
    "print(\"\\n1. ADVANCED MODEL SELECTION FOR WEIGHTED ENSEMBLE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate all model predictions for balanced selection analysis\n",
    "all_model_predictions = {}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Generating predictions for all Optuna-optimized models...\")\n",
    "\n",
    "# Add linear model predictions\n",
    "for name, model in optimized_linear_models.items():\n",
    "    print(f\"  Processing linear model: {name}\")\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=cv, method='predict')\n",
    "    all_model_predictions[name] = oof_pred\n",
    "\n",
    "# Add boosting model predictions  \n",
    "for name, model in optimized_models.items():\n",
    "    print(f\"  Processing boosting model: {name}\")\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=cv, method='predict')\n",
    "    all_model_predictions[name] = oof_pred\n",
    "\n",
    "def balanced_model_selection(model_predictions, target, max_models=4):\n",
    "    \"\"\"Select models balancing performance and diversity\"\"\"\n",
    "    models = list(model_predictions.keys())\n",
    "    scores = {}\n",
    "    \n",
    "    for model in models:\n",
    "        mae = mean_absolute_error(target, model_predictions[model])\n",
    "        \n",
    "        # Calculate average correlation with other models\n",
    "        correlations = []\n",
    "        for other_model in models:\n",
    "            if model != other_model:\n",
    "                corr = abs(np.corrcoef(model_predictions[model], \n",
    "                                     model_predictions[other_model])[0, 1])\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        avg_correlation = np.mean(correlations)\n",
    "        \n",
    "        # Balanced score: 60% performance + 40% diversity\n",
    "        performance_score = 1 / mae  # Higher is better\n",
    "        diversity_score = 1 - avg_correlation  # Higher is better (lower correlation)\n",
    "        \n",
    "        balanced_score = 0.6 * performance_score + 0.4 * diversity_score\n",
    "        scores[model] = {\n",
    "            'balanced_score': balanced_score,\n",
    "            'mae': mae,\n",
    "            'avg_correlation': avg_correlation,\n",
    "            'performance_score': performance_score,\n",
    "            'diversity_score': diversity_score\n",
    "        }\n",
    "    \n",
    "    # Select top models by balanced score\n",
    "    selected = sorted(scores.keys(), key=lambda x: scores[x]['balanced_score'], reverse=True)[:max_models]\n",
    "    return selected, scores\n",
    "\n",
    "# Apply balanced selection strategy\n",
    "print(f\"\\nApplying balanced selection strategy (performance + diversity)...\")\n",
    "selected_models, selection_scores = balanced_model_selection(all_model_predictions, y_full, max_models=4)\n",
    "\n",
    "print(f\"\\nüéØ BALANCED MODEL SELECTION RESULTS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected {len(selected_models)} models using balanced strategy:\")\n",
    "for i, model in enumerate(selected_models):\n",
    "    score_info = selection_scores[model]\n",
    "    print(f\"  {i+1}. {model}:\")\n",
    "    print(f\"     MAE: {score_info['mae']:.4f}\")\n",
    "    print(f\"     Avg Correlation: {score_info['avg_correlation']:.3f}\")\n",
    "    print(f\"     Diversity Score: {score_info['diversity_score']:.3f}\")\n",
    "    print(f\"     Performance Score: {score_info['performance_score']:.1f}\")\n",
    "    print(f\"     Balanced Score: {score_info['balanced_score']:.3f}\")\n",
    "\n",
    "# Build ensemble_models dict with selected models\n",
    "ensemble_models = {}\n",
    "for name in selected_models:\n",
    "    if name in optimized_linear_models:\n",
    "        ensemble_models[name] = optimized_linear_models[name]\n",
    "    else:\n",
    "        ensemble_models[name] = optimized_models[name]\n",
    "\n",
    "print(f\"\\n2. ENSEMBLE COMPOSITION (BALANCED SELECTION)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected {len(ensemble_models)} balanced models for ensemble:\")\n",
    "for name in ensemble_models.keys():\n",
    "    print(f\"  ‚úÖ {name} (Optuna-optimized)\")\n",
    "    \n",
    "print(f\"\\nTotal ensemble models: {len(ensemble_models)}\")\n",
    "\n",
    "# Store performance metrics for weight calculation using Optuna results\n",
    "model_performance = {}\n",
    "for name in ensemble_models.keys():\n",
    "    if name in cv_results_linear_optimized:  # Linear model\n",
    "        model_performance[name] = {\n",
    "            'mae': cv_results_linear_optimized[name]['test_mae'],\n",
    "            'r2': cv_results_linear_optimized[name]['test_r2']\n",
    "        }\n",
    "    else:  # Boosting model\n",
    "        model_performance[name] = {\n",
    "            'mae': cv_results_optimized[name]['test_mae'], \n",
    "            'r2': cv_results_optimized[name]['test_r2']\n",
    "        }\n",
    "\n",
    "print(f\"\\n3. SELECTED MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "for name, perf in model_performance.items():\n",
    "    print(f\"{name}: MAE = {perf['mae']:.4f}, R¬≤ = {perf['r2']:.4f}\")\n",
    "\n",
    "# Display the Optuna-optimized parameters being used\n",
    "print(f\"\\n4. OPTUNA PARAMETERS IN USE\")\n",
    "print(\"-\" * 50)\n",
    "for name in ensemble_models.keys():\n",
    "    if name in optimized_linear_params and optimized_linear_params[name]:\n",
    "        print(f\"\\n{name} (Linear):\")\n",
    "        for param, value in optimized_linear_params[name].items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {param}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {param}: {value}\")\n",
    "    elif name in optimized_params:\n",
    "        print(f\"\\n{name} (Boosting):\")\n",
    "        for param, value in optimized_params[name].items():\n",
    "            if isinstance(value, float) and 'rate' in param:\n",
    "                print(f\"  {param}: {value:.4f}\")\n",
    "            elif isinstance(value, (int, bool, str)):\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüöÄ Using balanced Optuna-optimized models for superior ensemble diversity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e2121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. GENERATING OUT-OF-FOLD PREDICTIONS (OPTUNA MODELS)\n",
      "------------------------------------------------------------\n",
      "Generating OOF predictions for Lasso (Optuna-optimized)...\n",
      "  OOF MAE: 2.7235\n",
      "Generating OOF predictions for Huber (Optuna-optimized)...\n",
      "  OOF MAE: 2.7262\n",
      "Generating OOF predictions for Ridge (Optuna-optimized)...\n",
      "  OOF MAE: 2.7264\n",
      "Generating OOF predictions for ElasticNet (Optuna-optimized)...\n",
      "  OOF MAE: 2.7260\n",
      "\n",
      "OOF prediction matrix shape: (1812, 4)\n",
      "\n",
      "6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Initial weights (based on Optuna-optimized model performance):\n",
      "  Lasso: 0.250\n",
      "  Huber: 0.250\n",
      "  Ridge: 0.250\n",
      "  ElasticNet: 0.250\n",
      "\n",
      "Optimization successful: True\n",
      "Optimal Optuna ensemble OOF MAE: 2.7234\n",
      "\n",
      "Optimal weights for Optuna-optimized models:\n",
      "  Lasso: 0.886\n",
      "  Huber: 0.062\n",
      "  Ridge: 0.052\n",
      "  ElasticNet: 0.000\n",
      "\n",
      "Improvement over best individual Optuna model:\n",
      "  Best individual MAE: 2.7235\n",
      "  Ensemble MAE: 2.7234\n",
      "  Improvement: 0.0000 (0.00%)\n",
      "\n",
      "‚ú® Ensemble optimization complete using Optuna-optimized base models!\n",
      "  OOF MAE: 2.7262\n",
      "Generating OOF predictions for Ridge (Optuna-optimized)...\n",
      "  OOF MAE: 2.7264\n",
      "Generating OOF predictions for ElasticNet (Optuna-optimized)...\n",
      "  OOF MAE: 2.7260\n",
      "\n",
      "OOF prediction matrix shape: (1812, 4)\n",
      "\n",
      "6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Initial weights (based on Optuna-optimized model performance):\n",
      "  Lasso: 0.250\n",
      "  Huber: 0.250\n",
      "  Ridge: 0.250\n",
      "  ElasticNet: 0.250\n",
      "\n",
      "Optimization successful: True\n",
      "Optimal Optuna ensemble OOF MAE: 2.7234\n",
      "\n",
      "Optimal weights for Optuna-optimized models:\n",
      "  Lasso: 0.886\n",
      "  Huber: 0.062\n",
      "  Ridge: 0.052\n",
      "  ElasticNet: 0.000\n",
      "\n",
      "Improvement over best individual Optuna model:\n",
      "  Best individual MAE: 2.7235\n",
      "  Ensemble MAE: 2.7234\n",
      "  Improvement: 0.0000 (0.00%)\n",
      "\n",
      "‚ú® Ensemble optimization complete using Optuna-optimized base models!\n"
     ]
    }
   ],
   "source": [
    "# Generate out-of-fold predictions for weight optimization with Optuna-optimized models\n",
    "print(\"\\n5. GENERATING OUT-OF-FOLD PREDICTIONS (OPTUNA MODELS)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Generate OOF predictions for each Optuna-optimized model\n",
    "oof_predictions = {}\n",
    "model_names = list(ensemble_models.keys())\n",
    "\n",
    "# Use the same CV strategy as the original optimization\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"Generating OOF predictions for {name} (Optuna-optimized)...\")\n",
    "    \n",
    "    # Use the same CV strategy as before\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=cv, method='predict')\n",
    "    oof_predictions[name] = oof_pred\n",
    "    \n",
    "    # Calculate OOF MAE\n",
    "    oof_mae = mean_absolute_error(y_full, oof_pred)\n",
    "    print(f\"  OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "# Create OOF prediction matrix\n",
    "oof_matrix = np.column_stack([oof_predictions[name] for name in model_names])\n",
    "print(f\"\\nOOF prediction matrix shape: {oof_matrix.shape}\")\n",
    "\n",
    "print(\"\\n6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def ensemble_mae_objective(weights, predictions, targets):\n",
    "    \"\"\"Objective function to minimize: weighted ensemble MAE\"\"\"\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()  # Normalize to sum to 1\n",
    "    ensemble_pred = np.dot(predictions, weights)\n",
    "    return mean_absolute_error(targets, ensemble_pred)\n",
    "\n",
    "# Initial weights based on inverse MAE (better models get higher weights)\n",
    "initial_weights = []\n",
    "for name in model_names:\n",
    "    mae = model_performance[name]['mae']\n",
    "    # Inverse weight: lower MAE = higher weight\n",
    "    weight = 1.0 / mae if mae > 0 else 1.0\n",
    "    initial_weights.append(weight)\n",
    "\n",
    "# Normalize initial weights\n",
    "initial_weights = np.array(initial_weights)\n",
    "initial_weights = initial_weights / initial_weights.sum()\n",
    "\n",
    "print(\"Initial weights (based on Optuna-optimized model performance):\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {initial_weights[i]:.3f}\")\n",
    "\n",
    "# Constraint: weights must sum to 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})\n",
    "\n",
    "# Bounds: each weight between 0 and 1\n",
    "bounds = [(0.0, 1.0) for _ in range(len(model_names))]\n",
    "\n",
    "# Optimize weights\n",
    "result = minimize(\n",
    "    ensemble_mae_objective,\n",
    "    initial_weights,\n",
    "    args=(oof_matrix, y_full),\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=constraints\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "optimal_mae = result.fun\n",
    "\n",
    "print(f\"\\nOptimization successful: {result.success}\")\n",
    "print(f\"Optimal Optuna ensemble OOF MAE: {optimal_mae:.4f}\")\n",
    "print(\"\\nOptimal weights for Optuna-optimized models:\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {optimal_weights[i]:.3f}\")\n",
    "\n",
    "# Calculate improvement over best individual Optuna-optimized model\n",
    "best_individual_mae = min([model_performance[name]['mae'] for name in model_names])\n",
    "improvement = best_individual_mae - optimal_mae\n",
    "print(f\"\\nImprovement over best individual Optuna model:\")\n",
    "print(f\"  Best individual MAE: {best_individual_mae:.4f}\")\n",
    "print(f\"  Ensemble MAE: {optimal_mae:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.4f} ({improvement/best_individual_mae*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚ú® Ensemble optimization complete using Optuna-optimized base models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ad7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. TRAINING FINAL OPTUNA-OPTIMIZED ENSEMBLE MODELS\n",
      "------------------------------------------------------------\n",
      "Training Lasso (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.85 to 109.50\n",
      "Training Huber (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.53 to 109.69\n",
      "Training Ridge (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.57 to 109.50\n",
      "Training ElasticNet (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.79 to 109.02\n",
      "\n",
      "All 4 Optuna-optimized models trained successfully!\n",
      "Test prediction matrix shape: (453, 4)\n",
      "\n",
      "8. GENERATING OPTUNA-ENHANCED ENSEMBLE PREDICTIONS\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced ensemble test predictions:\n",
      "  Range: 44.81 to 109.51\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Comparison with individual Optuna-optimized models:\n",
      "  Lasso (weight=0.886): mean=79.08, std=12.05\n",
      "  Huber (weight=0.062): mean=79.14, std=12.07\n",
      "  Ridge (weight=0.052): mean=79.13, std=12.05\n",
      "  ElasticNet (weight=0.000): mean=79.09, std=12.02\n",
      "\n",
      "9. CREATING OPTUNA-ENHANCED SUBMISSION FILE\n",
      "------------------------------------------------------------\n",
      "‚úÖ Optuna-enhanced submission saved: submission_optuna_weighted_ensemble_20251005_110735.csv\n",
      "üìÅ Path: /home/chrisfkh/sctp-ds-ai/mod3/kaggle_moneyball/submissions/submission_optuna_weighted_ensemble_20251005_110735.csv\n",
      "üìä Predictions shape: (453, 2)\n",
      "\n",
      "First 10 predictions:\n",
      "     ID          W\n",
      "0  1756  69.310039\n",
      "1  1282  74.449161\n",
      "2   351  84.348690\n",
      "3   421  87.120640\n",
      "4    57  93.288428\n",
      "5  1557  97.627928\n",
      "6   846  79.257531\n",
      "7  1658  84.093798\n",
      "8   112  72.888099\n",
      "9  2075  83.624521\n",
      "\n",
      "10. OPTUNA-ENHANCED WEIGHTED ENSEMBLE SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ensemble Composition (Optuna-optimized models):\n",
      "  Lasso: 88.6%\n",
      "  Huber: 6.2%\n",
      "  Ridge: 5.2%\n",
      "  ElasticNet: 0.0%\n",
      "\n",
      "Expected Performance:\n",
      "  Cross-validation MAE: 2.7234\n",
      "  Expected Kaggle score: ~2.72\n",
      "  Improvement vs best individual Optuna model: 0.0000\n",
      "\n",
      "üöÄ Phase 1 complete with Optuna-optimized models!\n"
     ]
    }
   ],
   "source": [
    "# Train final Optuna-optimized models and generate test predictions\n",
    "print(\"\\n7. TRAINING FINAL OPTUNA-OPTIMIZED ENSEMBLE MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train each Optuna-optimized model on the full training dataset\n",
    "final_models = {}\n",
    "test_predictions = {}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"Training {name} (Optuna-optimized) on full dataset...\")\n",
    "    \n",
    "    # Clone and train the Optuna-optimized model\n",
    "    final_model = model  # Already configured with Optuna parameters\n",
    "    final_model.fit(X_full, y_full)\n",
    "    final_models[name] = final_model\n",
    "    \n",
    "    # Generate test predictions\n",
    "    test_pred = final_model.predict(X_test_final)\n",
    "    test_predictions[name] = test_pred\n",
    "    \n",
    "    print(f\"  Test predictions range: {test_pred.min():.2f} to {test_pred.max():.2f}\")\n",
    "\n",
    "print(f\"\\nAll {len(final_models)} Optuna-optimized models trained successfully!\")\n",
    "\n",
    "# Create test prediction matrix\n",
    "test_matrix = np.column_stack([test_predictions[name] for name in model_names])\n",
    "print(f\"Test prediction matrix shape: {test_matrix.shape}\")\n",
    "\n",
    "print(\"\\n8. GENERATING OPTUNA-ENHANCED ENSEMBLE PREDICTIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Generate weighted ensemble predictions using Optuna-optimized models\n",
    "ensemble_test_pred = np.dot(test_matrix, optimal_weights)\n",
    "\n",
    "print(f\"Optuna-enhanced ensemble test predictions:\")\n",
    "print(f\"  Range: {ensemble_test_pred.min():.2f} to {ensemble_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {ensemble_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {ensemble_test_pred.std():.2f}\")\n",
    "\n",
    "# Compare with individual Optuna-optimized model predictions\n",
    "print(f\"\\nComparison with individual Optuna-optimized models:\")\n",
    "for i, name in enumerate(model_names):\n",
    "    individual_pred = test_predictions[name]\n",
    "    weight = optimal_weights[i]\n",
    "    print(f\"  {name} (weight={weight:.3f}): mean={individual_pred.mean():.2f}, std={individual_pred.std():.2f}\")\n",
    "\n",
    "print(f\"\\n9. CREATING OPTUNA-ENHANCED SUBMISSION FILE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],  # Use the actual ID column from test.csv\n",
    "    'W': ensemble_test_pred\n",
    "})\n",
    "\n",
    "# Generate timestamp for unique filename\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f\"submission_optuna_weighted_ensemble_{timestamp}.csv\"\n",
    "submission_path = SUB_DIR / submission_filename\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Optuna-enhanced submission saved: {submission_filename}\")\n",
    "print(f\"üìÅ Path: {submission_path}\")\n",
    "print(f\"üìä Predictions shape: {submission_df.shape}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(f\"\\n10. OPTUNA-ENHANCED WEIGHTED ENSEMBLE SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Ensemble Composition (Optuna-optimized models):\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {optimal_weights[i]:.1%}\")\n",
    "print(f\"\\nExpected Performance:\")\n",
    "print(f\"  Cross-validation MAE: {optimal_mae:.4f}\")\n",
    "print(f\"  Expected Kaggle score: ~{optimal_mae:.2f}\")\n",
    "print(f\"  Improvement vs best individual Optuna model: {improvement:.4f}\")\n",
    "print(f\"\\nüöÄ Phase 1 complete with Optuna-optimized models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889453b4",
   "metadata": {},
   "source": [
    "# Stacked Ensemble Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc6aaa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "1. CREATING DIVERSE OPTUNA-OPTIMIZED BASE MODELS\n",
      "------------------------------------------------------------\n",
      "Base models for stacking (Optuna-optimized): 11\n",
      "  ‚úÖ Ridge_optuna\n",
      "  ‚úÖ Ridge_conservative\n",
      "  ‚úÖ Ridge_aggressive\n",
      "  ‚úÖ Lasso_optuna\n",
      "  ‚úÖ Lasso_variation\n",
      "  ‚úÖ ElasticNet_optuna\n",
      "  ‚úÖ XGBoost_optuna\n",
      "  ‚úÖ XGBoost_conservative\n",
      "  ‚úÖ CatBoost_optuna\n",
      "  ‚úÖ Best_Linear\n",
      "  ‚úÖ Best_Boosting\n",
      "\n",
      "2. IMPLEMENTING STACKED ENSEMBLE WITH OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Generating Level 1 out-of-fold predictions with Optuna-optimized models...\n",
      "  Processing Ridge_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7264\n",
      "  Processing Ridge_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7530\n",
      "  Processing Ridge_aggressive (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7301\n",
      "  Processing Lasso_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7237\n",
      "  Processing Lasso_variation (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7316\n",
      "  Processing ElasticNet_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7262\n",
      "  Processing XGBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7262\n",
      "  Processing XGBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0556\n",
      "  Processing XGBoost_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0556\n",
      "  Processing XGBoost_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0939\n",
      "  Processing CatBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0939\n",
      "  Processing CatBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0349\n",
      "  Processing Best_Linear (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7235\n",
      "  Processing Best_Boosting (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0349\n",
      "  Processing Best_Linear (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7235\n",
      "  Processing Best_Boosting (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0335\n",
      "\n",
      "Level 1 OOF predictions shape: (1812, 11)\n",
      "Level 1 test predictions shape: (453, 11)\n",
      "üöÄ All base models use Optuna-optimized parameters!\n",
      "    OOF MAE: 3.0335\n",
      "\n",
      "Level 1 OOF predictions shape: (1812, 11)\n",
      "Level 1 test predictions shape: (453, 11)\n",
      "üöÄ All base models use Optuna-optimized parameters!\n"
     ]
    }
   ],
   "source": [
    "print(\"STACKED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"\\n1. CREATING DIVERSE OPTUNA-OPTIMIZED BASE MODELS\") \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create diverse base models using Optuna-optimized parameters for better generalization\n",
    "def create_optuna_linear_model(model_type, params, suffix=\"\"):\n",
    "    \"\"\"Create linear model with Optuna-optimized parameters\"\"\"\n",
    "    if model_type == 'Ridge':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Ridge(alpha=params.get('alpha', 1.0), random_state=42))\n",
    "        ])\n",
    "    elif model_type == 'Lasso':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', Lasso(\n",
    "                alpha=params.get('alpha', 0.01), \n",
    "                max_iter=params.get('max_iter', 10000),\n",
    "                tol=1e-3,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_type == 'ElasticNet':\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', ElasticNet(\n",
    "                alpha=params.get('alpha', 0.1),\n",
    "                l1_ratio=params.get('l1_ratio', 0.5),\n",
    "                max_iter=params.get('max_iter', 10000),\n",
    "                tol=1e-3,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "# Build stacking models using Optuna-optimized parameters\n",
    "stacking_models = {}\n",
    "\n",
    "# Linear models with Optuna-optimized parameters + variations for diversity\n",
    "if 'Ridge' in optimized_linear_params:\n",
    "    ridge_params = optimized_linear_params['Ridge']\n",
    "    # Use optimized alpha and create variations\n",
    "    base_alpha = ridge_params.get('alpha', 1.0)\n",
    "    stacking_models['Ridge_optuna'] = create_optuna_linear_model('Ridge', ridge_params)\n",
    "    stacking_models['Ridge_conservative'] = create_optuna_linear_model('Ridge', {'alpha': base_alpha * 5})\n",
    "    stacking_models['Ridge_aggressive'] = create_optuna_linear_model('Ridge', {'alpha': base_alpha * 0.2})\n",
    "\n",
    "if 'Lasso' in optimized_linear_params:\n",
    "    lasso_params = optimized_linear_params['Lasso']\n",
    "    stacking_models['Lasso_optuna'] = create_optuna_linear_model('Lasso', lasso_params)\n",
    "    # Create variation\n",
    "    base_alpha = lasso_params.get('alpha', 0.01)\n",
    "    stacking_models['Lasso_variation'] = create_optuna_linear_model('Lasso', {\n",
    "        'alpha': base_alpha * 2,\n",
    "        'max_iter': lasso_params.get('max_iter', 10000)\n",
    "    })\n",
    "\n",
    "if 'ElasticNet' in optimized_linear_params:\n",
    "    elasticnet_params = optimized_linear_params['ElasticNet']\n",
    "    stacking_models['ElasticNet_optuna'] = create_optuna_linear_model('ElasticNet', elasticnet_params)\n",
    "\n",
    "# Tree-based models with Optuna-optimized parameters\n",
    "if 'XGBoost' in optimized_params:\n",
    "    xgb_params = optimized_params['XGBoost'].copy()\n",
    "    # Use Optuna parameters but make conservative for stacking\n",
    "    xgb_params['n_estimators'] = min(xgb_params.get('n_estimators', 150), 150)  # Cap for speed\n",
    "    xgb_params['verbosity'] = 0\n",
    "    xgb_params['random_state'] = 42\n",
    "    \n",
    "    # Create XGBoost with Optuna parameters\n",
    "    stacking_models['XGBoost_optuna'] = XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Create conservative variation\n",
    "    conservative_params = xgb_params.copy()\n",
    "    conservative_params['max_depth'] = max(3, xgb_params.get('max_depth', 6) - 1)  # Shallower\n",
    "    conservative_params['learning_rate'] = xgb_params.get('learning_rate', 0.1) * 0.8  # Slower\n",
    "    stacking_models['XGBoost_conservative'] = XGBRegressor(**conservative_params)\n",
    "\n",
    "if 'CatBoost' in optimized_params:\n",
    "    cat_params = optimized_params['CatBoost'].copy()\n",
    "    # Use Optuna parameters but make conservative for stacking\n",
    "    cat_params['iterations'] = min(cat_params.get('iterations', 150), 150)  # Cap for speed\n",
    "    cat_params['verbose'] = False\n",
    "    cat_params['random_seed'] = 42\n",
    "    \n",
    "    stacking_models['CatBoost_optuna'] = CatBoostRegressor(**cat_params)\n",
    "\n",
    "# Add the best individual Optuna-optimized models\n",
    "stacking_models['Best_Linear'] = best_linear_model  # From Optuna optimization\n",
    "stacking_models['Best_Boosting'] = best_model       # From Optuna optimization\n",
    "\n",
    "print(f\"Base models for stacking (Optuna-optimized): {len(stacking_models)}\")\n",
    "for name in stacking_models.keys():\n",
    "    print(f\"  ‚úÖ {name}\")\n",
    "\n",
    "print(f\"\\n2. IMPLEMENTING STACKED ENSEMBLE WITH OPTUNA MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use the same CV folds for all models to ensure consistency  \n",
    "stacking_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Level 1: Generate out-of-fold predictions from Optuna-optimized base models\n",
    "print(\"Generating Level 1 out-of-fold predictions with Optuna-optimized models...\")\n",
    "\n",
    "level1_oof_preds = np.zeros((len(X_full), len(stacking_models)))\n",
    "level1_test_preds = np.zeros((len(X_test_final), len(stacking_models)))\n",
    "\n",
    "model_names_stack = list(stacking_models.keys())\n",
    "\n",
    "for i, (name, model) in enumerate(stacking_models.items()):\n",
    "    print(f\"  Processing {name} (Optuna-enhanced)...\")\n",
    "    \n",
    "    # Generate OOF predictions\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=stacking_cv, method='predict')\n",
    "    level1_oof_preds[:, i] = oof_pred\n",
    "    \n",
    "    # Train on full dataset and predict test set\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_full, y_full)\n",
    "    test_pred = model_clone.predict(X_test_final)\n",
    "    level1_test_preds[:, i] = test_pred\n",
    "    \n",
    "    # Calculate individual model OOF MAE\n",
    "    oof_mae = mean_absolute_error(y_full, oof_pred)\n",
    "    print(f\"    OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nLevel 1 OOF predictions shape: {level1_oof_preds.shape}\")\n",
    "print(f\"Level 1 test predictions shape: {level1_test_preds.shape}\")\n",
    "print(f\"üöÄ All base models use Optuna-optimized parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2700072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1 STACKING MODEL DIVERSITY ANALYSIS\n",
      "--------------------------------------------------\n",
      "Analyzing correlation between stacking models...\n",
      "  Generating predictions for Ridge_optuna...\n",
      "  Generating predictions for Ridge_conservative...\n",
      "  Generating predictions for Ridge_aggressive...\n",
      "  Generating predictions for Lasso_optuna...\n",
      "  Generating predictions for Lasso_variation...\n",
      "  Generating predictions for ElasticNet_optuna...\n",
      "  Generating predictions for XGBoost_optuna...\n",
      "  Generating predictions for XGBoost_conservative...\n",
      "  Generating predictions for XGBoost_conservative...\n",
      "  Generating predictions for CatBoost_optuna...\n",
      "  Generating predictions for CatBoost_optuna...\n",
      "  Generating predictions for Best_Linear...\n",
      "  Generating predictions for Best_Boosting...\n",
      "  Generating predictions for Best_Linear...\n",
      "  Generating predictions for Best_Boosting...\n",
      "\n",
      "Stacking models correlation matrix:\n",
      "------------------------------------------------------------\n",
      "Model                    Ridge_optu  Ridge_cons  Ridge_aggr  Lasso_optu  Lasso_vari  ElasticNet  XGBoost_op  XGBoost_co  \n",
      "Ridge_optuna             1.000       0.999       1.000       1.000       1.000       1.000       0.989       0.989       \n",
      "Ridge_conservative       0.999       1.000       0.999       0.999       1.000       1.000       0.991       0.991       \n",
      "Ridge_aggressive         1.000       0.999       1.000       1.000       0.999       1.000       0.988       0.988       \n",
      "Lasso_optuna             1.000       0.999       1.000       1.000       1.000       1.000       0.990       0.989       \n",
      "Lasso_variation          1.000       1.000       0.999       1.000       1.000       1.000       0.991       0.990       \n",
      "ElasticNet_optuna        1.000       1.000       1.000       1.000       1.000       1.000       0.990       0.990       \n",
      "XGBoost_optuna           0.989       0.991       0.988       0.990       0.991       0.990       1.000       1.000       \n",
      "XGBoost_conservative     0.989       0.991       0.988       0.989       0.990       0.990       1.000       1.000       \n",
      "... (3 more models)\n",
      "\n",
      "‚ö†Ô∏è  Found 55 highly correlated pairs (r > 0.95):\n",
      "  Ridge_optuna ‚Üî Ridge_conservative: r = 0.999\n",
      "    MAE: 2.7264 vs 2.7530\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_optuna ‚Üî Ridge_aggressive: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7301\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_optuna ‚Üî Lasso_optuna: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7237\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Ridge_optuna ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7262\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî XGBoost_optuna: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_optuna ‚Üî XGBoost_conservative: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_optuna ‚Üî CatBoost_optuna: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7235\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî Best_Boosting: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Ridge_conservative ‚Üî Ridge_aggressive: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7301\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Lasso_optuna: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7237\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7530 vs 2.7316\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7530 vs 2.7262\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî XGBoost_optuna: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_conservative ‚Üî XGBoost_conservative: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_conservative ‚Üî CatBoost_optuna: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_conservative ‚Üî Best_Linear: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7235\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Best_Boosting: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Ridge_aggressive ‚Üî Lasso_optuna: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7237\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî Lasso_variation: r = 0.999\n",
      "    MAE: 2.7301 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Ridge_aggressive ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7262\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî XGBoost_optuna: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_aggressive ‚Üî XGBoost_conservative: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_aggressive ‚Üî CatBoost_optuna: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_aggressive ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7235\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî Best_Boosting: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Lasso_optuna ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_optuna ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7262\n",
      "    Removing: ElasticNet_optuna\n",
      "  Lasso_optuna ‚Üî XGBoost_optuna: r = 0.990\n",
      "    MAE: 2.7237 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Lasso_optuna ‚Üî XGBoost_conservative: r = 0.989\n",
      "    MAE: 2.7237 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Lasso_optuna ‚Üî CatBoost_optuna: r = 0.990\n",
      "    MAE: 2.7237 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Lasso_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7235\n",
      "    Removing: Lasso_optuna\n",
      "  Lasso_optuna ‚Üî Best_Boosting: r = 0.989\n",
      "    MAE: 2.7237 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Lasso_variation ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7316 vs 2.7262\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_variation ‚Üî XGBoost_optuna: r = 0.991\n",
      "    MAE: 2.7316 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Lasso_variation ‚Üî XGBoost_conservative: r = 0.990\n",
      "    MAE: 2.7316 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Lasso_variation ‚Üî CatBoost_optuna: r = 0.991\n",
      "    MAE: 2.7316 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Lasso_variation ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7316 vs 2.7235\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_variation ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7316 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  ElasticNet_optuna ‚Üî XGBoost_optuna: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  ElasticNet_optuna ‚Üî XGBoost_conservative: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  ElasticNet_optuna ‚Üî CatBoost_optuna: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  ElasticNet_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7262 vs 2.7235\n",
      "    Removing: ElasticNet_optuna\n",
      "  ElasticNet_optuna ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  XGBoost_optuna ‚Üî XGBoost_conservative: r = 1.000\n",
      "    MAE: 3.0556 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_optuna ‚Üî CatBoost_optuna: r = 0.998\n",
      "    MAE: 3.0556 vs 3.0349\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_optuna ‚Üî Best_Linear: r = 0.990\n",
      "    MAE: 3.0556 vs 2.7235\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_optuna ‚Üî Best_Boosting: r = 0.998\n",
      "    MAE: 3.0556 vs 3.0335\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_conservative ‚Üî CatBoost_optuna: r = 0.998\n",
      "    MAE: 3.0939 vs 3.0349\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_conservative ‚Üî Best_Linear: r = 0.989\n",
      "    MAE: 3.0939 vs 2.7235\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_conservative ‚Üî Best_Boosting: r = 0.998\n",
      "    MAE: 3.0939 vs 3.0335\n",
      "    Removing: XGBoost_conservative\n",
      "  CatBoost_optuna ‚Üî Best_Linear: r = 0.990\n",
      "    MAE: 3.0349 vs 2.7235\n",
      "    Removing: CatBoost_optuna\n",
      "  CatBoost_optuna ‚Üî Best_Boosting: r = 0.999\n",
      "    MAE: 3.0349 vs 3.0335\n",
      "    Removing: CatBoost_optuna\n",
      "  Best_Linear ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7235 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "    üóëÔ∏è  Removing ElasticNet_optuna\n",
      "    üóëÔ∏è  Removing Lasso_optuna\n",
      "    üóëÔ∏è  Removing CatBoost_optuna\n",
      "    üóëÔ∏è  Removing XGBoost_conservative\n",
      "    üóëÔ∏è  Removing XGBoost_optuna\n",
      "    üóëÔ∏è  Removing Lasso_variation\n",
      "    üóëÔ∏è  Removing Ridge_aggressive\n",
      "    üóëÔ∏è  Removing Best_Boosting\n",
      "    üóëÔ∏è  Removing Ridge_optuna\n",
      "    üóëÔ∏è  Removing Ridge_conservative\n",
      "\n",
      "üìä Stacking models after correlation filtering: 1\n",
      "\n",
      "üìã Final stacking models for ensemble:\n",
      "   1. Best_Linear (MAE: 2.7235)\n",
      "\n",
      "üéØ Ready for stacking with 1 diverse base models!\n",
      "\n",
      "Stacking models correlation matrix:\n",
      "------------------------------------------------------------\n",
      "Model                    Ridge_optu  Ridge_cons  Ridge_aggr  Lasso_optu  Lasso_vari  ElasticNet  XGBoost_op  XGBoost_co  \n",
      "Ridge_optuna             1.000       0.999       1.000       1.000       1.000       1.000       0.989       0.989       \n",
      "Ridge_conservative       0.999       1.000       0.999       0.999       1.000       1.000       0.991       0.991       \n",
      "Ridge_aggressive         1.000       0.999       1.000       1.000       0.999       1.000       0.988       0.988       \n",
      "Lasso_optuna             1.000       0.999       1.000       1.000       1.000       1.000       0.990       0.989       \n",
      "Lasso_variation          1.000       1.000       0.999       1.000       1.000       1.000       0.991       0.990       \n",
      "ElasticNet_optuna        1.000       1.000       1.000       1.000       1.000       1.000       0.990       0.990       \n",
      "XGBoost_optuna           0.989       0.991       0.988       0.990       0.991       0.990       1.000       1.000       \n",
      "XGBoost_conservative     0.989       0.991       0.988       0.989       0.990       0.990       1.000       1.000       \n",
      "... (3 more models)\n",
      "\n",
      "‚ö†Ô∏è  Found 55 highly correlated pairs (r > 0.95):\n",
      "  Ridge_optuna ‚Üî Ridge_conservative: r = 0.999\n",
      "    MAE: 2.7264 vs 2.7530\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_optuna ‚Üî Ridge_aggressive: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7301\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_optuna ‚Üî Lasso_optuna: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7237\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Ridge_optuna ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7262\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî XGBoost_optuna: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_optuna ‚Üî XGBoost_conservative: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_optuna ‚Üî CatBoost_optuna: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7264 vs 2.7235\n",
      "    Removing: Ridge_optuna\n",
      "  Ridge_optuna ‚Üî Best_Boosting: r = 0.989\n",
      "    MAE: 2.7264 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Ridge_conservative ‚Üî Ridge_aggressive: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7301\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Lasso_optuna: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7237\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7530 vs 2.7316\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7530 vs 2.7262\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî XGBoost_optuna: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_conservative ‚Üî XGBoost_conservative: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_conservative ‚Üî CatBoost_optuna: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_conservative ‚Üî Best_Linear: r = 0.999\n",
      "    MAE: 2.7530 vs 2.7235\n",
      "    Removing: Ridge_conservative\n",
      "  Ridge_conservative ‚Üî Best_Boosting: r = 0.991\n",
      "    MAE: 2.7530 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Ridge_aggressive ‚Üî Lasso_optuna: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7237\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî Lasso_variation: r = 0.999\n",
      "    MAE: 2.7301 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Ridge_aggressive ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7262\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî XGBoost_optuna: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Ridge_aggressive ‚Üî XGBoost_conservative: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Ridge_aggressive ‚Üî CatBoost_optuna: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Ridge_aggressive ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7301 vs 2.7235\n",
      "    Removing: Ridge_aggressive\n",
      "  Ridge_aggressive ‚Üî Best_Boosting: r = 0.988\n",
      "    MAE: 2.7301 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Lasso_optuna ‚Üî Lasso_variation: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7316\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_optuna ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7262\n",
      "    Removing: ElasticNet_optuna\n",
      "  Lasso_optuna ‚Üî XGBoost_optuna: r = 0.990\n",
      "    MAE: 2.7237 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Lasso_optuna ‚Üî XGBoost_conservative: r = 0.989\n",
      "    MAE: 2.7237 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Lasso_optuna ‚Üî CatBoost_optuna: r = 0.990\n",
      "    MAE: 2.7237 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Lasso_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7237 vs 2.7235\n",
      "    Removing: Lasso_optuna\n",
      "  Lasso_optuna ‚Üî Best_Boosting: r = 0.989\n",
      "    MAE: 2.7237 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  Lasso_variation ‚Üî ElasticNet_optuna: r = 1.000\n",
      "    MAE: 2.7316 vs 2.7262\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_variation ‚Üî XGBoost_optuna: r = 0.991\n",
      "    MAE: 2.7316 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  Lasso_variation ‚Üî XGBoost_conservative: r = 0.990\n",
      "    MAE: 2.7316 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  Lasso_variation ‚Üî CatBoost_optuna: r = 0.991\n",
      "    MAE: 2.7316 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  Lasso_variation ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7316 vs 2.7235\n",
      "    Removing: Lasso_variation\n",
      "  Lasso_variation ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7316 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  ElasticNet_optuna ‚Üî XGBoost_optuna: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0556\n",
      "    Removing: XGBoost_optuna\n",
      "  ElasticNet_optuna ‚Üî XGBoost_conservative: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  ElasticNet_optuna ‚Üî CatBoost_optuna: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0349\n",
      "    Removing: CatBoost_optuna\n",
      "  ElasticNet_optuna ‚Üî Best_Linear: r = 1.000\n",
      "    MAE: 2.7262 vs 2.7235\n",
      "    Removing: ElasticNet_optuna\n",
      "  ElasticNet_optuna ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7262 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "  XGBoost_optuna ‚Üî XGBoost_conservative: r = 1.000\n",
      "    MAE: 3.0556 vs 3.0939\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_optuna ‚Üî CatBoost_optuna: r = 0.998\n",
      "    MAE: 3.0556 vs 3.0349\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_optuna ‚Üî Best_Linear: r = 0.990\n",
      "    MAE: 3.0556 vs 2.7235\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_optuna ‚Üî Best_Boosting: r = 0.998\n",
      "    MAE: 3.0556 vs 3.0335\n",
      "    Removing: XGBoost_optuna\n",
      "  XGBoost_conservative ‚Üî CatBoost_optuna: r = 0.998\n",
      "    MAE: 3.0939 vs 3.0349\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_conservative ‚Üî Best_Linear: r = 0.989\n",
      "    MAE: 3.0939 vs 2.7235\n",
      "    Removing: XGBoost_conservative\n",
      "  XGBoost_conservative ‚Üî Best_Boosting: r = 0.998\n",
      "    MAE: 3.0939 vs 3.0335\n",
      "    Removing: XGBoost_conservative\n",
      "  CatBoost_optuna ‚Üî Best_Linear: r = 0.990\n",
      "    MAE: 3.0349 vs 2.7235\n",
      "    Removing: CatBoost_optuna\n",
      "  CatBoost_optuna ‚Üî Best_Boosting: r = 0.999\n",
      "    MAE: 3.0349 vs 3.0335\n",
      "    Removing: CatBoost_optuna\n",
      "  Best_Linear ‚Üî Best_Boosting: r = 0.990\n",
      "    MAE: 2.7235 vs 3.0335\n",
      "    Removing: Best_Boosting\n",
      "    üóëÔ∏è  Removing ElasticNet_optuna\n",
      "    üóëÔ∏è  Removing Lasso_optuna\n",
      "    üóëÔ∏è  Removing CatBoost_optuna\n",
      "    üóëÔ∏è  Removing XGBoost_conservative\n",
      "    üóëÔ∏è  Removing XGBoost_optuna\n",
      "    üóëÔ∏è  Removing Lasso_variation\n",
      "    üóëÔ∏è  Removing Ridge_aggressive\n",
      "    üóëÔ∏è  Removing Best_Boosting\n",
      "    üóëÔ∏è  Removing Ridge_optuna\n",
      "    üóëÔ∏è  Removing Ridge_conservative\n",
      "\n",
      "üìä Stacking models after correlation filtering: 1\n",
      "\n",
      "üìã Final stacking models for ensemble:\n",
      "   1. Best_Linear (MAE: 2.7235)\n",
      "\n",
      "üéØ Ready for stacking with 1 diverse base models!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n1.1 STACKING MODEL DIVERSITY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze diversity of stacking models before implementing the ensemble\n",
    "print(\"Analyzing correlation between stacking models...\")\n",
    "\n",
    "stacking_predictions = {}\n",
    "temp_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate predictions for correlation analysis\n",
    "for name, model in stacking_models.items():\n",
    "    print(f\"  Generating predictions for {name}...\")\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=temp_cv, method='predict')\n",
    "    stacking_predictions[name] = oof_pred\n",
    "\n",
    "# Calculate correlation matrix for stacking models\n",
    "stacking_models_list = list(stacking_predictions.keys())\n",
    "n_models = len(stacking_models_list)\n",
    "correlation_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i, model1 in enumerate(stacking_models_list):\n",
    "    for j, model2 in enumerate(stacking_models_list):\n",
    "        correlation_matrix[i, j] = np.corrcoef(stacking_predictions[model1], \n",
    "                                             stacking_predictions[model2])[0, 1]\n",
    "\n",
    "print(f\"\\nStacking models correlation matrix:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display correlation matrix (truncated for readability)\n",
    "max_display = min(8, len(stacking_models_list))  # Show max 8 models for space\n",
    "print(f\"{'Model':<25}\", end=\"\")\n",
    "for i in range(max_display):\n",
    "    model_name = stacking_models_list[i][:10]  # Truncate long names\n",
    "    print(f\"{model_name:<12}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, model1 in enumerate(stacking_models_list):\n",
    "    if i >= max_display:\n",
    "        break\n",
    "    model_name = model1[:24]  # Truncate long names\n",
    "    print(f\"{model_name:<25}\", end=\"\")\n",
    "    for j in range(max_display):\n",
    "        print(f\"{correlation_matrix[i, j]:.3f}       \", end=\"\")\n",
    "    print()\n",
    "\n",
    "if len(stacking_models_list) > max_display:\n",
    "    print(f\"... ({len(stacking_models_list) - max_display} more models)\")\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "high_corr_threshold = 0.95  # Very high threshold for stacking (meta-learner can handle some correlation)\n",
    "correlated_pairs = []\n",
    "\n",
    "for i, model1 in enumerate(stacking_models_list):\n",
    "    for j, model2 in enumerate(stacking_models_list[i+1:], i+1):\n",
    "        correlation = correlation_matrix[i, j]\n",
    "        if correlation > high_corr_threshold:\n",
    "            mae1 = mean_absolute_error(y_full, stacking_predictions[model1])\n",
    "            mae2 = mean_absolute_error(y_full, stacking_predictions[model2])\n",
    "            correlated_pairs.append({\n",
    "                'model1': model1,\n",
    "                'model2': model2,\n",
    "                'correlation': correlation,\n",
    "                'mae1': mae1,\n",
    "                'mae2': mae2,\n",
    "                'worse_model': model1 if mae1 > mae2 else model2\n",
    "            })\n",
    "\n",
    "if correlated_pairs:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(correlated_pairs)} highly correlated pairs (r > {high_corr_threshold}):\")\n",
    "    models_to_remove = set()\n",
    "    \n",
    "    for pair in correlated_pairs:\n",
    "        print(f\"  {pair['model1']} ‚Üî {pair['model2']}: r = {pair['correlation']:.3f}\")\n",
    "        print(f\"    MAE: {pair['mae1']:.4f} vs {pair['mae2']:.4f}\")\n",
    "        print(f\"    Removing: {pair['worse_model']}\")\n",
    "        models_to_remove.add(pair['worse_model'])\n",
    "    \n",
    "    # Remove highly correlated models\n",
    "    for model in models_to_remove:\n",
    "        print(f\"    üóëÔ∏è  Removing {model}\")\n",
    "        del stacking_models[model]\n",
    "    \n",
    "    print(f\"\\nüìä Stacking models after correlation filtering: {len(stacking_models)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All stacking models have acceptable diversity (r ‚â§ {high_corr_threshold})\")\n",
    "\n",
    "# Calculate average correlation of remaining models\n",
    "remaining_models = list(stacking_models.keys())\n",
    "if len(remaining_models) > 1:\n",
    "    avg_correlations = []\n",
    "    for i, model1 in enumerate(remaining_models):\n",
    "        for j, model2 in enumerate(remaining_models[i+1:], i+1):\n",
    "            model1_idx = stacking_models_list.index(model1)\n",
    "            model2_idx = stacking_models_list.index(model2)\n",
    "            avg_correlations.append(abs(correlation_matrix[model1_idx, model2_idx]))\n",
    "    \n",
    "    mean_correlation = np.mean(avg_correlations)\n",
    "    print(f\"Average absolute correlation between remaining models: {mean_correlation:.3f}\")\n",
    "    \n",
    "    if mean_correlation < 0.3:\n",
    "        diversity_status = \"üü¢ Excellent diversity\"\n",
    "    elif mean_correlation < 0.6:\n",
    "        diversity_status = \"üü° Good diversity\"\n",
    "    else:\n",
    "        diversity_status = \"üü† Moderate diversity\"\n",
    "    \n",
    "    print(f\"Diversity assessment: {diversity_status}\")\n",
    "\n",
    "print(f\"\\nüìã Final stacking models for ensemble:\")\n",
    "for i, name in enumerate(stacking_models.keys(), 1):\n",
    "    mae = mean_absolute_error(y_full, stacking_predictions[name]) if name in stacking_predictions else \"N/A\"\n",
    "    print(f\"  {i:2d}. {name} (MAE: {mae:.4f})\" if mae != \"N/A\" else f\"  {i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for stacking with {len(stacking_models)} diverse base models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "686dd107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. IMPLEMENTING STACKED ENSEMBLE WITH FILTERED MODELS\n",
      "------------------------------------------------------------\n",
      "Generating Level 1 out-of-fold predictions with filtered diverse models...\n",
      "  Processing Best_Linear (Optuna-enhanced, diversity-filtered)...\n",
      "    OOF MAE: 2.7235\n",
      "\n",
      "Level 1 predictions generated:\n",
      "  OOF matrix shape: (1812, 1)\n",
      "  Test matrix shape: (453, 1)\n",
      "  Using 1 diverse base models\n",
      "\n",
      "Level 1 prediction diversity check:\n",
      "  ‚ö†Ô∏è Only 1 model available - no correlation analysis needed\n",
      "  Diversity status: üü° Single model - no diversity analysis\n",
      "\n",
      "‚úÖ Level 1 complete - Ready for meta-learner training!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n2. IMPLEMENTING STACKED ENSEMBLE WITH FILTERED MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Use the same CV folds for all models to ensure consistency  \n",
    "stacking_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Level 1: Generate out-of-fold predictions from filtered Optuna-optimized base models\n",
    "print(\"Generating Level 1 out-of-fold predictions with filtered diverse models...\")\n",
    "\n",
    "level1_oof_preds = np.zeros((len(X_full), len(stacking_models)))\n",
    "level1_test_preds = np.zeros((len(X_test_final), len(stacking_models)))\n",
    "\n",
    "model_names_stack = list(stacking_models.keys())\n",
    "\n",
    "for i, (name, model) in enumerate(stacking_models.items()):\n",
    "    print(f\"  Processing {name} (Optuna-enhanced, diversity-filtered)...\")\n",
    "    \n",
    "    # Generate OOF predictions\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=stacking_cv, method='predict')\n",
    "    level1_oof_preds[:, i] = oof_pred\n",
    "    \n",
    "    # Train on full dataset and predict test set\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_full, y_full)\n",
    "    test_pred = model_clone.predict(X_test_final)\n",
    "    level1_test_preds[:, i] = test_pred\n",
    "    \n",
    "    # Calculate individual model OOF MAE\n",
    "    oof_mae = mean_absolute_error(y_full, oof_pred)\n",
    "    print(f\"    OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nLevel 1 predictions generated:\")\n",
    "print(f\"  OOF matrix shape: {level1_oof_preds.shape}\")\n",
    "print(f\"  Test matrix shape: {level1_test_preds.shape}\")\n",
    "print(f\"  Using {len(model_names_stack)} diverse base models\")\n",
    "\n",
    "# Calculate correlation of Level 1 predictions to verify diversity\n",
    "print(f\"\\nLevel 1 prediction diversity check:\")\n",
    "\n",
    "if len(model_names_stack) < 2:\n",
    "    print(f\"  ‚ö†Ô∏è Only {len(model_names_stack)} model available - no correlation analysis needed\")\n",
    "    avg_level1_correlation = 0.0\n",
    "    diversity_status = \"üü° Single model - no diversity analysis\"\n",
    "else:\n",
    "    level1_correlations = np.corrcoef(level1_oof_preds.T)\n",
    "    \n",
    "    # Handle case where corrcoef returns a scalar (shouldn't happen with >= 2 models, but safety check)\n",
    "    if level1_correlations.ndim == 0:\n",
    "        avg_level1_correlation = float(level1_correlations)\n",
    "    else:\n",
    "        upper_tri_indices = np.triu_indices_from(level1_correlations, k=1)\n",
    "        avg_level1_correlation = np.mean(np.abs(level1_correlations[upper_tri_indices]))\n",
    "    \n",
    "    print(f\"  Average absolute correlation: {avg_level1_correlation:.3f}\")\n",
    "    \n",
    "    if avg_level1_correlation < 0.3:\n",
    "        diversity_status = \"üü¢ Excellent diversity for stacking\"\n",
    "    elif avg_level1_correlation < 0.6:\n",
    "        diversity_status = \"üü° Good diversity for stacking\"\n",
    "    else:\n",
    "        diversity_status = \"üü† Moderate diversity for stacking\"\n",
    "\n",
    "print(f\"  Diversity status: {diversity_status}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Level 1 complete - Ready for meta-learner training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54168822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. TRAINING LEVEL 2 META-LEARNER (OPTUNA-ENHANCED)\n",
      "------------------------------------------------------------\n",
      "Evaluating Optuna-enhanced meta-learners:\n",
      "  üöÄ Ridge_meta_optuna: MAE = 2.7240 (¬±0.0500)\n",
      "  üìä Ridge_meta_conservative: MAE = 2.7240 (¬±0.0500)\n",
      "  üìä Ridge_meta_aggressive: MAE = 2.7240 (¬±0.0500)\n",
      "  üöÄ Lasso_meta_optuna: MAE = 2.7240 (¬±0.0500)\n",
      "  üöÄ ElasticNet_meta_optuna: MAE = 2.7240 (¬±0.0500)\n",
      "  üìä Ridge_meta_standard: MAE = 2.7240 (¬±0.0500)\n",
      "  üìä LinearRegression_meta: MAE = 2.7240 (¬±0.0500)\n",
      "\n",
      "üèÜ Best meta-learner: Ridge_meta_conservative\n",
      "Best meta-learner CV MAE: 2.7240\n",
      "Uses Optuna optimization: ‚ùå No\n",
      "\n",
      "4. TRAINING FINAL OPTUNA-ENHANCED STACKED MODEL\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced stacked ensemble test predictions:\n",
      "  Range: 44.84 to 109.48\n",
      "  Mean: 79.06\n",
      "  Std: 12.04\n",
      "\n",
      "5. COMPARISON WITH PHASE 1 OPTUNA ENSEMBLE\n",
      "------------------------------------------------------------\n",
      "Phase 1 Optuna ensemble predictions:\n",
      "  Range: 44.81 to 109.51\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Phase 2 Optuna stacked predictions:\n",
      "  Range: 44.84 to 109.48\n",
      "  Mean: 79.06\n",
      "  Std: 12.04\n",
      "\n",
      "Correlation between Phase 1 and Phase 2 Optuna ensembles: 1.0000\n",
      "\n",
      "Phase 2 (Optuna-enhanced stacked ensemble):\n",
      "  CV MAE: 2.7240\n",
      "  Expected improvement vs Phase 1: -0.0006\n",
      "  ‚ö†Ô∏è Phase 2 CV did not improve Phase 1\n",
      "  üìä Both benefit from Optuna optimization\n",
      "\n",
      "üéØ Both ensembles now use Optuna-optimized models!\n"
     ]
    }
   ],
   "source": [
    "# Level 2: Train meta-learner with Optuna-enhanced parameters\n",
    "print(f\"\\n3. TRAINING LEVEL 2 META-LEARNER (OPTUNA-ENHANCED)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use Optuna-optimized parameters for meta-learners too\n",
    "meta_learners = {}\n",
    "\n",
    "# Create meta-learners using Optuna-optimized parameters when available\n",
    "if 'Ridge' in optimized_linear_params:\n",
    "    ridge_alpha = optimized_linear_params['Ridge'].get('alpha', 1.0)\n",
    "    meta_learners['Ridge_meta_optuna'] = Ridge(alpha=ridge_alpha)\n",
    "    meta_learners['Ridge_meta_conservative'] = Ridge(alpha=ridge_alpha * 10)  # More regularized\n",
    "    meta_learners['Ridge_meta_aggressive'] = Ridge(alpha=ridge_alpha * 0.1)   # Less regularized\n",
    "\n",
    "if 'Lasso' in optimized_linear_params:\n",
    "    lasso_alpha = optimized_linear_params['Lasso'].get('alpha', 0.01)\n",
    "    lasso_max_iter = optimized_linear_params['Lasso'].get('max_iter', 10000)\n",
    "    meta_learners['Lasso_meta_optuna'] = Lasso(alpha=lasso_alpha, max_iter=lasso_max_iter, tol=1e-3)\n",
    "\n",
    "if 'ElasticNet' in optimized_linear_params:\n",
    "    en_alpha = optimized_linear_params['ElasticNet'].get('alpha', 0.1)\n",
    "    en_l1_ratio = optimized_linear_params['ElasticNet'].get('l1_ratio', 0.5)\n",
    "    en_max_iter = optimized_linear_params['ElasticNet'].get('max_iter', 10000)\n",
    "    meta_learners['ElasticNet_meta_optuna'] = ElasticNet(\n",
    "        alpha=en_alpha, l1_ratio=en_l1_ratio, max_iter=en_max_iter, tol=1e-3\n",
    "    )\n",
    "\n",
    "# Add some standard options for comparison\n",
    "meta_learners['Ridge_meta_standard'] = Ridge(alpha=1.0)\n",
    "meta_learners['LinearRegression_meta'] = LinearRegression()\n",
    "\n",
    "best_meta_mae = float('inf')\n",
    "best_meta_name = None\n",
    "best_meta_model = None\n",
    "\n",
    "print(\"Evaluating Optuna-enhanced meta-learners:\")\n",
    "for name, meta_model in meta_learners.items():\n",
    "    # Cross-validate the meta-learner on OOF predictions\n",
    "    meta_cv_scores = cross_val_score(\n",
    "        meta_model, level1_oof_preds, y_full,\n",
    "        cv=5, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    meta_mae = -meta_cv_scores.mean()\n",
    "    meta_mae_std = meta_cv_scores.std()\n",
    "    \n",
    "    optuna_flag = \"üöÄ\" if \"optuna\" in name else \"üìä\"\n",
    "    print(f\"  {optuna_flag} {name}: MAE = {meta_mae:.4f} (¬±{meta_mae_std:.4f})\")\n",
    "    \n",
    "    if meta_mae < best_meta_mae:\n",
    "        best_meta_mae = meta_mae\n",
    "        best_meta_name = name\n",
    "        best_meta_model = meta_model\n",
    "\n",
    "print(f\"\\nüèÜ Best meta-learner: {best_meta_name}\")\n",
    "print(f\"Best meta-learner CV MAE: {best_meta_mae:.4f}\")\n",
    "is_optuna_meta = \"optuna\" in best_meta_name\n",
    "print(f\"Uses Optuna optimization: {'‚úÖ Yes' if is_optuna_meta else '‚ùå No'}\")\n",
    "\n",
    "# Train the best meta-learner on all OOF predictions\n",
    "print(f\"\\n4. TRAINING FINAL OPTUNA-ENHANCED STACKED MODEL\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "final_meta_model = clone(best_meta_model)\n",
    "final_meta_model.fit(level1_oof_preds, y_full)\n",
    "\n",
    "# Generate final stacked predictions\n",
    "stacked_test_pred = final_meta_model.predict(level1_test_preds)\n",
    "\n",
    "print(f\"Optuna-enhanced stacked ensemble test predictions:\")\n",
    "print(f\"  Range: {stacked_test_pred.min():.2f} to {stacked_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {stacked_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {stacked_test_pred.std():.2f}\")\n",
    "\n",
    "# Compare with Phase 1 Optuna ensemble\n",
    "print(f\"\\n5. COMPARISON WITH PHASE 1 OPTUNA ENSEMBLE\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Phase 1 Optuna ensemble predictions:\")\n",
    "print(f\"  Range: {ensemble_test_pred.min():.2f} to {ensemble_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {ensemble_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {ensemble_test_pred.std():.2f}\")\n",
    "\n",
    "print(f\"\\nPhase 2 Optuna stacked predictions:\")\n",
    "print(f\"  Range: {stacked_test_pred.min():.2f} to {stacked_test_pred.max():.2f}\")  \n",
    "print(f\"  Mean: {stacked_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {stacked_test_pred.std():.2f}\")\n",
    "\n",
    "# Calculate correlation between Phase 1 and Phase 2 predictions\n",
    "correlation = np.corrcoef(ensemble_test_pred, stacked_test_pred)[0, 1]\n",
    "print(f\"\\nCorrelation between Phase 1 and Phase 2 Optuna ensembles: {correlation:.4f}\")\n",
    "\n",
    "print(f\"\\nPhase 2 (Optuna-enhanced stacked ensemble):\")\n",
    "print(f\"  CV MAE: {best_meta_mae:.4f}\")\n",
    "improvement_vs_phase1 = optimal_mae - best_meta_mae\n",
    "print(f\"  Expected improvement vs Phase 1: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "if best_meta_mae < optimal_mae:\n",
    "    print(f\"  ‚úÖ Phase 2 shows improvement over Phase 1!\")\n",
    "    print(f\"  üöÄ Optuna optimization helped both phases!\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Phase 2 CV did not improve Phase 1\")\n",
    "    print(f\"  üìä Both benefit from Optuna optimization\")\n",
    "    \n",
    "print(f\"\\nüéØ Both ensembles now use Optuna-optimized models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb2464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. CREATING OPTUNA-ENHANCED STACKED ENSEMBLE SUBMISSION\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Optuna-enhanced stacked ensemble submission saved: submission_optuna_stacked_ensemble_20251005_111103.csv\n",
      "üìÅ Path: /home/chrisfkh/sctp-ds-ai/mod3/kaggle_moneyball/submissions/submission_optuna_stacked_ensemble_20251005_111103.csv\n",
      "üìä Predictions shape: (453, 2)\n",
      "\n",
      "First 10 predictions:\n",
      "     ID          W\n",
      "0  1756  69.289646\n",
      "1  1282  74.423262\n",
      "2   351  84.325458\n",
      "3   421  87.096260\n",
      "4    57  93.275050\n",
      "5  1557  97.585979\n",
      "6   846  79.219717\n",
      "7  1658  84.070777\n",
      "8   112  72.882226\n",
      "9  2075  83.601103\n",
      "\n",
      "7. OPTUNA-ENHANCED STACKED ENSEMBLE SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "Base Models (all Optuna-optimized): 1\n",
      "  üöÄ Best_Linear\n",
      "\n",
      "Meta-learner: Ridge_meta_conservative\n",
      "Meta-learner status: üìä Standard parameters\n",
      "\n",
      "Expected Performance:\n",
      "  CV MAE: 2.7240\n",
      "  Expected Kaggle improvement vs Phase 1: -0.0006\n",
      "\n",
      "üéØ PHASE COMPARISON SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "Phase 1 (Weighted): MAE = 2.7234 | Correlation: 1.000\n",
      "Phase 2 (Stacked):  MAE = 2.7240 | Improvement: -0.0006\n",
      "üèÜ Current leader: Phase 1 (Weighted)\n",
      "\n",
      "‚ú® Both phases now leverage full Optuna optimization!\n",
      "üöÄ Ready to submit the best performing ensemble to Kaggle!\n",
      "\n",
      "üìà OPTIMIZATION JOURNEY\n",
      "----------------------------------------------------------------------\n",
      "1. ‚úÖ Boosting models optimized with Optuna\n",
      "2. ‚úÖ Linear models optimized with Optuna\n",
      "3. ‚úÖ Phase 1 ensemble uses Optuna models\n",
      "4. ‚úÖ Phase 2 ensemble uses Optuna models + meta-learner\n",
      "5. üéØ Ready for Kaggle submission with optimized ensembles!\n"
     ]
    }
   ],
   "source": [
    "# Create Optuna-enhanced stacked ensemble submission\n",
    "print(f\"\\n6. CREATING OPTUNA-ENHANCED STACKED ENSEMBLE SUBMISSION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create submission DataFrame for Optuna-enhanced stacked ensemble\n",
    "stacked_submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'W': stacked_test_pred\n",
    "})\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp_stacked = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "stacked_submission_filename = f\"submission_optuna_stacked_ensemble_{timestamp_stacked}.csv\"\n",
    "stacked_submission_path = SUB_DIR / stacked_submission_filename\n",
    "\n",
    "# Save submission\n",
    "stacked_submission_df.to_csv(stacked_submission_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Optuna-enhanced stacked ensemble submission saved: {stacked_submission_filename}\")\n",
    "print(f\"üìÅ Path: {stacked_submission_path}\")\n",
    "print(f\"üìä Predictions shape: {stacked_submission_df.shape}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(stacked_submission_df.head(10))\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(f\"\\n7. OPTUNA-ENHANCED STACKED ENSEMBLE SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Base Models (all Optuna-optimized): {len(stacking_models)}\")\n",
    "for name in model_names_stack:\n",
    "    optuna_flag = \"üöÄ\" if any(x in name.lower() for x in ['optuna', 'best']) else \"üìä\"\n",
    "    print(f\"  {optuna_flag} {name}\")\n",
    "\n",
    "print(f\"\\nMeta-learner: {best_meta_name}\")\n",
    "meta_optuna_status = \"üöÄ Uses Optuna optimization\" if \"optuna\" in best_meta_name else \"üìä Standard parameters\"\n",
    "print(f\"Meta-learner status: {meta_optuna_status}\")\n",
    "\n",
    "print(f\"\\nExpected Performance:\")\n",
    "print(f\"  CV MAE: {best_meta_mae:.4f}\")\n",
    "print(f\"  Expected Kaggle improvement vs Phase 1: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ PHASE COMPARISON SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Phase 1 (Weighted): MAE = {optimal_mae:.4f} | Correlation: {correlation:.3f}\")\n",
    "print(f\"Phase 2 (Stacked):  MAE = {best_meta_mae:.4f} | Improvement: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "winner = \"Phase 2 (Stacked)\" if best_meta_mae < optimal_mae else \"Phase 1 (Weighted)\"\n",
    "print(f\"üèÜ Current leader: {winner}\")\n",
    "\n",
    "print(f\"\\n‚ú® Both phases now leverage full Optuna optimization!\")\n",
    "print(f\"üöÄ Ready to submit the best performing ensemble to Kaggle!\")\n",
    "\n",
    "# Show the optimization journey\n",
    "print(f\"\\nüìà OPTIMIZATION JOURNEY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"1. ‚úÖ Boosting models optimized with Optuna\")\n",
    "print(f\"2. ‚úÖ Linear models optimized with Optuna\")  \n",
    "print(f\"3. ‚úÖ Phase 1 ensemble uses Optuna models\")\n",
    "print(f\"4. ‚úÖ Phase 2 ensemble uses Optuna models + meta-learner\")\n",
    "print(f\"5. üéØ Ready for Kaggle submission with optimized ensembles!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

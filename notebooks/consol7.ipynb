{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869c9052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1812, 51)\n",
      "Test set shape: (453, 45)\n",
      "'W' column in train dataset: True\n",
      "'W' column in test dataset: False\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Build robust path to data folder (notebooks and data are siblings)\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "SUB_DIR = Path.cwd().parent / 'submissions'\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)  # This is for final predictions (no 'W' column)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"'W' column in train dataset: {'W' in train_df.columns}\")\n",
    "print(f\"'W' column in test dataset: {'W' in test_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6ffd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform analysis for outlier detection and perform outlier handling\n",
    "\n",
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "\n",
    "# print(\"COMPREHENSIVE OUTLIER ANALYSIS AND HANDLING\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # Get numerical columns but exclude categorical/engineered features that shouldn't be treated as numerical\n",
    "# numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# # Exclude target, ID, and categorical/engineered columns\n",
    "# exclude_cols = ['W', 'ID', 'yearID', 'year_label', 'decade_label', 'win_bins']\n",
    "# numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "# print(f\"Excluding from outlier analysis: {exclude_cols}\")\n",
    "# print(f\"Remaining numerical features for outlier analysis: {len(numerical_cols)}\")\n",
    "\n",
    "# # 1. Identify outliers using multiple methods\n",
    "# print(f\"\\n1. OUTLIER DETECTION ON {len(numerical_cols)} FEATURES\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# outlier_summary = {}\n",
    "# for col in numerical_cols:\n",
    "#     if col in train_df.columns and train_df[col].std() > 0:\n",
    "#         # IQR method\n",
    "#         Q1 = train_df[col].quantile(0.25)\n",
    "#         Q3 = train_df[col].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - 1.5 * IQR\n",
    "#         upper_bound = Q3 + 1.5 * IQR\n",
    "#         iqr_outliers = ((train_df[col] < lower_bound) | (train_df[col] > upper_bound)).sum()\n",
    "        \n",
    "#         # Z-score method  \n",
    "#         z_scores = np.abs(stats.zscore(train_df[col].dropna()))\n",
    "#         z_outliers = (z_scores > 3).sum()\n",
    "        \n",
    "#         outlier_summary[col] = {\n",
    "#             'iqr_outliers': iqr_outliers,\n",
    "#             'z_outliers': z_outliers,\n",
    "#             'outlier_rate': max(iqr_outliers, z_outliers) / len(train_df)\n",
    "#         }\n",
    "\n",
    "# # Sort by outlier rate\n",
    "# sorted_outliers = sorted(outlier_summary.items(), \n",
    "#                         key=lambda x: x[1]['outlier_rate'], reverse=True)\n",
    "\n",
    "# print(\"Top 10 features with highest outlier rates:\")\n",
    "# for col, stats_dict in sorted_outliers[:10]:\n",
    "#     rate = stats_dict['outlier_rate']\n",
    "#     iqr_count = stats_dict['iqr_outliers'] \n",
    "#     z_count = stats_dict['z_outliers']\n",
    "#     print(f\"  {col:>15}: {rate*100:.1f}% (IQR: {iqr_count}, Z-score: {z_count})\")\n",
    "\n",
    "# # 2. Select features for outlier handling (>5% outlier rate)\n",
    "# features_to_handle = [col for col, stats_dict in sorted_outliers \n",
    "#                      if stats_dict['outlier_rate'] > 0.05]\n",
    "\n",
    "# print(f\"\\n2. FEATURES SELECTED FOR OUTLIER HANDLING\")\n",
    "# print(\"-\" * 50)\n",
    "# print(f\"Features with >5% outlier rate: {len(features_to_handle)}\")\n",
    "# for feature in features_to_handle:\n",
    "#     rate = outlier_summary[feature]['outlier_rate']\n",
    "#     print(f\"  {feature}: {rate*100:.1f}%\")\n",
    "\n",
    "# if features_to_handle:\n",
    "#     print(f\"\\n3. APPLYING OUTLIER HANDLING\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # Store original for comparison\n",
    "#     train_df_original = train_df.copy()\n",
    "    \n",
    "#     # Handle each problematic feature\n",
    "#     bounds_applied = {}\n",
    "    \n",
    "#     # Filter to only handle columns that exist in both train AND test datasets\n",
    "#     valid_features_to_handle = [col for col in features_to_handle \n",
    "#                                if col in train_df.columns and col in test_df.columns]\n",
    "    \n",
    "#     print(f\"Processing {len(valid_features_to_handle)} features that exist in both train and test sets\")\n",
    "    \n",
    "#     for col in valid_features_to_handle:\n",
    "#         # Calculate bounds on TRAINING data only\n",
    "#         lower_bound = train_df[col].quantile(0.01)\n",
    "#         upper_bound = train_df[col].quantile(0.99)\n",
    "        \n",
    "#         # Store bounds for reference\n",
    "#         bounds_applied[col] = {\n",
    "#             'lower': lower_bound,\n",
    "#             'upper': upper_bound,\n",
    "#             'original_range': f\"{train_df[col].min():.2f} to {train_df[col].max():.2f}\"\n",
    "#         }\n",
    "        \n",
    "#         # Apply SAME bounds to both datasets\n",
    "#         train_df[col] = train_df[col].clip(lower_bound, upper_bound)\n",
    "#         test_df[col] = test_df[col].clip(lower_bound, upper_bound)  # Same bounds!\n",
    "        \n",
    "#         bounds_applied[col]['new_range'] = f\"{train_df[col].min():.2f} to {train_df[col].max():.2f}\"\n",
    "    \n",
    "#     # Report on any features that were excluded from processing\n",
    "#     excluded_features = [col for col in features_to_handle if col not in valid_features_to_handle]\n",
    "#     if excluded_features:\n",
    "#         print(f\"\\n‚ö†Ô∏è  Skipped {len(excluded_features)} features not present in test set:\")\n",
    "#         for col in excluded_features:\n",
    "#             print(f\"    - {col} (train-only feature)\")\n",
    "    \n",
    "#     print(f\"\\n‚úÖ Outlier handling complete!\")\n",
    "#     print(f\"Features processed: {len(bounds_applied)}\")\n",
    "#     print(f\"All rows preserved: Train {train_df.shape[0]}, Test {test_df.shape[0]}\")\n",
    "    \n",
    "#     print(f\"\\n4. IMPACT SUMMARY\")\n",
    "#     print(\"-\" * 50)\n",
    "#     for col, bounds in bounds_applied.items():\n",
    "#         print(f\"{col}:\")\n",
    "#         print(f\"  Bounds applied: {bounds['lower']:.2f} to {bounds['upper']:.2f}\")\n",
    "#         print(f\"  Before: {bounds['original_range']}\")\n",
    "#         print(f\"  After:  {bounds['new_range']}\")\n",
    "#         print()\n",
    "\n",
    "# else:\n",
    "#     print(f\"\\n‚úÖ No features require outlier handling (all <5% outlier rate)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d476af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created derived features: R_per_game, RA_per_game\n",
      "Train - R_per_game range: 2.409 to 6.884\n",
      "Train - RA_per_game range: 2.458 to 7.686\n",
      "Test - R_per_game range: 2.783 to 6.896\n",
      "Test - RA_per_game range: 2.867 to 6.865\n",
      "\n",
      "Created derived feature: Expected_Wins\n",
      "Train - Expected_Wins range: 35.860 to 119.963\n",
      "Test - Expected_Wins range: 40.352 to 107.111\n",
      "\n",
      "Created derived feature: Times_On_Base\n",
      "Train - Times_On_Base range: 1367.000 to 2415.000\n",
      "Test - Times_On_Base range: 1453.000 to 2327.000\n",
      "\n",
      "Created derived feature: BB_Rate\n",
      "Train - BB_Rate range: 0.051 to 0.136\n",
      "Test - BB_Rate range: 0.052 to 0.123\n",
      "\n",
      "Created derived feature: HR_Rate\n",
      "Train - HR_Rate range: 0.001 to 0.047\n",
      "Test - HR_Rate range: 0.001 to 0.045\n",
      "\n",
      "Created derived feature: OBP\n",
      "Train - OBP range: 0.262 to 0.382\n",
      "Test - OBP range: 0.267 to 0.382\n",
      "\n",
      "Created derived feature: SLG\n",
      "Train - SLG range: 0.274 to 0.491\n",
      "Test - SLG range: 0.261 to 0.488\n",
      "\n",
      "Created derived feature: OPS\n",
      "Train - OPS range: 0.539 to 0.870\n",
      "Test - OPS range: 0.530 to 0.870\n",
      "\n",
      "Created derived feature: Times_On_Base_Allowed\n",
      "Train - Times_On_Base_Allowed range: 1441.000 to 2536.000\n",
      "Test - Times_On_Base_Allowed range: 1454.000 to 2421.000\n",
      "\n",
      "Created derived feature: WHIP\n",
      "Train - WHIP range: 1.025 to 1.848\n",
      "Test - WHIP range: 1.028 to 1.776\n",
      "\n",
      "Created derived feature: K_per_9\n",
      "Train - K_per_9 range: 2.102 to 9.353\n",
      "Test - K_per_9 range: 2.064 to 8.704\n",
      "\n",
      "Created derived feature: HR_per_9\n",
      "Train - HR_per_9 range: 0.032 to 1.610\n",
      "Test - HR_per_9 range: 0.040 to 1.429\n",
      "\n",
      "Created derived feature: REI\n",
      "Train - REI range: 1.475 to 2.545\n",
      "Test - REI range: 1.554 to 2.402\n",
      "\n",
      "Created derived feature: PEI\n",
      "Train - PEI range: 0.975 to 28.452\n",
      "Test - PEI range: 1.215 to 27.720\n",
      "\n",
      "Created derived feature: Era_Adjusted_OBP\n",
      "Train - Era_Adjusted_OBP range: 0.246 to 0.434\n",
      "Test - Era_Adjusted_OBP range: 0.254 to 0.405\n",
      "\n",
      "Created derived feature: Era_Adjusted_SLG\n",
      "Train - Era_Adjusted_SLG range: 0.289 to 0.501\n",
      "Test - Era_Adjusted_SLG range: 0.298 to 0.482\n",
      "\n",
      "Created derived feature: Era_Adjusted_OPS\n",
      "Train - Era_Adjusted_OPS range: 0.536 to 0.910\n",
      "Test - Era_Adjusted_OPS range: 0.564 to 0.850\n",
      "\n",
      "Created derived feature: Era_Adjusted_WHIP\n",
      "Train - Era_Adjusted_WHIP range: 1.084 to 1.882\n",
      "Test - Era_Adjusted_WHIP range: 1.127 to 1.707\n",
      "\n",
      "Created derived feature: Era_Adjusted_K_per_9\n",
      "Train - Era_Adjusted_K_per_9 range: 1.847 to 9.626\n",
      "Test - Era_Adjusted_K_per_9 range: 1.795 to 8.963\n",
      "\n",
      "Created derived feature: Era_Adjusted_HR_per_9\n",
      "Train - Era_Adjusted_HR_per_9 range: 0.040 to 1.584\n",
      "Test - Era_Adjusted_HR_per_9 range: 0.043 to 1.347\n",
      "\n",
      "Created derived feature: Era_Adjusted_BB_Rate\n",
      "Train - Era_Adjusted_BB_Rate range: 0.049 to 0.136\n",
      "Test - Era_Adjusted_BB_Rate range: 0.046 to 0.133\n",
      "\n",
      "Created derived feature: Era_Adjusted_HR_Rate\n",
      "Train - Era_Adjusted_HR_Rate range: 0.001 to 0.047\n",
      "Test - Era_Adjusted_HR_Rate range: 0.001 to 0.043\n"
     ]
    }
   ],
   "source": [
    "# Create derived features for both train and test sets\n",
    "\n",
    "# R_per_game: Runs per game\n",
    "# RA_per_game: Runs allowed per game\n",
    "train_df['R_per_game'] = train_df['R'] / train_df['G']\n",
    "train_df['RA_per_game'] = train_df['RA'] / train_df['G']\n",
    "test_df['R_per_game'] = test_df['R'] / test_df['G']\n",
    "test_df['RA_per_game'] = test_df['RA'] / test_df['G']\n",
    "\n",
    "print(f\"\\nCreated derived features: R_per_game, RA_per_game\")\n",
    "print(f\"Train - R_per_game range: {train_df['R_per_game'].min():.3f} to {train_df['R_per_game'].max():.3f}\")\n",
    "print(f\"Train - RA_per_game range: {train_df['RA_per_game'].min():.3f} to {train_df['RA_per_game'].max():.3f}\")\n",
    "print(f\"Test - R_per_game range: {test_df['R_per_game'].min():.3f} to {test_df['R_per_game'].max():.3f}\")\n",
    "print(f\"Test - RA_per_game range: {test_df['RA_per_game'].min():.3f} to {test_df['RA_per_game'].max():.3f}\")\n",
    "\n",
    "# Expected Wins of Season = G √ó (R¬≤) / (R¬≤ + RA¬≤)\n",
    "train_df['Expected_Wins'] = train_df['G'] * (train_df['R_per_game'] ** 2) / ((train_df['R_per_game'] ** 2) + (train_df['RA_per_game'] ** 2))\n",
    "test_df['Expected_Wins'] = test_df['G'] * (test_df['R_per_game'] ** 2) / ((test_df['R_per_game'] ** 2) + (test_df['RA_per_game'] ** 2))\n",
    "# train_df['Expected_Wins'] = train_df['G'] * (train_df['R'] ** 2) / ((train_df['R'] ** 2) + (train_df['RA'] ** 2))\n",
    "# test_df['Expected_Wins'] = test_df['G'] * (test_df['R'] ** 2) / ((test_df['R'] ** 2) + (test_df['RA'] ** 2))\n",
    "print(f\"\\nCreated derived feature: Expected_Wins\")   \n",
    "print(f\"Train - Expected_Wins range: {train_df['Expected_Wins'].min():.3f} to {train_df['Expected_Wins'].max():.3f}\")\n",
    "print(f\"Test - Expected_Wins range: {test_df['Expected_Wins'].min():.3f} to {test_df['Expected_Wins'].max():.3f}\")\n",
    "\n",
    "# Times getting on base\n",
    "train_df['Times_On_Base'] = train_df['H'] + train_df['BB']\n",
    "test_df['Times_On_Base'] = test_df['H'] + test_df['BB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: Times_On_Base\")\n",
    "print(f\"Train - Times_On_Base range: {train_df['Times_On_Base'].min():.3f} to {train_df['Times_On_Base'].max():.3f}\")\n",
    "print(f\"Test - Times_On_Base range: {test_df['Times_On_Base'].min():.3f} to {test_df['Times_On_Base'].max():.3f}\")\n",
    "\n",
    "# BB Rate (Walk Percentage) - BB / AB + BB\n",
    "train_df['BB_Rate'] = train_df['BB'] / (train_df['AB'] + train_df['BB'])\n",
    "test_df['BB_Rate'] = test_df['BB'] / (test_df['AB'] + test_df['BB'])\n",
    "\n",
    "print(f\"\\nCreated derived feature: BB_Rate\")\n",
    "print(f\"Train - BB_Rate range: {train_df['BB_Rate'].min():.3f} to {train_df['BB_Rate'].max():.3f}\") \n",
    "print(f\"Test - BB_Rate range: {test_df['BB_Rate'].min():.3f} to {test_df['BB_Rate'].max():.3f}\")\n",
    "\n",
    "# Home Run Rate - HR / AB\n",
    "train_df['HR_Rate'] = train_df['HR'] / train_df['AB']\n",
    "test_df['HR_Rate'] = test_df['HR'] / test_df['AB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: HR_Rate\")\n",
    "print(f\"Train - HR_Rate range: {train_df['HR_Rate'].min():.3f} to {train_df['HR_Rate'].max():.3f}\")\n",
    "print(f\"Test - HR_Rate range: {test_df['HR_Rate'].min():.3f} to {test_df['HR_Rate'].max():.3f}\")\n",
    "\n",
    "# On-Base Percentage (OBP) - (H + BB) / (AB + BB)\n",
    "train_df['OBP'] = (train_df['H'] + train_df['BB']) / (train_df['AB'] + train_df['BB'])\n",
    "test_df['OBP'] = (test_df['H'] + test_df['BB']) / (test_df['AB'] + test_df['BB'])\n",
    "\n",
    "print(f\"\\nCreated derived feature: OBP\")\n",
    "print(f\"Train - OBP range: {train_df['OBP'].min():.3f} to {train_df['OBP'].max():.3f}\") \n",
    "print(f\"Test - OBP range: {test_df['OBP'].min():.3f} to {test_df['OBP'].max():.3f}\")\n",
    "\n",
    "# Slugging Percentage (SLG)\n",
    "# Singles = H - (2B + 3B + HR)\n",
    "# Total Bases = Singles + (2 * 2B) + (3 * 3B) + (4 * HR)\n",
    "# SLG = Total Bases / AB\n",
    "Singles_train = train_df['H'] - (train_df['2B'] + train_df['3B'] + train_df['HR'])\n",
    "Total_Bases_train = Singles_train + (2 * train_df['2B']) + (3 * train_df['3B']) + (4 * train_df['HR'])\n",
    "train_df['SLG'] = Total_Bases_train / train_df['AB']  \n",
    "\n",
    "Singles_test = test_df['H'] - (test_df['2B'] + test_df['3B'] + test_df['HR'])\n",
    "Total_Bases_test = Singles_test + (2 * test_df['2B']) + (3 * test_df['3B']) + (4 * test_df['HR'])\n",
    "test_df['SLG'] = Total_Bases_test / test_df['AB']\n",
    "\n",
    "print(f\"\\nCreated derived feature: SLG\")\n",
    "print(f\"Train - SLG range: {train_df['SLG'].min():.3f} to {train_df['SLG'].max():.3f}\") \n",
    "print(f\"Test - SLG range: {test_df['SLG'].min():.3f} to {test_df['SLG'].max():.3f}\")    \n",
    "\n",
    "# Combined On-Base Plus Slugging (OPS) - OBP + SLG\n",
    "train_df['OPS'] = train_df['OBP'] + train_df['SLG']\n",
    "test_df['OPS'] = test_df['OBP'] + test_df['SLG']\n",
    "\n",
    "print(f\"\\nCreated derived feature: OPS\")\n",
    "print(f\"Train - OPS range: {train_df['OPS'].min():.3f} to {train_df['OPS'].max():.3f}\") \n",
    "print(f\"Test - OPS range: {test_df['OPS'].min():.3f} to {test_df['OPS'].max():.3f}\")\n",
    "\n",
    "# Time on Base Allowed - HA + BBA\n",
    "train_df['Times_On_Base_Allowed'] = train_df['HA'] + train_df['BBA']\n",
    "test_df['Times_On_Base_Allowed'] = test_df['HA'] + test_df['BBA']\n",
    "\n",
    "print(f\"\\nCreated derived feature: Times_On_Base_Allowed\")\n",
    "print(f\"Train - Times_On_Base_Allowed range: {train_df['Times_On_Base_Allowed'].min():.3f} to {train_df['Times_On_Base_Allowed'].max():.3f}\")\n",
    "print(f\"Test - Times_On_Base_Allowed range: {test_df['Times_On_Base_Allowed'].min():.3f} to {test_df['Times_On_Base_Allowed'].max():.3f}\")\n",
    "\n",
    "# WHIP (Walks plus Hits per Inning Pitched)\n",
    "# Inings Pitched = IPouts / 3\n",
    "# Times_On_Base_Per_Inning = Times_On_Base_Allowed / Inings_Pitched\n",
    "train_df['Innings_Pitched'] = train_df['IPouts'] / 3\n",
    "train_df['WHIP'] = train_df['Times_On_Base_Allowed'] / train_df['Innings_Pitched']\n",
    "test_df['Innings_Pitched'] = test_df['IPouts'] / 3\n",
    "test_df['WHIP'] = test_df['Times_On_Base_Allowed'] / test_df['Innings_Pitched']\n",
    "\n",
    "print(f\"\\nCreated derived feature: WHIP\")\n",
    "print(f\"Train - WHIP range: {train_df['WHIP'].min():.3f} to {train_df['WHIP'].max():.3f}\")\n",
    "print(f\"Test - WHIP range: {test_df['WHIP'].min():.3f} to {test_df['WHIP'].max():.3f}\")\n",
    "\n",
    "# K/9 (Strikeouts per 9 Innings) - SOA / Innings_Pitched * 9\n",
    "train_df['K_per_9'] = (train_df['SOA'] / train_df['Innings_Pitched']) * 9\n",
    "test_df['K_per_9'] = (test_df['SOA'] / test_df['Innings_Pitched']) * 9  \n",
    "\n",
    "print(f\"\\nCreated derived feature: K_per_9\")\n",
    "print(f\"Train - K_per_9 range: {train_df['K_per_9'].min():.3f} to {train_df['K_per_9'].max():.3f}\")\n",
    "print(f\"Test - K_per_9 range: {test_df['K_per_9'].min():.3f} to {test_df['K_per_9'].max():.3f}\")\n",
    "\n",
    "# HR/9 (Home Runs Allowed per 9 Innings) - HRA / Innings_Pitched * 9\n",
    "train_df['HR_per_9'] = (train_df['HRA'] / train_df['Innings_Pitched']) * 9\n",
    "test_df['HR_per_9'] = (test_df['HRA'] / test_df['Innings_Pitched']) * 9\n",
    "\n",
    "print(f\"\\nCreated derived feature: HR_per_9\")\n",
    "print(f\"Train - HR_per_9 range: {train_df['HR_per_9'].min():.3f} to {train_df['HR_per_9'].max():.3f}\")\n",
    "print(f\"Test - HR_per_9 range: {test_df['HR_per_9'].min():.3f} to {test_df['HR_per_9'].max():.3f}\")\n",
    "\n",
    "# Run Environment Index (REI) - (R + RA) / G / mlb_rpg\n",
    "train_df['REI'] = (train_df['R'] + train_df['RA']) / train_df['G'] / train_df['mlb_rpg']\n",
    "test_df['REI'] = (test_df['R'] + test_df['RA']) / test_df['G'] / test_df['mlb_rpg']\n",
    "print(f\"\\nCreated derived feature: REI\")\n",
    "print(f\"Train - REI range: {train_df['REI'].min():.3f} to {train_df['REI'].max():.3f}\")\n",
    "print(f\"Test - REI range: {test_df['REI'].min():.3f} to {test_df['REI'].max():.3f}\")    \n",
    "\n",
    "# Power Environement Index (PEI) -  (HR + HRA) / G / (mlb_rpg * avg_hr_rate)\n",
    "avg_hr_rate = train_df['HR_Rate'].mean()\n",
    "train_df['PEI'] = (train_df['HR'] + train_df['HRA']) / train_df['G'] / (train_df['mlb_rpg'] * avg_hr_rate)\n",
    "test_df['PEI'] = (test_df['HR'] + test_df['HRA']) / test_df['G'] / (test_df['mlb_rpg'] * avg_hr_rate)\n",
    "print(f\"\\nCreated derived feature: PEI\")\n",
    "print(f\"Train - PEI range: {train_df['PEI'].min():.3f} to {train_df['PEI'].max():.3f}\")\n",
    "print(f\"Test - PEI range: {test_df['PEI'].min():.3f} to {test_df['PEI'].max():.3f}\") \n",
    "\n",
    "# Era adjusted OBP, SLG, OPS, WHIP, K_per_9, HR_per_9, BB_Rate, HR_Rate\n",
    "# Historical average runs per game (RPG) for MLB\n",
    "historical_avg_rpg_train = train_df['mlb_rpg'].mean()\n",
    "historical_avg_rpg_test = test_df['mlb_rpg'].mean()\n",
    "# historical_avg_rpg_train = 4.4\n",
    "# historical_avg_rpg_test = 4.4\n",
    "\n",
    "# Era adjusted OBP\n",
    "train_df['Era_Adjusted_OBP'] = train_df['OBP'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_OBP'] = test_df['OBP'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_OBP\")\n",
    "print(f\"Train - Era_Adjusted_OBP range: {train_df['Era_Adjusted_OBP'].min():.3f} to {train_df['Era_Adjusted_OBP'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_OBP range: {test_df['Era_Adjusted_OBP'].min():.3f} to {test_df['Era_Adjusted_OBP'].max():.3f}\") \n",
    "\n",
    "# Era adjusted SLG\n",
    "train_df['Era_Adjusted_SLG'] = train_df['SLG'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_SLG'] = test_df['SLG'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_SLG\")\n",
    "print(f\"Train - Era_Adjusted_SLG range: {train_df['Era_Adjusted_SLG'].min():.3f} to {train_df['Era_Adjusted_SLG'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_SLG range: {test_df['Era_Adjusted_SLG'].min():.3f} to {test_df['Era_Adjusted_SLG'].max():.3f}\") \n",
    "\n",
    "# Era adjusted OPS\n",
    "train_df['Era_Adjusted_OPS'] = train_df['OPS'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_OPS'] = test_df['OPS'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_OPS\")\n",
    "print(f\"Train - Era_Adjusted_OPS range: {train_df['Era_Adjusted_OPS'].min():.3f} to {train_df['Era_Adjusted_OPS'].max():.3f}\") \n",
    "print(f\"Test - Era_Adjusted_OPS range: {test_df['Era_Adjusted_OPS'].min():.3f} to {test_df['Era_Adjusted_OPS'].max():.3f}\")\n",
    "\n",
    "# Era adjusted WHIP\n",
    "train_df['Era_Adjusted_WHIP'] = train_df['WHIP'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_WHIP'] = test_df['WHIP'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_WHIP\")\n",
    "print(f\"Train - Era_Adjusted_WHIP range: {train_df['Era_Adjusted_WHIP'].min():.3f} to {train_df['Era_Adjusted_WHIP'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_WHIP range: {test_df['Era_Adjusted_WHIP'].min():.3f} to {test_df['Era_Adjusted_WHIP'].max():.3f}\")\n",
    "\n",
    "# Era adjusted K_per_9\n",
    "train_df['Era_Adjusted_K_per_9'] = train_df['K_per_9'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_K_per_9'] = test_df['K_per_9'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_K_per_9\")\n",
    "print(f\"Train - Era_Adjusted_K_per_9 range: {train_df['Era_Adjusted_K_per_9'].min():.3f} to {train_df['Era_Adjusted_K_per_9'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_K_per_9 range: {test_df['Era_Adjusted_K_per_9'].min():.3f} to {test_df['Era_Adjusted_K_per_9'].max():.3f}\") \n",
    "\n",
    "# Era adjusted HR_per_9\n",
    "train_df['Era_Adjusted_HR_per_9'] = train_df['HR_per_9'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_HR_per_9'] = test_df['HR_per_9'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_HR_per_9\")\n",
    "print(f\"Train - Era_Adjusted_HR_per_9 range: {train_df['Era_Adjusted_HR_per_9'].min():.3f} to {train_df['Era_Adjusted_HR_per_9'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_HR_per_9 range: {test_df['Era_Adjusted_HR_per_9'].min():.3f} to {test_df['Era_Adjusted_HR_per_9'].max():.3f}\")\n",
    "\n",
    "# Era adjusted BB_Rate\n",
    "train_df['Era_Adjusted_BB_Rate'] = train_df['BB_Rate'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_BB_Rate'] = test_df['BB_Rate'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_BB_Rate\")\n",
    "print(f\"Train - Era_Adjusted_BB_Rate range: {train_df['Era_Adjusted_BB_Rate'].min():.3f} to {train_df['Era_Adjusted_BB_Rate'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_BB_Rate range: {test_df['Era_Adjusted_BB_Rate'].min():.3f} to {test_df['Era_Adjusted_BB_Rate'].max():.3f}\") \n",
    "\n",
    "# Era adjusted HR_Rate\n",
    "train_df['Era_Adjusted_HR_Rate'] = train_df['HR_Rate'] * (historical_avg_rpg_train / train_df['mlb_rpg'])\n",
    "test_df['Era_Adjusted_HR_Rate'] = test_df['HR_Rate'] * (historical_avg_rpg_test / test_df['mlb_rpg'])\n",
    "print(f\"\\nCreated derived feature: Era_Adjusted_HR_Rate\")\n",
    "print(f\"Train - Era_Adjusted_HR_Rate range: {train_df['Era_Adjusted_HR_Rate'].min():.3f} to {train_df['Era_Adjusted_HR_Rate'].max():.3f}\")\n",
    "print(f\"Test - Era_Adjusted_HR_Rate range: {test_df['Era_Adjusted_HR_Rate'].min():.3f} to {test_df['Era_Adjusted_HR_Rate'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028cb31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLIFIED TEAM ARCHETYPE + EXPECTED_WINS INTERACTIONS\n",
      "============================================================\n",
      "\n",
      "1. CREATING FOCUSED TEAM ARCHETYPES\n",
      "--------------------------------------------------\n",
      "Archetype clustering with 3 key features:\n",
      "  ‚Ä¢ Era_Adjusted_OPS\n",
      "  ‚Ä¢ Era_Adjusted_WHIP\n",
      "  ‚Ä¢ Expected_Wins\n",
      "\n",
      "2. OPTIMIZING FOR INTERPRETABLE ARCHETYPES\n",
      "--------------------------------------------------\n",
      "  k=3: Silhouette=0.3206, Balance=0.09, Combined=0.4162\n",
      "  k=4: Silhouette=0.3051, Balance=0.13, Combined=0.2987\n",
      "  k=5: Silhouette=0.3071, Balance=0.27, Combined=0.2938\n",
      "\n",
      "üèÜ Selected: k=3\n",
      "Train cluster distribution: [678 553 581]\n",
      "Test cluster distribution: [173 146 134]\n",
      "\n",
      "3. TEAM ARCHETYPE ANALYSIS & NAMING\n",
      "--------------------------------------------------\n",
      "\n",
      "Cluster 0: Pitching_Strong\n",
      "  Teams: 678 | Avg Wins: 85.3\n",
      "  Expected Wins: 85.7\n",
      "  Era Adj OPS: 0.694 | Era Adj WHIP: 1.264\n",
      "\n",
      "Cluster 1: Struggling_Teams\n",
      "  Teams: 553 | Avg Wins: 64.8\n",
      "  Expected Wins: 64.3\n",
      "  Era Adj OPS: 0.689 | Era Adj WHIP: 1.450\n",
      "\n",
      "Cluster 2: Elite_Teams\n",
      "  Teams: 581 | Avg Wins: 86.0\n",
      "  Expected Wins: 87.0\n",
      "  Era Adj OPS: 0.767 | Era Adj WHIP: 1.400\n",
      "\n",
      "4. CREATING EXPECTED_WINS INTERACTION FEATURES\n",
      "------------------------------------------------------------\n",
      "‚úÖ Created 8 interaction features:\n",
      "  ‚Ä¢ Pitching_Strong_Expected_Boost\n",
      "  ‚Ä¢ Pitching_Strong_Expected_Squared\n",
      "  ‚Ä¢ Struggling_Teams_Expected_Boost\n",
      "  ‚Ä¢ Struggling_Teams_Expected_Squared\n",
      "  ‚Ä¢ Elite_Teams_Expected_Boost\n",
      "  ‚Ä¢ Elite_Teams_Expected_Squared\n",
      "  ‚Ä¢ Expected_Wins_Relative\n",
      "  ‚Ä¢ Expected_Achievement_Ratio\n",
      "\n",
      "5. FEATURE VALIDATION\n",
      "--------------------------------------------------\n",
      "Feature correlations with wins:\n",
      "  Struggling_Teams_Expected_Boost: 0.6756\n",
      "  Struggling_Teams_Expected_Squared: 0.6165\n",
      "  Expected_Wins_Relative: 0.6076\n",
      "  Expected_Achievement_Ratio: 0.6035\n",
      "  Pitching_Strong_Expected_Squared: 0.4438\n",
      "  Elite_Teams_Expected_Squared: 0.4257\n",
      "  Pitching_Strong_Expected_Boost: 0.4043\n",
      "  Elite_Teams_Expected_Boost: 0.3926\n",
      "\n",
      "üìà CORRELATION COMPARISON\n",
      "--------------------------------------------------\n",
      "Original clustering best: 0.5584\n",
      "Enhanced clustering best: 0.5269\n",
      "Simplified approach best: 0.6756\n",
      "Improvement vs Enhanced: +0.1487\n",
      "Improvement vs Original: +0.1172\n",
      "\n",
      "Clustering Quality:\n",
      "  Silhouette Score: 0.3206\n",
      "  Cluster Balance: 53.6\n",
      "\n",
      "üéØ SIMPLIFIED APPROACH SUMMARY\n",
      "--------------------------------------------------\n",
      "‚úÖ Focused on 3 key features for interpretability\n",
      "‚úÖ Created 8 targeted Expected_Wins interactions\n",
      "‚úÖ Meaningful archetype names: ['Pitching_Strong', 'Struggling_Teams', 'Elite_Teams']\n",
      "‚úÖ Best correlation: 0.6756\n",
      "üèÜ Simplified approach outperforms enhanced clustering!\n",
      "\n",
      "üí° Ready for modeling with high-impact, interpretable features!\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED HIGH-IMPACT CLUSTER INTERACTION APPROACH\n",
    "print(\"SIMPLIFIED TEAM ARCHETYPE + EXPECTED_WINS INTERACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. CREATE SIMPLIFIED TEAM ARCHETYPES USING KEY DISCRIMINATORS\n",
    "print(\"\\n1. CREATING FOCUSED TEAM ARCHETYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use just 3 most discriminative features for cleaner clustering\n",
    "archetype_features = [\n",
    "    'Era_Adjusted_OPS',    # Offensive capability\n",
    "    'Era_Adjusted_WHIP',   # Pitching capability  \n",
    "    'Expected_Wins'        # Overall team quality\n",
    "]\n",
    "\n",
    "print(f\"Archetype clustering with {len(archetype_features)} key features:\")\n",
    "for feature in archetype_features:\n",
    "    print(f\"  ‚Ä¢ {feature}\")\n",
    "\n",
    "# Prepare archetype data\n",
    "archetype_data_train = train_df[archetype_features].copy()\n",
    "archetype_data_test = test_df[archetype_features].copy()\n",
    "\n",
    "# Handle missing values\n",
    "archetype_data_train = archetype_data_train.fillna(archetype_data_train.median())\n",
    "archetype_data_test = archetype_data_test.fillna(archetype_data_train.median())\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "archetype_scaler = RobustScaler()\n",
    "archetype_train_scaled = archetype_scaler.fit_transform(archetype_data_train)\n",
    "archetype_test_scaled = archetype_scaler.transform(archetype_data_test)\n",
    "\n",
    "# 2. FIND OPTIMAL CLUSTERS WITH FOCUS ON INTERPRETABILITY\n",
    "print(f\"\\n2. OPTIMIZING FOR INTERPRETABLE ARCHETYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_silhouette = -1\n",
    "best_k_simple = 3\n",
    "best_labels_simple = None\n",
    "\n",
    "for k in range(3, 6):  # Test fewer options for interpretability\n",
    "    kmeans_simple = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    labels_simple = kmeans_simple.fit_predict(archetype_train_scaled)\n",
    "    \n",
    "    sil_score = silhouette_score(archetype_train_scaled, labels_simple)\n",
    "    cluster_sizes = np.bincount(labels_simple)\n",
    "    balance_score = np.std(cluster_sizes) / np.mean(cluster_sizes)\n",
    "    \n",
    "    # Prioritize interpretability (fewer clusters) and balance\n",
    "    combined_score = sil_score - (balance_score * 0.05) + (0.1 if k == 3 else 0)  # Bonus for k=3\n",
    "    \n",
    "    print(f\"  k={k}: Silhouette={sil_score:.4f}, Balance={balance_score:.2f}, Combined={combined_score:.4f}\")\n",
    "    \n",
    "    if combined_score > best_silhouette:\n",
    "        best_silhouette = combined_score\n",
    "        best_k_simple = k\n",
    "        best_labels_simple = labels_simple\n",
    "        best_kmeans_simple = kmeans_simple\n",
    "\n",
    "# Apply best clustering\n",
    "train_clusters_simple = best_labels_simple\n",
    "test_clusters_simple = best_kmeans_simple.predict(archetype_test_scaled)\n",
    "\n",
    "train_df['Team_Archetype_Simple'] = train_clusters_simple\n",
    "test_df['Team_Archetype_Simple'] = test_clusters_simple\n",
    "\n",
    "print(f\"\\nüèÜ Selected: k={best_k_simple}\")\n",
    "print(f\"Train cluster distribution: {np.bincount(train_clusters_simple)}\")\n",
    "print(f\"Test cluster distribution: {np.bincount(test_clusters_simple)}\")\n",
    "\n",
    "# 3. ANALYZE AND NAME ARCHETYPES\n",
    "print(f\"\\n3. TEAM ARCHETYPE ANALYSIS & NAMING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "archetype_names = {}\n",
    "for cluster_id in range(best_k_simple):\n",
    "    cluster_mask = train_clusters_simple == cluster_id\n",
    "    cluster_teams = train_df[cluster_mask]\n",
    "    \n",
    "    avg_wins = cluster_teams['W'].mean()\n",
    "    avg_ops = cluster_teams['Era_Adjusted_OPS'].mean()\n",
    "    avg_whip = cluster_teams['Era_Adjusted_WHIP'].mean()\n",
    "    avg_expected = cluster_teams['Expected_Wins'].mean()\n",
    "    \n",
    "    # Intelligent naming based on characteristics\n",
    "    if avg_expected > 82 and avg_ops > 0.72:\n",
    "        name = \"Elite_Teams\"\n",
    "    elif avg_whip < 1.35 and avg_expected > 78:\n",
    "        name = \"Pitching_Strong\"\n",
    "    elif avg_expected > 70:  # More inclusive threshold\n",
    "        name = \"Competitive_Teams\"\n",
    "    elif avg_expected < 70:\n",
    "        name = \"Struggling_Teams\"\n",
    "    else:\n",
    "        name = f\"Archetype_{cluster_id}\"\n",
    "    \n",
    "    archetype_names[cluster_id] = name\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}: {name}\")\n",
    "    print(f\"  Teams: {cluster_mask.sum()} | Avg Wins: {avg_wins:.1f}\")\n",
    "    print(f\"  Expected Wins: {avg_expected:.1f}\")\n",
    "    print(f\"  Era Adj OPS: {avg_ops:.3f} | Era Adj WHIP: {avg_whip:.3f}\")\n",
    "\n",
    "# 4. CREATE HIGH-IMPACT EXPECTED_WINS INTERACTIONS\n",
    "print(f\"\\n4. CREATING EXPECTED_WINS INTERACTION FEATURES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "interaction_features_created = []\n",
    "\n",
    "# A. Archetype-specific Expected Wins boost\n",
    "for cluster_id in range(best_k_simple):\n",
    "    archetype_name = archetype_names[cluster_id]\n",
    "    \n",
    "    # Expected Wins boost for this archetype\n",
    "    feature_name = f'{archetype_name}_Expected_Boost'\n",
    "    train_df[feature_name] = (\n",
    "        (train_df['Team_Archetype_Simple'] == cluster_id) * train_df['Expected_Wins']\n",
    "    )\n",
    "    test_df[feature_name] = (\n",
    "        (test_df['Team_Archetype_Simple'] == cluster_id) * test_df['Expected_Wins']\n",
    "    )\n",
    "    interaction_features_created.append(feature_name)\n",
    "    \n",
    "    # Expected Wins squared for this archetype (non-linear relationship)\n",
    "    feature_name_sq = f'{archetype_name}_Expected_Squared'\n",
    "    train_df[feature_name_sq] = (\n",
    "        (train_df['Team_Archetype_Simple'] == cluster_id) * (train_df['Expected_Wins'] ** 2)\n",
    "    )\n",
    "    test_df[feature_name_sq] = (\n",
    "        (test_df['Team_Archetype_Simple'] == cluster_id) * (test_df['Expected_Wins'] ** 2)\n",
    "    )\n",
    "    interaction_features_created.append(feature_name_sq)\n",
    "\n",
    "# B. Expected Wins relative to archetype peers\n",
    "train_df['Expected_Wins_Relative'] = (\n",
    "    train_df['Expected_Wins'] - \n",
    "    train_df.groupby('Team_Archetype_Simple')['Expected_Wins'].transform('mean')\n",
    ")\n",
    "test_df['Expected_Wins_Relative'] = (\n",
    "    test_df['Expected_Wins'] - \n",
    "    test_df.groupby('Team_Archetype_Simple')['Expected_Wins'].transform('mean')\n",
    ")\n",
    "interaction_features_created.append('Expected_Wins_Relative')\n",
    "\n",
    "# C. Expected Wins achievement ratio (Expected vs Archetype average)\n",
    "archetype_expected_avg = train_df.groupby('Team_Archetype_Simple')['Expected_Wins'].mean()\n",
    "\n",
    "train_df['Expected_Achievement_Ratio'] = train_df.apply(\n",
    "    lambda row: row['Expected_Wins'] / archetype_expected_avg[row['Team_Archetype_Simple']], axis=1\n",
    ")\n",
    "test_df['Expected_Achievement_Ratio'] = test_df.apply(\n",
    "    lambda row: row['Expected_Wins'] / archetype_expected_avg[row['Team_Archetype_Simple']], axis=1\n",
    ")\n",
    "interaction_features_created.append('Expected_Achievement_Ratio')\n",
    "\n",
    "print(f\"‚úÖ Created {len(interaction_features_created)} interaction features:\")\n",
    "for feature in interaction_features_created:\n",
    "    print(f\"  ‚Ä¢ {feature}\")\n",
    "\n",
    "# 5. VALIDATE NEW FEATURES\n",
    "print(f\"\\n5. FEATURE VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate correlations with target\n",
    "simple_correlations = []\n",
    "for feature in interaction_features_created:\n",
    "    corr = abs(train_df[feature].corr(train_df['W']))\n",
    "    simple_correlations.append((feature, corr))\n",
    "\n",
    "simple_correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Feature correlations with wins:\")\n",
    "for feature, corr in simple_correlations:\n",
    "    print(f\"  {feature}: {corr:.4f}\")\n",
    "\n",
    "# Compare with original clustering\n",
    "best_new_corr = simple_correlations[0][1]\n",
    "original_best_corr = 0.5584\n",
    "enhanced_best_corr = 0.5269\n",
    "\n",
    "print(f\"\\nüìà CORRELATION COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original clustering best: {original_best_corr:.4f}\")\n",
    "print(f\"Enhanced clustering best: {enhanced_best_corr:.4f}\")\n",
    "print(f\"Simplified approach best: {best_new_corr:.4f}\")\n",
    "print(f\"Improvement vs Enhanced: {best_new_corr - enhanced_best_corr:+.4f}\")\n",
    "print(f\"Improvement vs Original: {best_new_corr - original_best_corr:+.4f}\")\n",
    "\n",
    "# Final silhouette score\n",
    "final_silhouette = silhouette_score(archetype_train_scaled, train_clusters_simple)\n",
    "print(f\"\\nClustering Quality:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"  Cluster Balance: {np.std(np.bincount(train_clusters_simple)):.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ SIMPLIFIED APPROACH SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úÖ Focused on 3 key features for interpretability\")\n",
    "print(f\"‚úÖ Created {len(interaction_features_created)} targeted Expected_Wins interactions\")\n",
    "print(f\"‚úÖ Meaningful archetype names: {list(archetype_names.values())}\")\n",
    "print(f\"‚úÖ Best correlation: {best_new_corr:.4f}\")\n",
    "\n",
    "if best_new_corr > enhanced_best_corr:\n",
    "    print(f\"üèÜ Simplified approach outperforms enhanced clustering!\")\n",
    "else:\n",
    "    print(f\"üìä Results competitive with enhanced approach\")\n",
    "\n",
    "print(f\"\\nüí° Ready for modeling with high-impact, interpretable features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e5425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New archetype features: ['Archetype_1', 'Archetype_2']\n",
      "Feature shapes: [553, 581]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the archetype feature for modeling\n",
    "archetype_dummies_train = pd.get_dummies(train_df['Team_Archetype_Simple'], prefix='Archetype', drop_first=True)\n",
    "archetype_dummies_test = pd.get_dummies(test_df['Team_Archetype_Simple'], prefix='Archetype', drop_first=True)\n",
    "train_df = pd.concat([train_df, archetype_dummies_train], axis=1)\n",
    "test_df = pd.concat([test_df, archetype_dummies_test], axis=1)  \n",
    "\n",
    "# Check new columns\n",
    "new_archetype_cols = [col for col in train_df.columns if col.startswith('Archetype_')]\n",
    "print(\"New archetype features:\", new_archetype_cols)\n",
    "print(\"Feature shapes:\", [train_df[col].sum() for col in new_archetype_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71b7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available default features: 75\n",
      "Available features:\n",
      "G\n",
      "R\n",
      "AB\n",
      "H\n",
      "2B\n",
      "3B\n",
      "HR\n",
      "BB\n",
      "SO\n",
      "SB\n",
      "RA\n",
      "ER\n",
      "ERA\n",
      "CG\n",
      "SHO\n",
      "SV\n",
      "IPouts\n",
      "HA\n",
      "HRA\n",
      "BBA\n",
      "SOA\n",
      "E\n",
      "DP\n",
      "FP\n",
      "Expected_Wins\n",
      "Times_On_Base\n",
      "Times_On_Base_Allowed\n",
      "mlb_rpg\n",
      "Era_Adjusted_OBP\n",
      "Era_Adjusted_SLG\n",
      "Era_Adjusted_OPS\n",
      "Era_Adjusted_WHIP\n",
      "Era_Adjusted_K_per_9\n",
      "Era_Adjusted_HR_per_9\n",
      "Era_Adjusted_BB_Rate\n",
      "Era_Adjusted_HR_Rate\n",
      "OBP\n",
      "SLG\n",
      "OPS\n",
      "WHIP\n",
      "K_per_9\n",
      "HR_per_9\n",
      "BB_Rate\n",
      "HR_Rate\n",
      "PEI\n",
      "REI\n",
      "Expected_Wins_Relative\n",
      "Expected_Achievement_Ratio\n",
      "Struggling_Teams_Expected_Boost\n",
      "Elite_Teams_Expected_Squared\n",
      "Struggling_Teams_Expected_Squared\n",
      "Elite_Teams_Expected_Boost\n",
      "Pitching_Strong_Expected_Squared\n",
      "Pitching_Strong_Expected_Boost\n",
      "Archetype_1\n",
      "Archetype_2\n",
      "era_1\n",
      "era_2\n",
      "era_3\n",
      "era_4\n",
      "era_5\n",
      "era_6\n",
      "era_7\n",
      "era_8\n",
      "decade_1910\n",
      "decade_1920\n",
      "decade_1930\n",
      "decade_1940\n",
      "decade_1950\n",
      "decade_1960\n",
      "decade_1970\n",
      "decade_1980\n",
      "decade_1990\n",
      "decade_2000\n",
      "decade_2010\n"
     ]
    }
   ],
   "source": [
    "default_features = [\n",
    "    # Basic Statistics\n",
    "    'G', 'R', 'AB', 'H', '2B', '3B', 'HR', 'BB', 'SO', 'SB', \n",
    "    'RA', 'ER', 'ERA', 'CG', 'SHO', 'SV', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA',\n",
    "    'E', 'DP', 'FP', \n",
    "\n",
    "    # 'CS', 'HBP', 'SF','attendance', 'BPF', 'PPF',\n",
    "    \n",
    "    # Derived Features\n",
    "    'Expected_Wins', 'Times_On_Base', 'Times_On_Base_Allowed', 'mlb_rpg',\n",
    "\n",
    "    'Era_Adjusted_OBP', 'Era_Adjusted_SLG', 'Era_Adjusted_OPS', 'Era_Adjusted_WHIP',\n",
    "    'Era_Adjusted_K_per_9', 'Era_Adjusted_HR_per_9', 'Era_Adjusted_BB_Rate', 'Era_Adjusted_HR_Rate',\n",
    "    \n",
    "    'OBP', 'SLG', 'OPS', 'WHIP', 'K_per_9', 'HR_per_9', 'BB_Rate', 'HR_Rate', \n",
    "    \n",
    "    'PEI', 'REI',\n",
    "\n",
    "    # Cluster interaction features  \n",
    "    'Expected_Wins_Relative', 'Expected_Achievement_Ratio', 'Struggling_Teams_Expected_Boost', \n",
    "    'Elite_Teams_Expected_Squared', 'Struggling_Teams_Expected_Squared', 'Elite_Teams_Expected_Boost', \n",
    "    'Pitching_Strong_Expected_Squared', 'Pitching_Strong_Expected_Boost',\n",
    "\n",
    "    'Archetype_1', 'Archetype_2',  # One-hot encoded archetype features\n",
    "    \n",
    "    # Era Indicators\n",
    "    'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8',\n",
    "    \n",
    "    # Decade Indicators\n",
    "    'decade_1910', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950',\n",
    "    'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000', 'decade_2010'\n",
    " ]\n",
    "\n",
    "\n",
    "# Filter features that exist in both training data AND test data\n",
    "available_features = [col for col in default_features \n",
    "                     if col in train_df.columns and col in test_df.columns]\n",
    "print(f\"Number of available default features: {len(available_features)}\")\n",
    "\n",
    "# Print available features in a column\n",
    "print(\"Available features:\")\n",
    "for feature in available_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edd8c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1449, 75)\n",
      "Validation set shape: (363, 75)\n",
      "Final test set shape: (453, 75)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data (split the train.csv for model evaluation)\n",
    "X_full = train_df[available_features]\n",
    "y_full = train_df['W']\n",
    "\n",
    "# Split training data into train/validation sets for model evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare final test data for predictions (this has no target variable)\n",
    "X_test_final = test_df[available_features]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Final test set shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a238e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è  Found and protecting 2 Archetype features:\n",
      "    ‚Ä¢ Archetype_1\n",
      "    ‚Ä¢ Archetype_2\n",
      "\n",
      "üîç CORRELATION ANALYSIS\n",
      "==================================================\n",
      "Correlation threshold: 0.95\n",
      "Original features: 75\n",
      "Protected features: 2\n",
      "Analyzable features: 73\n",
      "Features to remove: 17\n",
      "\n",
      "Highly correlated features to remove:\n",
      "  ‚Ä¢ ERA (corr=0.959 with RA)\n",
      "  ‚Ä¢ FP (corr=0.996 with E)\n",
      "  ‚Ä¢ Era_Adjusted_K_per_9 (corr=0.953 with SOA)\n",
      "  ‚Ä¢ Era_Adjusted_HR_per_9 (corr=0.981 with HRA)\n",
      "  ‚Ä¢ Era_Adjusted_HR_Rate (corr=0.979 with HR)\n",
      "  ‚Ä¢ OPS (corr=0.969 with SLG)\n",
      "  ‚Ä¢ K_per_9 (corr=0.999 with SOA)\n",
      "  ‚Ä¢ HR_per_9 (corr=0.999 with HRA)\n",
      "  ‚Ä¢ BB_Rate (corr=0.982 with BB)\n",
      "  ‚Ä¢ HR_Rate (corr=0.999 with HR)\n",
      "  ‚Ä¢ PEI (corr=0.959 with Era_Adjusted_HR_Rate)\n",
      "  ‚Ä¢ Expected_Achievement_Ratio (corr=0.991 with Expected_Wins_Relative)\n",
      "  ‚Ä¢ Struggling_Teams_Expected_Squared (corr=0.992 with Struggling_Teams_Expected_Boost)\n",
      "  ‚Ä¢ Elite_Teams_Expected_Boost (corr=0.992 with Elite_Teams_Expected_Squared)\n",
      "  ‚Ä¢ Pitching_Strong_Expected_Boost (corr=0.992 with Pitching_Strong_Expected_Squared)\n",
      "  ‚Ä¢ decade_1910 (corr=1.000 with era_1)\n",
      "  ‚Ä¢ decade_2010 (corr=1.000 with era_8)\n",
      "\n",
      "Features after removal: 58\n",
      "Features removed: 17\n",
      "‚úÖ Protected archetype features preserved: 2\n",
      "Dimensionality reduction: 23.3% (of analyzable features)\n",
      "\n",
      "üìä UPDATED DATASET INFO\n",
      "==================================================\n",
      "X_full shape: (1812, 58)\n",
      "X_test_final shape: (453, 58)\n",
      "Available features updated: 58\n",
      "‚úÖ Feature alignment verified between train and test sets\n",
      "\n",
      "üîÑ Variables updated for downstream compatibility:\n",
      "  ‚Ä¢ X_full: (1812, 58)\n",
      "  ‚Ä¢ X_test_final: (453, 58)\n",
      "  ‚Ä¢ available_features: 58 features\n",
      "\n",
      "üí° To disable correlation removal, simply comment out this entire cell\n"
     ]
    }
   ],
   "source": [
    "# Remove highly correlated features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_correlated_features(X_train, X_test, threshold=0.95, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features from training and test sets.\n",
    "    Excludes one-hot encoded Team_Archetype features from correlation analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature DataFrame\n",
    "    - X_test: Test feature DataFrame  \n",
    "    - threshold: Correlation threshold (default 0.95)\n",
    "    - verbose: Print information about removed features\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_filtered, X_test_filtered: DataFrames with correlated features removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # üõ°Ô∏è PROTECT ONE-HOT ENCODED ARCHETYPE FEATURES\n",
    "    # Check if Team_Archetype_ features exist in the dataset\n",
    "    archetype_features = [col for col in X_train.columns if col.startswith('Archetype_')]\n",
    "    \n",
    "    if archetype_features:\n",
    "        protected_features = archetype_features\n",
    "        if verbose:\n",
    "            print(f\"üõ°Ô∏è  Found and protecting {len(protected_features)} Archetype features:\")\n",
    "            for feat in protected_features:\n",
    "                print(f\"    ‚Ä¢ {feat}\")\n",
    "    else:\n",
    "        protected_features = []\n",
    "        if verbose:\n",
    "            print(f\"‚ÑπÔ∏è  No Archetype_ features found in dataset - skipping protection\")\n",
    "    \n",
    "    # Only analyze non-protected features for correlation\n",
    "    analyzable_features = [col for col in X_train.columns if col not in protected_features]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîç CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Correlation threshold: {threshold}\")\n",
    "        print(f\"Original features: {X_train.shape[1]}\")\n",
    "        print(f\"Protected features: {len(protected_features)}\")\n",
    "        print(f\"Analyzable features: {len(analyzable_features)}\")\n",
    "    \n",
    "    # Calculate correlation matrix ONLY on non-protected features\n",
    "    if analyzable_features:\n",
    "        analysis_data = X_train[analyzable_features]\n",
    "        corr_matrix = analysis_data.corr().abs()\n",
    "\n",
    "        # Find pairs of highly correlated features\n",
    "        upper_tri = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        \n",
    "        # Find features to remove (only among analyzable features)\n",
    "        features_to_remove = [column for column in upper_tri.columns \n",
    "                             if any(upper_tri[column] > threshold)]\n",
    "    else:\n",
    "        features_to_remove = []\n",
    "        if verbose:\n",
    "            print(f\"‚ö†Ô∏è  No analyzable features found - all features are protected!\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Features to remove: {len(features_to_remove)}\")\n",
    "        \n",
    "        if features_to_remove:\n",
    "            print(f\"\\nHighly correlated features to remove:\")\n",
    "            for feature in features_to_remove:\n",
    "                # Find what it's correlated with\n",
    "                high_corr = upper_tri[feature].dropna()\n",
    "                high_corr = high_corr[high_corr > threshold]\n",
    "                if len(high_corr) > 0:\n",
    "                    corr_with = high_corr.index[0]\n",
    "                    corr_value = high_corr.iloc[0]\n",
    "                    print(f\"  ‚Ä¢ {feature} (corr={corr_value:.3f} with {corr_with})\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No highly correlated features found above threshold {threshold}\")\n",
    "    \n",
    "    # Remove highly correlated features from both datasets\n",
    "    X_train_filtered = X_train.drop(columns=features_to_remove)\n",
    "    X_test_filtered = X_test.drop(columns=features_to_remove)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFeatures after removal: {X_train_filtered.shape[1]}\")\n",
    "        print(f\"Features removed: {len(features_to_remove)}\")\n",
    "        print(f\"‚úÖ Protected archetype features preserved: {len(protected_features)}\")\n",
    "        if len(features_to_remove) > 0:\n",
    "            improvement = len(features_to_remove) / len(analyzable_features) * 100 if analyzable_features else 0\n",
    "            print(f\"Dimensionality reduction: {improvement:.1f}% (of analyzable features)\")\n",
    "    \n",
    "    return X_train_filtered, X_test_filtered\n",
    "\n",
    "# Apply correlation removal to our datasets\n",
    "# Store original datasets for backup\n",
    "X_full_original = X_full.copy()\n",
    "X_test_final_original = X_test_final.copy()\n",
    "\n",
    "# Remove correlated features\n",
    "X_full_filtered, X_test_final_filtered = remove_correlated_features(\n",
    "    X_full, X_test_final, \n",
    "    threshold=0.95, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Update the main datasets (so later cells use the filtered versions)\n",
    "X_full = X_full_filtered\n",
    "X_test_final = X_test_final_filtered\n",
    "\n",
    "# Update available_features list to match the filtered features\n",
    "available_features_filtered = list(X_full.columns)\n",
    "\n",
    "print(f\"\\nüìä UPDATED DATASET INFO\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"X_full shape: {X_full.shape}\")\n",
    "print(f\"X_test_final shape: {X_test_final.shape}\")\n",
    "print(f\"Available features updated: {len(available_features_filtered)}\")\n",
    "\n",
    "# Verify both datasets have the same features\n",
    "assert list(X_full.columns) == list(X_test_final.columns), \"Feature mismatch between train and test!\"\n",
    "print(f\"‚úÖ Feature alignment verified between train and test sets\")\n",
    "\n",
    "# Update available_features for downstream compatibility\n",
    "available_features = available_features_filtered\n",
    "\n",
    "print(f\"\\nüîÑ Variables updated for downstream compatibility:\")\n",
    "print(f\"  ‚Ä¢ X_full: {X_full.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test_final: {X_test_final.shape}\")  \n",
    "print(f\"  ‚Ä¢ available_features: {len(available_features)} features\")\n",
    "print(f\"\\nüí° To disable correlation removal, simply comment out this entire cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37fccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA-OPTIMIZED BOOSTING MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (1812, 58)\n",
      "Features being used: ['G', 'R', 'AB', 'H', '2B', '3B', 'HR', 'BB', 'SO', 'SB', 'RA', 'ER', 'CG', 'SHO', 'SV', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA', 'E', 'DP', 'Expected_Wins', 'Times_On_Base', 'Times_On_Base_Allowed', 'mlb_rpg', 'Era_Adjusted_OBP', 'Era_Adjusted_SLG', 'Era_Adjusted_OPS', 'Era_Adjusted_WHIP', 'Era_Adjusted_BB_Rate', 'OBP', 'SLG', 'WHIP', 'REI', 'Expected_Wins_Relative', 'Struggling_Teams_Expected_Boost', 'Elite_Teams_Expected_Squared', 'Pitching_Strong_Expected_Squared', 'Archetype_1', 'Archetype_2', 'era_1', 'era_2', 'era_3', 'era_4', 'era_5', 'era_6', 'era_7', 'era_8', 'decade_1920', 'decade_1930', 'decade_1940', 'decade_1950', 'decade_1960', 'decade_1970', 'decade_1980', 'decade_1990', 'decade_2000']\n",
      "\n",
      "üîç HYPERPARAMETER OPTIMIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Optimizing XGBoost hyperparameters...\n",
      "  Best MAE: 3.0207 (Time: 38.6s)\n",
      "\n",
      "Optimizing CatBoost hyperparameters...\n",
      "  Best MAE: 3.0236 (Time: 152.4s)\n",
      "\n",
      "üìã OPTIMIZATION SUMMARY\n",
      "--------------------------------------------------\n",
      "XGBoost:\n",
      "  Best CV MAE: 3.0207\n",
      "  Optimization time: 38.6s\n",
      "  Trials completed: 50\n",
      "\n",
      "CatBoost:\n",
      "  Best CV MAE: 3.0236\n",
      "  Optimization time: 152.4s\n",
      "  Trials completed: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import boosting libraries and Optuna\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"OPTUNA-OPTIMIZED BOOSTING MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "X = X_full\n",
    "y = y_full\n",
    "\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Features being used: {list(X.columns)}\")\n",
    "\n",
    "# Define objective functions for Optuna hyperparameter optimization\n",
    "def xgboost_objective(trial):\n",
    "    \"\"\"Objective function for XGBoost hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    \n",
    "    # Try GPU first, fallback to CPU if needed\n",
    "    try:\n",
    "        params['device'] = 'cuda'\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\n",
    "    except:\n",
    "        params['device'] = 'cpu'\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    return -scores.mean()  # Optuna minimizes, so negate MAE\n",
    "\n",
    "def catboost_objective(trial):\n",
    "    \"\"\"Objective function for CatBoost hyperparameter tuning\"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    # Try GPU first, fallback to CPU if needed\n",
    "    try:\n",
    "        params['task_type'] = 'GPU'\n",
    "        model = CatBoostRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\n",
    "    except:\n",
    "        params['task_type'] = 'CPU'\n",
    "        model = CatBoostRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optimize hyperparameters for each model\n",
    "print(\"\\nüîç HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "optimized_params = {}\n",
    "optimization_results = {}\n",
    "\n",
    "# XGBoost optimization\n",
    "print(\"\\nOptimizing XGBoost hyperparameters...\")\n",
    "start_time = time.time()\n",
    "xgb_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "xgb_study.optimize(xgboost_objective, n_trials=50, show_progress_bar=False)\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "optimized_params['XGBoost'] = xgb_study.best_params\n",
    "optimization_results['XGBoost'] = {\n",
    "    'best_mae': xgb_study.best_value,\n",
    "    'optimization_time': xgb_time,\n",
    "    'n_trials': len(xgb_study.trials)\n",
    "}\n",
    "print(f\"  Best MAE: {xgb_study.best_value:.4f} (Time: {xgb_time:.1f}s)\")\n",
    "\n",
    "# CatBoost optimization\n",
    "print(\"\\nOptimizing CatBoost hyperparameters...\")\n",
    "start_time = time.time()\n",
    "cat_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "cat_study.optimize(catboost_objective, n_trials=50, show_progress_bar=False)\n",
    "cat_time = time.time() - start_time\n",
    "\n",
    "optimized_params['CatBoost'] = cat_study.best_params\n",
    "optimization_results['CatBoost'] = {\n",
    "    'best_mae': cat_study.best_value,\n",
    "    'optimization_time': cat_time,\n",
    "    'n_trials': len(cat_study.trials)\n",
    "}\n",
    "print(f\"  Best MAE: {cat_study.best_value:.4f} (Time: {cat_time:.1f}s)\")\n",
    "\n",
    "print(f\"\\nüìã OPTIMIZATION SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, result in optimization_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Best CV MAE: {result['best_mae']:.4f}\")\n",
    "    print(f\"  Optimization time: {result['optimization_time']:.1f}s\")\n",
    "    print(f\"  Trials completed: {result['n_trials']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bf2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è BUILDING OPTIMIZED MODELS\n",
      "--------------------------------------------------\n",
      "\n",
      "Building optimized XGBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0536\n",
      "\n",
      "Building optimized CatBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0536\n",
      "\n",
      "Building optimized CatBoost...\n",
      "  ‚úÖ GPU | CV MAE: 3.0233\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)   GPU       \n",
      "------------------------------------------------------------------------------------------\n",
      "CatBoost               0.9146    3.0233     0.0297 ‚úì        3.6    üöÄ\n",
      "XGBoost                0.9136    3.0536     0.0379 ‚úì        1.2    üöÄ\n",
      "\n",
      "üéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Selected Model: CatBoost\n",
      "Selection Reason: Selected for low overfitting (0.0297) with minimal MAE penalty (0.0000)\n",
      "MAE: 3.0233, Overfitting: 0.0297\n",
      "\n",
      "üèÜ BEST OPTIMIZED MODEL: CatBoost\n",
      "   CV MAE: 3.0233 (¬±0.0872)\n",
      "   CV R¬≤: 0.9146 (¬±0.0053)\n",
      "   Overfitting: 0.0297 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR CatBoost:\n",
      "----------------------------------------\n",
      "  iterations: 169\n",
      "  depth: 3\n",
      "  learning_rate: 0.1418\n",
      "  l2_leaf_reg: 9.8306\n",
      "  bagging_temperature: 0.5858\n",
      "  random_strength: 0.2112\n",
      "  border_count: 74\n",
      "\n",
      "üìä FEATURE IMPORTANCE (CatBoost)\n",
      "----------------------------------------\n",
      "Training CatBoost on full dataset for feature importance...\n",
      "  ‚úÖ GPU | CV MAE: 3.0233\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)   GPU       \n",
      "------------------------------------------------------------------------------------------\n",
      "CatBoost               0.9146    3.0233     0.0297 ‚úì        3.6    üöÄ\n",
      "XGBoost                0.9136    3.0536     0.0379 ‚úì        1.2    üöÄ\n",
      "\n",
      "üéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Selected Model: CatBoost\n",
      "Selection Reason: Selected for low overfitting (0.0297) with minimal MAE penalty (0.0000)\n",
      "MAE: 3.0233, Overfitting: 0.0297\n",
      "\n",
      "üèÜ BEST OPTIMIZED MODEL: CatBoost\n",
      "   CV MAE: 3.0233 (¬±0.0872)\n",
      "   CV R¬≤: 0.9146 (¬±0.0053)\n",
      "   Overfitting: 0.0297 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR CatBoost:\n",
      "----------------------------------------\n",
      "  iterations: 169\n",
      "  depth: 3\n",
      "  learning_rate: 0.1418\n",
      "  l2_leaf_reg: 9.8306\n",
      "  bagging_temperature: 0.5858\n",
      "  random_strength: 0.2112\n",
      "  border_count: 74\n",
      "\n",
      "üìä FEATURE IMPORTANCE (CatBoost)\n",
      "----------------------------------------\n",
      "Training CatBoost on full dataset for feature importance...\n",
      "\n",
      "Top 15 Features:\n",
      "         Expected_Wins: 78.0702\n",
      "  Expected_Wins_Relative: 12.3253\n",
      "                    SV: 3.0826\n",
      "  Struggling_Teams_Expected_Boost: 1.4345\n",
      "                    CG: 0.6768\n",
      "                   SHO: 0.5573\n",
      "                   OBP: 0.4681\n",
      "                IPouts: 0.3829\n",
      "                    AB: 0.2358\n",
      "  Era_Adjusted_BB_Rate: 0.1806\n",
      "                    RA: 0.1703\n",
      "                    HA: 0.1668\n",
      "                    SB: 0.1573\n",
      "                     R: 0.1499\n",
      "      Era_Adjusted_SLG: 0.1448\n",
      "                   HRA: 0.1412\n",
      "                    3B: 0.1355\n",
      "                    DP: 0.1334\n",
      "                  WHIP: 0.1222\n",
      "                    2B: 0.1085\n",
      "                   BBA: 0.0998\n",
      "                   SLG: 0.0963\n",
      "                   REI: 0.0901\n",
      "     Era_Adjusted_WHIP: 0.0899\n",
      "                     E: 0.0817\n",
      "                    SO: 0.0741\n",
      "                    HR: 0.0733\n",
      "  Times_On_Base_Allowed: 0.0618\n",
      "  Pitching_Strong_Expected_Squared: 0.0562\n",
      "                     G: 0.0516\n",
      "         Times_On_Base: 0.0516\n",
      "                    BB: 0.0511\n",
      "                   SOA: 0.0452\n",
      "      Era_Adjusted_OPS: 0.0426\n",
      "                     H: 0.0407\n",
      "           decade_1970: 0.0270\n",
      "      Era_Adjusted_OBP: 0.0266\n",
      "  Elite_Teams_Expected_Squared: 0.0262\n",
      "                    ER: 0.0252\n",
      "           decade_1940: 0.0168\n",
      "           decade_1960: 0.0147\n",
      "                 era_4: 0.0105\n",
      "               mlb_rpg: 0.0028\n",
      "                 era_2: 0.0000\n",
      "                 era_3: 0.0000\n",
      "                 era_1: 0.0000\n",
      "                 era_5: 0.0000\n",
      "                 era_6: 0.0000\n",
      "                 era_7: 0.0000\n",
      "                 era_8: 0.0000\n",
      "           decade_1920: 0.0000\n",
      "           decade_1930: 0.0000\n",
      "           Archetype_2: 0.0000\n",
      "           decade_1950: 0.0000\n",
      "           Archetype_1: 0.0000\n",
      "           decade_1980: 0.0000\n",
      "           decade_1990: 0.0000\n",
      "           decade_2000: 0.0000\n",
      "\n",
      "‚ú® OPTIMIZATION COMPLETE!\n",
      "   Best model improved MAE through Optuna hyperparameter tuning\n",
      "   Ready for ensemble or final predictions\n",
      "\n",
      "Top 15 Features:\n",
      "         Expected_Wins: 78.0702\n",
      "  Expected_Wins_Relative: 12.3253\n",
      "                    SV: 3.0826\n",
      "  Struggling_Teams_Expected_Boost: 1.4345\n",
      "                    CG: 0.6768\n",
      "                   SHO: 0.5573\n",
      "                   OBP: 0.4681\n",
      "                IPouts: 0.3829\n",
      "                    AB: 0.2358\n",
      "  Era_Adjusted_BB_Rate: 0.1806\n",
      "                    RA: 0.1703\n",
      "                    HA: 0.1668\n",
      "                    SB: 0.1573\n",
      "                     R: 0.1499\n",
      "      Era_Adjusted_SLG: 0.1448\n",
      "                   HRA: 0.1412\n",
      "                    3B: 0.1355\n",
      "                    DP: 0.1334\n",
      "                  WHIP: 0.1222\n",
      "                    2B: 0.1085\n",
      "                   BBA: 0.0998\n",
      "                   SLG: 0.0963\n",
      "                   REI: 0.0901\n",
      "     Era_Adjusted_WHIP: 0.0899\n",
      "                     E: 0.0817\n",
      "                    SO: 0.0741\n",
      "                    HR: 0.0733\n",
      "  Times_On_Base_Allowed: 0.0618\n",
      "  Pitching_Strong_Expected_Squared: 0.0562\n",
      "                     G: 0.0516\n",
      "         Times_On_Base: 0.0516\n",
      "                    BB: 0.0511\n",
      "                   SOA: 0.0452\n",
      "      Era_Adjusted_OPS: 0.0426\n",
      "                     H: 0.0407\n",
      "           decade_1970: 0.0270\n",
      "      Era_Adjusted_OBP: 0.0266\n",
      "  Elite_Teams_Expected_Squared: 0.0262\n",
      "                    ER: 0.0252\n",
      "           decade_1940: 0.0168\n",
      "           decade_1960: 0.0147\n",
      "                 era_4: 0.0105\n",
      "               mlb_rpg: 0.0028\n",
      "                 era_2: 0.0000\n",
      "                 era_3: 0.0000\n",
      "                 era_1: 0.0000\n",
      "                 era_5: 0.0000\n",
      "                 era_6: 0.0000\n",
      "                 era_7: 0.0000\n",
      "                 era_8: 0.0000\n",
      "           decade_1920: 0.0000\n",
      "           decade_1930: 0.0000\n",
      "           Archetype_2: 0.0000\n",
      "           decade_1950: 0.0000\n",
      "           Archetype_1: 0.0000\n",
      "           decade_1980: 0.0000\n",
      "           decade_1990: 0.0000\n",
      "           decade_2000: 0.0000\n",
      "\n",
      "‚ú® OPTIMIZATION COMPLETE!\n",
      "   Best model improved MAE through Optuna hyperparameter tuning\n",
      "   Ready for ensemble or final predictions\n"
     ]
    }
   ],
   "source": [
    "# Build models with optimized parameters and perform detailed comparison\n",
    "print(\"\\nüèóÔ∏è BUILDING OPTIMIZED MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create models with optimized parameters\n",
    "def create_optimized_model(model_name, params):\n",
    "    \"\"\"Create a model instance with optimized parameters\"\"\"\n",
    "    if model_name == 'XGBoost':\n",
    "        # Try GPU first, fallback to CPU\n",
    "        try:\n",
    "            params_gpu = params.copy()\n",
    "            params_gpu['device'] = 'cuda'\n",
    "            params_gpu['tree_method'] = 'hist'\n",
    "            params_gpu['verbosity'] = 0\n",
    "            model = XGBRegressor(**params_gpu)\n",
    "            # Test if GPU works\n",
    "            model.fit(X[:100], y[:100])\n",
    "            return model, '‚úÖ GPU'\n",
    "        except:\n",
    "            params_cpu = params.copy()\n",
    "            params_cpu['device'] = 'cpu'\n",
    "            params_cpu['tree_method'] = 'hist' \n",
    "            params_cpu['verbosity'] = 0\n",
    "            return XGBRegressor(**params_cpu), '‚ö†Ô∏è CPU'\n",
    "            \n",
    "    elif model_name == 'CatBoost':\n",
    "        try:\n",
    "            params_gpu = params.copy()\n",
    "            params_gpu['task_type'] = 'GPU'\n",
    "            params_gpu['verbose'] = False\n",
    "            model = CatBoostRegressor(**params_gpu)\n",
    "            # Test if GPU works\n",
    "            model.fit(X[:100], y[:100])\n",
    "            return model, '‚úÖ GPU'\n",
    "        except:\n",
    "            params_cpu = params.copy()\n",
    "            params_cpu['task_type'] = 'CPU'\n",
    "            params_cpu['verbose'] = False\n",
    "            return CatBoostRegressor(**params_cpu), '‚ö†Ô∏è CPU'\n",
    "\n",
    "# Build optimized models\n",
    "optimized_models = {}\n",
    "cv_results_optimized = {}\n",
    "\n",
    "for name, params in optimized_params.items():\n",
    "    print(f\"\\nBuilding optimized {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, gpu_status = create_optimized_model(name, params)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        model, X, y,\n",
    "        cv=5,\n",
    "        scoring=['r2', 'neg_mean_absolute_error'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=1 if 'GPU' in gpu_status else -1\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    cv_results_optimized[name] = {\n",
    "        'test_r2': cv_scores['test_r2'].mean(),\n",
    "        'test_r2_std': cv_scores['test_r2'].std(),\n",
    "        'test_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "        'test_mae_std': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "        'train_r2': cv_scores['train_r2'].mean(),\n",
    "        'overfitting': cv_scores['train_r2'].mean() - cv_scores['test_r2'].mean(),\n",
    "        'time': end_time - start_time,\n",
    "        'gpu_status': gpu_status\n",
    "    }\n",
    "    \n",
    "    optimized_models[name] = model\n",
    "    print(f\"  {gpu_status} | CV MAE: {cv_results_optimized[name]['test_mae']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OPTIMIZED MODELS RESULTS SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<22} {'Test R¬≤':<10} {'Test MAE':<11} {'Overfitting':<13} {'Time (s)':<10} {'GPU':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Sort by Test MAE (lower is better) for initial ranking\n",
    "sorted_results = sorted(cv_results_optimized.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "for name, result in sorted_results:\n",
    "    overfit_warning = \"‚ö†Ô∏è\" if result['overfitting'] > 0.05 else \"‚úì\"\n",
    "    gpu_icon = \"üöÄ\" if \"GPU\" in result['gpu_status'] else \"üíª\"\n",
    "    print(f\"{name:<22} {result['test_r2']:.4f}    {result['test_mae']:.4f}     \"\n",
    "          f\"{result['overfitting']:>6.4f} {overfit_warning:<5} {result['time']:>6.1f}    {gpu_icon}\")\n",
    "\n",
    "print(f\"\\nüéØ INTELLIGENT MODEL SELECTION (Balancing Performance & Overfitting)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Select best model considering both performance and overfitting\n",
    "def select_best_model_with_overfitting_control(results, overfitting_threshold=0.05, mae_tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Select the best model balancing performance and overfitting.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of model results\n",
    "        overfitting_threshold: Maximum acceptable overfitting gap (train_r2 - test_r2)\n",
    "        mae_tolerance: MAE tolerance for accepting a less overfitting model over the best performer\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_model_name, reason)\n",
    "    \"\"\"\n",
    "    # Sort by MAE first\n",
    "    sorted_by_mae = sorted(results.items(), key=lambda x: x[1]['test_mae'])\n",
    "    \n",
    "    # Find models that don't overfit significantly\n",
    "    non_overfitting_models = [\n",
    "        (name, result) for name, result in sorted_by_mae \n",
    "        if result['overfitting'] <= overfitting_threshold\n",
    "    ]\n",
    "    \n",
    "    best_mae_model = sorted_by_mae[0]\n",
    "    best_mae = best_mae_model[1]['test_mae']\n",
    "    \n",
    "    if non_overfitting_models:\n",
    "        # Check if the best non-overfitting model is within acceptable MAE tolerance\n",
    "        best_non_overfit = non_overfitting_models[0]\n",
    "        mae_diff = best_non_overfit[1]['test_mae'] - best_mae\n",
    "        \n",
    "        if mae_diff <= mae_tolerance:\n",
    "            return best_non_overfit[0], f\"Selected for low overfitting ({best_non_overfit[1]['overfitting']:.4f}) with minimal MAE penalty ({mae_diff:.4f})\"\n",
    "        else:\n",
    "            # Check if the best MAE model overfits significantly\n",
    "            if best_mae_model[1]['overfitting'] > overfitting_threshold:\n",
    "                return best_non_overfit[0], f\"Selected to avoid overfitting. Best MAE model overfits by {best_mae_model[1]['overfitting']:.4f}\"\n",
    "            else:\n",
    "                return best_mae_model[0], f\"Selected for best MAE ({best_mae:.4f}) with acceptable overfitting ({best_mae_model[1]['overfitting']:.4f})\"\n",
    "    else:\n",
    "        # All models overfit, choose the one with least overfitting among top performers\n",
    "        print(\"  ‚ö†Ô∏è All models show overfitting. Selecting least overfitting among top 3 MAE performers.\")\n",
    "        top_3_mae = sorted_by_mae[:3]\n",
    "        least_overfit_of_top3 = min(top_3_mae, key=lambda x: x[1]['overfitting'])\n",
    "        return least_overfit_of_top3[0], f\"Least overfitting ({least_overfit_of_top3[1]['overfitting']:.4f}) among top 3 MAE performers\"\n",
    "\n",
    "# Apply intelligent model selection\n",
    "best_model_name, selection_reason = select_best_model_with_overfitting_control(cv_results_optimized)\n",
    "best_model = optimized_models[best_model_name]\n",
    "best_mae = cv_results_optimized[best_model_name]['test_mae']\n",
    "best_overfitting = cv_results_optimized[best_model_name]['overfitting']\n",
    "\n",
    "print(f\"\\nSelected Model: {best_model_name}\")\n",
    "print(f\"Selection Reason: {selection_reason}\")\n",
    "print(f\"MAE: {best_mae:.4f}, Overfitting: {best_overfitting:.4f}\")\n",
    "\n",
    "# Show comparison with pure MAE-based selection\n",
    "pure_mae_best = sorted_results[0][0]\n",
    "if pure_mae_best != best_model_name:\n",
    "    pure_mae_result = cv_results_optimized[pure_mae_best]\n",
    "    print(f\"\\nComparison with pure MAE selection:\")\n",
    "    print(f\"  Pure MAE Best: {pure_mae_best} (MAE: {pure_mae_result['test_mae']:.4f}, Overfitting: {pure_mae_result['overfitting']:.4f})\")\n",
    "    print(f\"  Selected Model: {best_model_name} (MAE: {best_mae:.4f}, Overfitting: {best_overfitting:.4f})\")\n",
    "    mae_diff = best_mae - pure_mae_result['test_mae']\n",
    "    overfit_improvement = pure_mae_result['overfitting'] - best_overfitting\n",
    "    print(f\"  Trade-off: +{mae_diff:.4f} MAE for -{overfit_improvement:.4f} overfitting reduction\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST OPTIMIZED MODEL: {best_model_name}\")\n",
    "print(f\"   CV MAE: {best_mae:.4f} (¬±{cv_results_optimized[best_model_name]['test_mae_std']:.4f})\")\n",
    "print(f\"   CV R¬≤: {cv_results_optimized[best_model_name]['test_r2']:.4f} (¬±{cv_results_optimized[best_model_name]['test_r2_std']:.4f})\")\n",
    "print(f\"   Overfitting: {best_overfitting:.4f} ({'‚ö†Ô∏è' if best_overfitting > 0.05 else '‚úì'} {'High' if best_overfitting > 0.05 else 'Acceptable'})\")\n",
    "\n",
    "# Display optimized parameters\n",
    "print(f\"\\nüîß OPTIMIZED PARAMETERS FOR {best_model_name}:\")\n",
    "print(\"-\" * 40)\n",
    "for param, value in optimized_params[best_model_name].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "# Feature importance for best model\n",
    "print(f\"\\nüìä FEATURE IMPORTANCE ({best_model_name})\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training {best_model_name} on full dataset for feature importance...\")\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 15 Features:\")\n",
    "    for i, row in importance_df.head(60).iterrows():\n",
    "        print(f\"  {row['feature']:>20}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® OPTIMIZATION COMPLETE!\")\n",
    "print(f\"   Best model improved MAE through Optuna hyperparameter tuning\")\n",
    "print(f\"   Ready for ensemble or final predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc9b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA-OPTIMIZED LINEAR MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (1812, 58)\n",
      "Using 58 engineered features\n",
      "\n",
      "üõ°Ô∏è Found 2 Archetype_ features - excluding from scaling:\n",
      "    ‚Ä¢ Archetype_1\n",
      "    ‚Ä¢ Archetype_2\n",
      "üìä Scaling 56 continuous features, preserving 2 one-hot features\n",
      "\n",
      "üîç LINEAR MODELS HYPERPARAMETER OPTIMIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Optimizing Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7258 (Time: 0.6s)\n",
      "\n",
      "Optimizing Lasso hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7258 (Time: 0.6s)\n",
      "\n",
      "Optimizing Lasso hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7195 (Time: 0.7s)\n",
      "\n",
      "Optimizing ElasticNet hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7195 (Time: 0.7s)\n",
      "\n",
      "Optimizing ElasticNet hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7203 (Time: 0.7s)\n",
      "\n",
      "Optimizing Huber hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7203 (Time: 0.7s)\n",
      "\n",
      "Optimizing Huber hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7262 (Time: 3.3s)\n",
      "\n",
      "Optimizing Polynomial_Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7262 (Time: 3.3s)\n",
      "\n",
      "Optimizing Polynomial_Ridge hyperparameters...\n",
      "  ‚úÖ Best MAE: 2.7726 (Time: 18.4s)\n",
      "\n",
      "Testing Linear Regression (no hyperparameters)...\n",
      "  ‚úÖ MAE: 2.7456 (Time: 0.0s)\n",
      "\n",
      "üìã LINEAR MODELS OPTIMIZATION SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ridge:\n",
      "  Best CV MAE: 2.7258\n",
      "  Optimization time: 0.6s\n",
      "  Trials completed: 30\n",
      "\n",
      "Lasso:\n",
      "  Best CV MAE: 2.7195\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "ElasticNet:\n",
      "  Best CV MAE: 2.7203\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "Huber:\n",
      "  Best CV MAE: 2.7262\n",
      "  Optimization time: 3.3s\n",
      "  Trials completed: 30\n",
      "\n",
      "Polynomial_Ridge:\n",
      "  Best CV MAE: 2.7726\n",
      "  Optimization time: 18.4s\n",
      "  Trials completed: 30\n",
      "\n",
      "LinearRegression:\n",
      "  Best CV MAE: 2.7456\n",
      "  Optimization time: 0.0s\n",
      "  Trials completed: 1\n",
      "\n",
      "\n",
      "üõ°Ô∏è SMART SCALING SUMMARY:\n",
      "   ‚úÖ Protected 2 Archetype_ features from scaling in regular models\n",
      "   üìä Used column-aware scaling for 56 continuous features\n",
      "   ‚ö†Ô∏è Polynomial models use standard scaling after feature expansion (column structure lost)\n",
      "  ‚úÖ Best MAE: 2.7726 (Time: 18.4s)\n",
      "\n",
      "Testing Linear Regression (no hyperparameters)...\n",
      "  ‚úÖ MAE: 2.7456 (Time: 0.0s)\n",
      "\n",
      "üìã LINEAR MODELS OPTIMIZATION SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ridge:\n",
      "  Best CV MAE: 2.7258\n",
      "  Optimization time: 0.6s\n",
      "  Trials completed: 30\n",
      "\n",
      "Lasso:\n",
      "  Best CV MAE: 2.7195\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "ElasticNet:\n",
      "  Best CV MAE: 2.7203\n",
      "  Optimization time: 0.7s\n",
      "  Trials completed: 30\n",
      "\n",
      "Huber:\n",
      "  Best CV MAE: 2.7262\n",
      "  Optimization time: 3.3s\n",
      "  Trials completed: 30\n",
      "\n",
      "Polynomial_Ridge:\n",
      "  Best CV MAE: 2.7726\n",
      "  Optimization time: 18.4s\n",
      "  Trials completed: 30\n",
      "\n",
      "LinearRegression:\n",
      "  Best CV MAE: 2.7456\n",
      "  Optimization time: 0.0s\n",
      "  Trials completed: 1\n",
      "\n",
      "\n",
      "üõ°Ô∏è SMART SCALING SUMMARY:\n",
      "   ‚úÖ Protected 2 Archetype_ features from scaling in regular models\n",
      "   üìä Used column-aware scaling for 56 continuous features\n",
      "   ‚ö†Ô∏è Polynomial models use standard scaling after feature expansion (column structure lost)\n"
     ]
    }
   ],
   "source": [
    "# Import linear model libraries and continue using Optuna\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Comprehensive warning suppression for clean output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\") \n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # üîß Suppress all convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Objective did not converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*coordinate_descent.*\")  # üîß Suppress coordinate descent warnings\n",
    "\n",
    "print(\"OPTUNA-OPTIMIZED LINEAR MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "X_linear = X_full\n",
    "y_linear = y_full\n",
    "\n",
    "print(f\"\\nDataset shape: {X_linear.shape}\")\n",
    "print(f\"Using {len(available_features)} engineered features\")\n",
    "\n",
    "# üõ°Ô∏è SMART SCALING: Check for one-hot encoded features to exclude from scaling\n",
    "archetype_features = [col for col in X_linear.columns if col.startswith('Archetype_')]\n",
    "continuous_features = [col for col in X_linear.columns if col not in archetype_features]\n",
    "\n",
    "if archetype_features:\n",
    "    print(f\"\\nüõ°Ô∏è Found {len(archetype_features)} Archetype_ features - excluding from scaling:\")\n",
    "    for feat in archetype_features:\n",
    "        print(f\"    ‚Ä¢ {feat}\")\n",
    "    print(f\"üìä Scaling {len(continuous_features)} continuous features, preserving {len(archetype_features)} one-hot features\")\n",
    "    \n",
    "    # Create smart preprocessor that only scales continuous features\n",
    "    def create_smart_preprocessor():\n",
    "        return ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('scaler', StandardScaler(), continuous_features),\n",
    "                ('passthrough', 'passthrough', archetype_features)\n",
    "            ],\n",
    "            remainder='passthrough'  # Keep any other features as-is\n",
    "        )\n",
    "        \n",
    "    # Special preprocessor for polynomial features (scales all after polynomial expansion)\n",
    "    def create_polynomial_preprocessor():\n",
    "        return StandardScaler()  # After polynomial expansion, we lose column structure anyway\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  No Archetype_ features found - using standard scaling for all {len(continuous_features)} features\")\n",
    "    \n",
    "    # Use standard scaler for all features\n",
    "    def create_smart_preprocessor():\n",
    "        return StandardScaler()\n",
    "        \n",
    "    def create_polynomial_preprocessor():\n",
    "        return StandardScaler()\n",
    "\n",
    "# Define objective functions for Optuna hyperparameter optimization\n",
    "def ridge_objective(trial):\n",
    "    \"\"\"Objective function for Ridge regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', create_smart_preprocessor()),\n",
    "        ('model', Ridge(alpha=alpha, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def lasso_objective(trial):\n",
    "    \"\"\"Objective function for Lasso regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)  # üîß Raised minimum alpha for better convergence\n",
    "    max_iter = trial.suggest_int('max_iter', 10000, 20000)  # üîß Much higher iteration range\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', create_smart_preprocessor()),\n",
    "        ('model', Lasso(alpha=alpha, max_iter=max_iter, tol=1e-3, random_state=42, warm_start=False))  # üîß Relaxed tolerance, no warm start\n",
    "    ])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # üîß Local warning suppression\n",
    "        scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def elasticnet_objective(trial):\n",
    "    \"\"\"Objective function for ElasticNet regression hyperparameter tuning\"\"\"\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)  # üîß Raised minimum alpha\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "    max_iter = trial.suggest_int('max_iter', 10000, 20000)  # üîß Much higher iteration range\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', create_smart_preprocessor()),\n",
    "        ('model', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, tol=1e-3, random_state=42, warm_start=False))  # üîß Relaxed tolerance\n",
    "    ])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # üîß Local warning suppression  \n",
    "        scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def huber_objective(trial):\n",
    "    \"\"\"Objective function for Huber regression hyperparameter tuning\"\"\"\n",
    "    epsilon = trial.suggest_float('epsilon', 1.1, 3.0)\n",
    "    alpha = trial.suggest_float('alpha', 0.0001, 1.0, log=True)\n",
    "    max_iter = trial.suggest_int('max_iter', 1000, 5000)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', create_smart_preprocessor()),\n",
    "        ('model', HuberRegressor(epsilon=epsilon, alpha=alpha, max_iter=max_iter, tol=1e-05))\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def polynomial_ridge_objective(trial):\n",
    "    \"\"\"Objective function for Polynomial Ridge regression hyperparameter tuning\"\"\"\n",
    "    degree = trial.suggest_int('degree', 2, 3)\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 1000.0, log=True)\n",
    "    include_bias = trial.suggest_categorical('include_bias', [True, False])\n",
    "    \n",
    "    # üîß FIXED: Use polynomial preprocessor that works with numpy arrays\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias)),\n",
    "        ('scaler', create_polynomial_preprocessor()),  # Simple StandardScaler after polynomial expansion\n",
    "        ('model', Ridge(alpha=alpha, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress any polynomial-related warnings\n",
    "        scores = cross_val_score(model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optimize hyperparameters for each linear model\n",
    "print(\"\\nüîç LINEAR MODELS HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "optimized_linear_params = {}\n",
    "linear_optimization_results = {}\n",
    "\n",
    "# List of models to optimize\n",
    "linear_models_to_optimize = [\n",
    "    ('Ridge', ridge_objective),\n",
    "    ('Lasso', lasso_objective), \n",
    "    ('ElasticNet', elasticnet_objective),\n",
    "    ('Huber', huber_objective),\n",
    "    ('Polynomial_Ridge', polynomial_ridge_objective)  # Now fixed\n",
    "]\n",
    "\n",
    "for model_name, objective_func in linear_models_to_optimize:\n",
    "    print(f\"\\nOptimizing {model_name} hyperparameters...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create study for this model\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        study.optimize(objective_func, n_trials=30, show_progress_bar=False)\n",
    "        optimization_time = time.time() - start_time\n",
    "        \n",
    "        optimized_linear_params[model_name] = study.best_params\n",
    "        linear_optimization_results[model_name] = {\n",
    "            'best_mae': study.best_value,\n",
    "            'optimization_time': optimization_time,\n",
    "            'n_trials': len(study.trials),\n",
    "            'status': 'Success'\n",
    "        }\n",
    "        print(f\"  ‚úÖ Best MAE: {study.best_value:.4f} (Time: {optimization_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "        linear_optimization_results[model_name] = {\n",
    "            'status': 'Failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Also include basic Linear Regression (no hyperparameters to optimize)\n",
    "print(f\"\\nTesting Linear Regression (no hyperparameters)...\")\n",
    "start_time = time.time()\n",
    "linear_reg_model = Pipeline([\n",
    "    ('preprocessor', create_smart_preprocessor()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "scores = cross_val_score(linear_reg_model, X_linear, y_linear, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "linear_reg_time = time.time() - start_time\n",
    "\n",
    "optimized_linear_params['LinearRegression'] = {}\n",
    "linear_optimization_results['LinearRegression'] = {\n",
    "    'best_mae': -scores.mean(),\n",
    "    'optimization_time': linear_reg_time,\n",
    "    'n_trials': 1,\n",
    "    'status': 'Success'\n",
    "}\n",
    "print(f\"  ‚úÖ MAE: {-scores.mean():.4f} (Time: {linear_reg_time:.1f}s)\")\n",
    "\n",
    "print(f\"\\nüìã LINEAR MODELS OPTIMIZATION SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "successful_linear = {k: v for k, v in linear_optimization_results.items() if v.get('status') == 'Success'}\n",
    "for model_name, result in successful_linear.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Best CV MAE: {result['best_mae']:.4f}\")\n",
    "    print(f\"  Optimization time: {result['optimization_time']:.1f}s\")\n",
    "    print(f\"  Trials completed: {result['n_trials']}\")\n",
    "    print()\n",
    "\n",
    "# Show smart scaling info\n",
    "print(f\"\\nüõ°Ô∏è SMART SCALING SUMMARY:\")\n",
    "if archetype_features:\n",
    "    print(f\"   ‚úÖ Protected {len(archetype_features)} Archetype_ features from scaling in regular models\")\n",
    "    print(f\"   üìä Used column-aware scaling for {len(continuous_features)} continuous features\")\n",
    "    print(f\"   ‚ö†Ô∏è Polynomial models use standard scaling after feature expansion (column structure lost)\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è No Archetype_ features found - standard scaling applied to all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca7aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è BUILDING OPTIMIZED LINEAR MODELS\n",
      "--------------------------------------------------\n",
      "\n",
      "üõ°Ô∏è SCALING SUMMARY:\n",
      "   Continuous features (scaled): 56\n",
      "   One-hot features (preserved): 2\n",
      "   Smart scaling: ‚úÖ Activated (except polynomial models)\n",
      "\n",
      "Building optimized Ridge...\n",
      "  ‚úÖ CV MAE: 2.7343\n",
      "\n",
      "Building optimized Lasso...\n",
      "  ‚úÖ CV MAE: 2.7272\n",
      "\n",
      "Building optimized ElasticNet...\n",
      "  ‚úÖ CV MAE: 2.7278\n",
      "\n",
      "Building optimized Huber...\n",
      "  ‚úÖ CV MAE: 2.7339\n",
      "\n",
      "Building optimized Polynomial_Ridge...\n",
      "  ‚úÖ CV MAE: 2.7789\n",
      "\n",
      "Building optimized LinearRegression...\n",
      "  ‚úÖ CV MAE: 2.7589\n",
      "\n",
      "==========================================================================================\n",
      "OPTIMIZED LINEAR MODELS RESULTS SUMMARY\n",
      "==========================================================================================\n",
      "Model                  Test R¬≤    Test MAE    Overfitting   Time (s)  \n",
      "------------------------------------------------------------------------------------------\n",
      "Lasso                  0.9311    2.7272     0.0043 ‚úì        0.0\n",
      "ElasticNet             0.9310    2.7278     0.0043 ‚úì        0.0\n",
      "Huber                  0.9305    2.7339     0.0052 ‚úì        0.1\n",
      "Ridge                  0.9306    2.7343     0.0051 ‚úì        0.0\n",
      "LinearRegression       0.9293    2.7589     0.0066 ‚úì        0.0\n",
      "Polynomial_Ridge       0.9278    2.7789     0.0166 ‚úì        0.2\n",
      "\n",
      "üéØ INTELLIGENT LINEAR MODEL SELECTION\n",
      "--------------------------------------------------\n",
      "\n",
      "Selected Linear Model: Lasso\n",
      "Selection Reason: Selected for low overfitting (0.0043) with minimal MAE penalty (0.0000)\n",
      "MAE: 2.7272, Overfitting: 0.0043\n",
      "\n",
      "üèÜ BEST OPTIMIZED LINEAR MODEL: Lasso\n",
      "   CV MAE: 2.7272 (¬±0.0511)\n",
      "   CV R¬≤: 0.9311 (¬±0.0067)\n",
      "   Overfitting: 0.0043 (‚úì Acceptable)\n",
      "\n",
      "üîß OPTIMIZED PARAMETERS FOR Lasso:\n",
      "----------------------------------------\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "üîÑ COMPARISON WITH BOOSTING MODELS\n",
      "--------------------------------------------------\n",
      "Best Boosting Model: CatBoost (MAE: 3.0233)\n",
      "Best Linear Model: Lasso (MAE: 2.7272)\n",
      "‚úÖ Linear model outperforms boosting by 0.2961\n",
      "\n",
      "üõ°Ô∏è SMART SCALING APPLIED:\n",
      "   ‚úÖ Protected 2 one-hot encoded Archetype_ features from scaling\n",
      "   üìä Scaled 56 continuous features using ColumnTransformer\n",
      "      ‚Ä¢ Archetype_1 (preserved as 0/1 values)\n",
      "      ‚Ä¢ Archetype_2 (preserved as 0/1 values)\n",
      "   ‚ö†Ô∏è Polynomial_Ridge uses StandardScaler after feature expansion (loses column structure)\n",
      "\n",
      "‚ú® LINEAR OPTIMIZATION COMPLETE!\n",
      "   All linear models optimized with Optuna hyperparameter tuning\n",
      "   Smart scaling preserves one-hot encoded categorical features\n",
      "   Ready for enhanced ensemble with optimized linear models\n"
     ]
    }
   ],
   "source": [
    "# Build optimized linear models and perform detailed comparison\n",
    "print(\"\\nüèóÔ∏è BUILDING OPTIMIZED LINEAR MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create models with optimized parameters using smart scaling\n",
    "def create_optimized_linear_model(model_name, params):\n",
    "    \"\"\"Create a linear model instance with optimized parameters and smart scaling\"\"\"\n",
    "    \n",
    "    if model_name == 'Ridge':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', create_smart_preprocessor()),\n",
    "            ('model', Ridge(alpha=params['alpha'], random_state=42))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Lasso':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', create_smart_preprocessor()),\n",
    "            ('model', Lasso(\n",
    "                alpha=params['alpha'], \n",
    "                max_iter=params['max_iter'], \n",
    "                tol=1e-4,  # üîß Improved tolerance\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'ElasticNet':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', create_smart_preprocessor()),\n",
    "            ('model', ElasticNet(\n",
    "                alpha=params['alpha'], \n",
    "                l1_ratio=params['l1_ratio'], \n",
    "                max_iter=params['max_iter'],\n",
    "                tol=1e-4,  # üîß Improved tolerance \n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Huber':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', create_smart_preprocessor()),\n",
    "            ('model', HuberRegressor(\n",
    "                epsilon=params['epsilon'],\n",
    "                alpha=params['alpha'],\n",
    "                max_iter=params['max_iter'],\n",
    "                tol=1e-05\n",
    "            ))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'Polynomial_Ridge':\n",
    "        # üîß FIXED: Use correct pipeline structure for polynomial models\n",
    "        return Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=params['degree'], include_bias=params['include_bias'])),\n",
    "            ('scaler', create_polynomial_preprocessor()),  # Use polynomial preprocessor\n",
    "            ('model', Ridge(alpha=params['alpha'], random_state=42))\n",
    "        ])\n",
    "    \n",
    "    elif model_name == 'LinearRegression':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', create_smart_preprocessor()),\n",
    "            ('model', LinearRegression())\n",
    "        ])\n",
    "\n",
    "# Build optimized linear models\n",
    "optimized_linear_models = {}\n",
    "cv_results_linear_optimized = {}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è SCALING SUMMARY:\")\n",
    "if archetype_features:\n",
    "    print(f\"   Continuous features (scaled): {len(continuous_features)}\")\n",
    "    print(f\"   One-hot features (preserved): {len(archetype_features)}\")\n",
    "    print(f\"   Smart scaling: ‚úÖ Activated (except polynomial models)\")\n",
    "else:\n",
    "    print(f\"   All features (scaled): {len(continuous_features)}\")\n",
    "    print(f\"   Smart scaling: ‚ÑπÔ∏è Not needed (no Archetype_ features)\")\n",
    "\n",
    "for name in successful_linear.keys():\n",
    "    print(f\"\\nBuilding optimized {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model = create_optimized_linear_model(name, optimized_linear_params[name])\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_validate(\n",
    "            model, X_linear, y_linear,\n",
    "            cv=cv,\n",
    "            scoring=['r2', 'neg_mean_absolute_error'],\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        cv_results_linear_optimized[name] = {\n",
    "            'test_r2': cv_scores['test_r2'].mean(),\n",
    "            'test_r2_std': cv_scores['test_r2'].std(),\n",
    "            'test_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "            'test_mae_std': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "            'train_r2': cv_scores['train_r2'].mean(),\n",
    "            'overfitting': cv_scores['train_r2'].mean() - cv_scores['test_r2'].mean(),\n",
    "            'time': end_time - start_time\n",
    "        }\n",
    "        \n",
    "        optimized_linear_models[name] = model\n",
    "        print(f\"  ‚úÖ CV MAE: {cv_results_linear_optimized[name]['test_mae']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to build {name}: {str(e)}\")\n",
    "        # Remove from successful_linear if it fails here\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OPTIMIZED LINEAR MODELS RESULTS SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<22} {'Test R¬≤':<10} {'Test MAE':<11} {'Overfitting':<13} {'Time (s)':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Sort by Test MAE (lower is better)\n",
    "sorted_linear_results = sorted(cv_results_linear_optimized.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "for name, result in sorted_linear_results:\n",
    "    overfit_warning = \"‚ö†Ô∏è\" if result['overfitting'] > 0.05 else \"‚úì\"\n",
    "    print(f\"{name:<22} {result['test_r2']:.4f}    {result['test_mae']:.4f}     \"\n",
    "          f\"{result['overfitting']:>6.4f} {overfit_warning:<5} {result['time']:>6.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ INTELLIGENT LINEAR MODEL SELECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Apply intelligent model selection (similar to boosting models)\n",
    "best_linear_model_name, linear_selection_reason = select_best_model_with_overfitting_control(\n",
    "    cv_results_linear_optimized, \n",
    "    overfitting_threshold=0.05, \n",
    "    mae_tolerance=0.01\n",
    ")\n",
    "best_linear_model = optimized_linear_models[best_linear_model_name]\n",
    "best_linear_mae = cv_results_linear_optimized[best_linear_model_name]['test_mae']\n",
    "best_linear_overfitting = cv_results_linear_optimized[best_linear_model_name]['overfitting']\n",
    "\n",
    "print(f\"\\nSelected Linear Model: {best_linear_model_name}\")\n",
    "print(f\"Selection Reason: {linear_selection_reason}\")\n",
    "print(f\"MAE: {best_linear_mae:.4f}, Overfitting: {best_linear_overfitting:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST OPTIMIZED LINEAR MODEL: {best_linear_model_name}\")\n",
    "print(f\"   CV MAE: {best_linear_mae:.4f} (¬±{cv_results_linear_optimized[best_linear_model_name]['test_mae_std']:.4f})\")\n",
    "print(f\"   CV R¬≤: {cv_results_linear_optimized[best_linear_model_name]['test_r2']:.4f} (¬±{cv_results_linear_optimized[best_linear_model_name]['test_r2_std']:.4f})\")\n",
    "print(f\"   Overfitting: {best_linear_overfitting:.4f} ({'‚ö†Ô∏è' if best_linear_overfitting > 0.05 else '‚úì'} {'High' if best_linear_overfitting > 0.05 else 'Acceptable'})\")\n",
    "\n",
    "# Display optimized parameters\n",
    "if optimized_linear_params[best_linear_model_name]:  # Skip if empty (LinearRegression)\n",
    "    print(f\"\\nüîß OPTIMIZED PARAMETERS FOR {best_linear_model_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for param, value in optimized_linear_params[best_linear_model_name].items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {param}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüîÑ COMPARISON WITH BOOSTING MODELS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Best Boosting Model: {best_model_name} (MAE: {best_mae:.4f})\")\n",
    "print(f\"Best Linear Model: {best_linear_model_name} (MAE: {best_linear_mae:.4f})\")\n",
    "\n",
    "if best_linear_mae < best_mae:\n",
    "    print(f\"‚úÖ Linear model outperforms boosting by {best_mae - best_linear_mae:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Boosting model outperforms linear by {best_linear_mae - best_mae:.4f}\")\n",
    "\n",
    "# Display smart scaling summary\n",
    "print(f\"\\nüõ°Ô∏è SMART SCALING APPLIED:\")\n",
    "if archetype_features:\n",
    "    print(f\"   ‚úÖ Protected {len(archetype_features)} one-hot encoded Archetype_ features from scaling\")\n",
    "    print(f\"   üìä Scaled {len(continuous_features)} continuous features using ColumnTransformer\")\n",
    "    for feat in archetype_features:\n",
    "        print(f\"      ‚Ä¢ {feat} (preserved as 0/1 values)\")\n",
    "    print(f\"   ‚ö†Ô∏è Polynomial_Ridge uses StandardScaler after feature expansion (loses column structure)\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è No Archetype_ features found - applied standard scaling to all features\")\n",
    "\n",
    "print(f\"\\n‚ú® LINEAR OPTIMIZATION COMPLETE!\")\n",
    "print(f\"   All linear models optimized with Optuna hyperparameter tuning\")\n",
    "print(f\"   Smart scaling preserves one-hot encoded categorical features\")\n",
    "print(f\"   Ready for enhanced ensemble with optimized linear models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "957a735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Top 2 Optimized Linear Models:\n",
      "   1. Lasso: MAE = 2.7272, R¬≤ = 0.9311\n",
      "   2. ElasticNet: MAE = 2.7278, R¬≤ = 0.9310\n",
      "\n",
      "üìä EXTRACTING FEATURE IMPORTANCES...\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing Lasso...\n",
      "   ‚úÖ Extracted 58 feature coefficients\n",
      "   üìà Top 5 most important features:\n",
      "      1. scaler__Expected_Wins: 6.5823 (|6.5823|)\n",
      "      2. scaler__IPouts: 4.2999 (|4.2999|)\n",
      "      3. scaler__AB: -4.2306 (|4.2306|)\n",
      "      4. scaler__SV: 4.2003 (|4.2003|)\n",
      "      5. scaler__CG: 3.5754 (|3.5754|)\n",
      "      6. scaler__R: 2.6823 (|2.6823|)\n",
      "      7. scaler__OBP: 1.3485 (|1.3485|)\n",
      "      8. scaler__RA: -1.0979 (|1.0979|)\n",
      "      9. scaler__H: 0.9998 (|0.9998|)\n",
      "      10. scaler__WHIP: -0.9721 (|0.9721|)\n",
      "      11. scaler__era_1: -0.8237 (|0.8237|)\n",
      "      12. scaler__SLG: 0.8205 (|0.8205|)\n",
      "      13. scaler__Era_Adjusted_BB_Rate: -0.7093 (|0.7093|)\n",
      "      14. scaler__G: -0.6839 (|0.6839|)\n",
      "      15. scaler__E: -0.6401 (|0.6401|)\n",
      "      16. scaler__SHO: 0.5505 (|0.5505|)\n",
      "      17. scaler__decade_1940: 0.5387 (|0.5387|)\n",
      "      18. scaler__HRA: -0.4510 (|0.4510|)\n",
      "      19. scaler__decade_1920: -0.3459 (|0.3459|)\n",
      "      20. scaler__SOA: 0.3448 (|0.3448|)\n",
      "      21. scaler__decade_1930: 0.3338 (|0.3338|)\n",
      "      22. scaler__decade_1990: -0.2410 (|0.2410|)\n",
      "      23. scaler__REI: 0.2191 (|0.2191|)\n",
      "      24. scaler__BB: -0.2024 (|0.2024|)\n",
      "      25. scaler__decade_1980: -0.1403 (|0.1403|)\n",
      "      26. scaler__era_2: 0.1195 (|0.1195|)\n",
      "      27. scaler__era_6: -0.0959 (|0.0959|)\n",
      "      28. scaler__Struggling_Teams_Expected_Boost: -0.0860 (|0.0860|)\n",
      "      29. scaler__era_4: 0.0830 (|0.0830|)\n",
      "      30. scaler__era_8: 0.0635 (|0.0635|)\n",
      "      31. scaler__2B: -0.0486 (|0.0486|)\n",
      "      32. scaler__DP: 0.0440 (|0.0440|)\n",
      "      33. scaler__Era_Adjusted_SLG: 0.0386 (|0.0386|)\n",
      "      34. scaler__SB: -0.0368 (|0.0368|)\n",
      "      35. scaler__decade_1950: 0.0286 (|0.0286|)\n",
      "      36. scaler__3B: 0.0149 (|0.0149|)\n",
      "      37. scaler__BBA: -0.0110 (|0.0110|)\n",
      "      38. scaler__decade_1960: -0.0000 (|0.0000|)\n",
      "      39. scaler__decade_1970: 0.0000 (|0.0000|)\n",
      "      40. scaler__era_5: -0.0000 (|0.0000|)\n",
      "      41. scaler__decade_2000: -0.0000 (|0.0000|)\n",
      "      42. scaler__era_7: -0.0000 (|0.0000|)\n",
      "      43. passthrough__Archetype_1: -0.0000 (|0.0000|)\n",
      "      44. scaler__Era_Adjusted_WHIP: -0.0000 (|0.0000|)\n",
      "      45. scaler__era_3: 0.0000 (|0.0000|)\n",
      "      46. scaler__Pitching_Strong_Expected_Squared: -0.0000 (|0.0000|)\n",
      "      47. scaler__Elite_Teams_Expected_Squared: 0.0000 (|0.0000|)\n",
      "      48. scaler__Expected_Wins_Relative: -0.0000 (|0.0000|)\n",
      "      49. scaler__Era_Adjusted_OPS: 0.0000 (|0.0000|)\n",
      "      50. scaler__Era_Adjusted_OBP: 0.0000 (|0.0000|)\n",
      "      51. scaler__mlb_rpg: -0.0000 (|0.0000|)\n",
      "      52. scaler__Times_On_Base_Allowed: -0.0000 (|0.0000|)\n",
      "      53. scaler__Times_On_Base: 0.0000 (|0.0000|)\n",
      "      54. scaler__HA: 0.0000 (|0.0000|)\n",
      "      55. scaler__ER: -0.0000 (|0.0000|)\n",
      "      56. scaler__SO: 0.0000 (|0.0000|)\n",
      "      57. scaler__HR: -0.0000 (|0.0000|)\n",
      "      58. passthrough__Archetype_2: 0.0000 (|0.0000|)\n",
      "\n",
      "üìà FEATURE IMPORTANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Lasso:\n",
      "  ‚Ä¢ Total features: 58\n",
      "  ‚Ä¢ Max coefficient: 6.5823\n",
      "  ‚Ä¢ Mean coefficient: 0.6501\n",
      "  ‚Ä¢ Features with |coef| > 0.1: 26\n",
      "  ‚Ä¢ Features with |coef| > 0.01: 37\n",
      "\n",
      "‚ú® Feature importance analysis complete!\n",
      "\n",
      "Analyzing ElasticNet...\n",
      "   ‚úÖ Extracted 58 feature coefficients\n",
      "   üìà Top 5 most important features:\n",
      "      1. scaler__Expected_Wins: 6.1513 (|6.1513|)\n",
      "      2. scaler__IPouts: 4.2022 (|4.2022|)\n",
      "      3. scaler__SV: 4.1755 (|4.1755|)\n",
      "      4. scaler__AB: -3.8270 (|3.8270|)\n",
      "      5. scaler__CG: 3.5319 (|3.5319|)\n",
      "      6. scaler__R: 2.9487 (|2.9487|)\n",
      "      7. scaler__OBP: 1.8506 (|1.8506|)\n",
      "      8. scaler__RA: -1.4337 (|1.4337|)\n",
      "      9. scaler__era_1: -0.7749 (|0.7749|)\n",
      "      10. scaler__WHIP: -0.7340 (|0.7340|)\n",
      "      11. scaler__SLG: 0.7313 (|0.7313|)\n",
      "      12. scaler__Era_Adjusted_BB_Rate: -0.6794 (|0.6794|)\n",
      "      13. scaler__E: -0.6729 (|0.6729|)\n",
      "      14. scaler__G: -0.5844 (|0.5844|)\n",
      "      15. scaler__SHO: 0.5684 (|0.5684|)\n",
      "      16. scaler__decade_1940: 0.5443 (|0.5443|)\n",
      "      17. scaler__BB: -0.5037 (|0.5037|)\n",
      "      18. scaler__HRA: -0.4464 (|0.4464|)\n",
      "      19. scaler__H: 0.4191 (|0.4191|)\n",
      "      20. scaler__SOA: 0.3497 (|0.3497|)\n",
      "      21. scaler__decade_1930: 0.3339 (|0.3339|)\n",
      "      22. scaler__decade_1920: -0.3304 (|0.3304|)\n",
      "      23. scaler__REI: 0.2870 (|0.2870|)\n",
      "      24. scaler__Era_Adjusted_WHIP: -0.2419 (|0.2419|)\n",
      "      25. scaler__decade_1990: -0.2371 (|0.2371|)\n",
      "      26. scaler__Era_Adjusted_OPS: 0.1417 (|0.1417|)\n",
      "      27. scaler__decade_1980: -0.1388 (|0.1388|)\n",
      "      28. scaler__era_2: 0.1341 (|0.1341|)\n",
      "      29. scaler__ER: -0.1076 (|0.1076|)\n",
      "      30. scaler__era_6: -0.0996 (|0.0996|)\n",
      "      31. scaler__Struggling_Teams_Expected_Boost: -0.0986 (|0.0986|)\n",
      "      32. scaler__era_4: 0.0863 (|0.0863|)\n",
      "      33. scaler__2B: -0.0559 (|0.0559|)\n",
      "      34. scaler__era_8: 0.0500 (|0.0500|)\n",
      "      35. scaler__DP: 0.0496 (|0.0496|)\n",
      "      36. scaler__Era_Adjusted_SLG: 0.0379 (|0.0379|)\n",
      "      37. scaler__decade_1950: 0.0342 (|0.0342|)\n",
      "      38. scaler__SB: -0.0214 (|0.0214|)\n",
      "      39. scaler__BBA: -0.0126 (|0.0126|)\n",
      "      40. scaler__3B: 0.0109 (|0.0109|)\n",
      "      41. scaler__decade_1970: 0.0048 (|0.0048|)\n",
      "      42. scaler__Times_On_Base_Allowed: -0.0000 (|0.0000|)\n",
      "      43. scaler__decade_1960: -0.0000 (|0.0000|)\n",
      "      44. scaler__decade_2000: -0.0000 (|0.0000|)\n",
      "      45. passthrough__Archetype_1: -0.0000 (|0.0000|)\n",
      "      46. scaler__HR: -0.0000 (|0.0000|)\n",
      "      47. scaler__era_3: 0.0000 (|0.0000|)\n",
      "      48. scaler__era_7: -0.0000 (|0.0000|)\n",
      "      49. scaler__era_5: -0.0000 (|0.0000|)\n",
      "      50. scaler__mlb_rpg: -0.0000 (|0.0000|)\n",
      "      51. scaler__SO: 0.0000 (|0.0000|)\n",
      "      52. scaler__Pitching_Strong_Expected_Squared: -0.0000 (|0.0000|)\n",
      "      53. scaler__Elite_Teams_Expected_Squared: 0.0000 (|0.0000|)\n",
      "      54. scaler__Expected_Wins_Relative: 0.0000 (|0.0000|)\n",
      "      55. scaler__HA: 0.0000 (|0.0000|)\n",
      "      56. scaler__Times_On_Base: 0.0000 (|0.0000|)\n",
      "      57. scaler__Era_Adjusted_OBP: 0.0000 (|0.0000|)\n",
      "      58. passthrough__Archetype_2: 0.0000 (|0.0000|)\n",
      "\n",
      "üìà FEATURE IMPORTANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Lasso:\n",
      "  ‚Ä¢ Total features: 58\n",
      "  ‚Ä¢ Max coefficient: 6.5823\n",
      "  ‚Ä¢ Mean coefficient: 0.6501\n",
      "  ‚Ä¢ Features with |coef| > 0.1: 26\n",
      "  ‚Ä¢ Features with |coef| > 0.01: 37\n",
      "\n",
      "ElasticNet:\n",
      "  ‚Ä¢ Total features: 58\n",
      "  ‚Ä¢ Max coefficient: 6.1513\n",
      "  ‚Ä¢ Mean coefficient: 0.6490\n",
      "  ‚Ä¢ Features with |coef| > 0.1: 29\n",
      "  ‚Ä¢ Features with |coef| > 0.01: 40\n",
      "\n",
      "‚ú® Feature importance analysis complete!\n",
      "\n",
      "‚ö†Ô∏è Could not extract feature importances from enough models\n",
      "   Successfully analyzed: 2 models\n",
      "   Need at least 2 models for comparison\n"
     ]
    }
   ],
   "source": [
    "# Feature importance visualization for top 2 optimized linear models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get the top 2 linear models by MAE performance\n",
    "top_2_linear = [name for name, _ in sorted_linear_results[:2]]\n",
    "print(f\"üèÜ Top 2 Optimized Linear Models:\")\n",
    "for i, model_name in enumerate(top_2_linear, 1):\n",
    "    mae = cv_results_linear_optimized[model_name]['test_mae']\n",
    "    r2 = cv_results_linear_optimized[model_name]['test_r2']\n",
    "    print(f\"   {i}. {model_name}: MAE = {mae:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä EXTRACTING FEATURE IMPORTANCES...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Function to get feature importances from linear models\n",
    "def get_linear_feature_importance(model, feature_names):\n",
    "    \"\"\"Extract feature importances from linear models\"\"\"\n",
    "    try:\n",
    "        # Get the actual linear model (last step in pipeline)\n",
    "        linear_model = model.named_steps['model']\n",
    "        \n",
    "        # Get coefficients\n",
    "        if hasattr(linear_model, 'coef_'):\n",
    "            coefficients = linear_model.coef_\n",
    "            \n",
    "            # Handle different coefficient shapes\n",
    "            if coefficients.ndim > 1:\n",
    "                coefficients = coefficients.flatten()\n",
    "            \n",
    "            # Create feature importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'coefficient': coefficients,\n",
    "                'abs_coefficient': np.abs(coefficients)\n",
    "            }).sort_values('abs_coefficient', ascending=False)\n",
    "            \n",
    "            return importance_df\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Model does not have coefficients\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error extracting coefficients: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fit the top 2 models to get feature importances\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in top_2_linear:\n",
    "    print(f\"\\nAnalyzing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get the model\n",
    "        model = optimized_linear_models[model_name]\n",
    "        \n",
    "        # Fit the model to get coefficients\n",
    "        model.fit(X_linear, y_linear)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        if hasattr(model.named_steps.get('preprocessor'), 'get_feature_names_out'):\n",
    "            # For models with ColumnTransformer\n",
    "            feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        elif hasattr(model.named_steps.get('poly'), 'get_feature_names_out'):\n",
    "            # For polynomial models\n",
    "            # First get base feature names\n",
    "            if hasattr(model.named_steps.get('scaler'), 'get_feature_names_out'):\n",
    "                base_features = model.named_steps['scaler'].get_feature_names_out()\n",
    "            else:\n",
    "                base_features = X_linear.columns\n",
    "            feature_names = model.named_steps['poly'].get_feature_names_out(base_features)\n",
    "        else:\n",
    "            # Fallback to original feature names\n",
    "            feature_names = X_linear.columns\n",
    "        \n",
    "        # Get feature importance\n",
    "        importance_df = get_linear_feature_importance(model, feature_names)\n",
    "        \n",
    "        if importance_df is not None:\n",
    "            feature_importance_results[model_name] = importance_df\n",
    "            print(f\"   ‚úÖ Extracted {len(importance_df)} feature coefficients\")\n",
    "            print(f\"   üìà Top 5 most important features:\")\n",
    "            for i, (_, row) in enumerate(importance_df.head(60).iterrows(), 1):\n",
    "                print(f\"      {i}. {row['feature']}: {row['coefficient']:.4f} (|{row['abs_coefficient']:.4f}|)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to analyze {model_name}: {str(e)}\")\n",
    "\n",
    "# # Create visualization\n",
    "# if len(feature_importance_results) >= 2:\n",
    "#     print(f\"\\nüé® CREATING FEATURE IMPORTANCE VISUALIZATIONS...\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # Set up the plot\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "#     fig.suptitle('Feature Importance Comparison: Top 2 Optimized Linear Models', \n",
    "#                  fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "#     colors = ['#2E86AB', '#A23B72']  # Blue and Purple\n",
    "    \n",
    "#     for idx, (model_name, importance_df) in enumerate(feature_importance_results.items()):\n",
    "#         ax = axes[idx]\n",
    "        \n",
    "#         # Get top 15 features for visualization\n",
    "#         top_features = importance_df.head(15)\n",
    "        \n",
    "#         # Create horizontal bar plot\n",
    "#         bars = ax.barh(range(len(top_features)), top_features['abs_coefficient'], \n",
    "#                        color=colors[idx], alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "#         # Customize the plot\n",
    "#         ax.set_yticks(range(len(top_features)))\n",
    "#         ax.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "#         ax.set_xlabel('Absolute Coefficient Value', fontsize=12, fontweight='bold')\n",
    "#         ax.set_title(f'{model_name}\\n(MAE: {cv_results_linear_optimized[model_name][\"test_mae\"]:.4f})', \n",
    "#                      fontsize=14, fontweight='bold', pad=20)\n",
    "#         ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        \n",
    "#         # Add value labels on bars\n",
    "#         for i, (bar, coef) in enumerate(zip(bars, top_features['coefficient'])):\n",
    "#             width = bar.get_width()\n",
    "#             ax.text(width + max(top_features['abs_coefficient']) * 0.01, bar.get_y() + bar.get_height()/2,\n",
    "#                    f'{coef:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "#         # Invert y-axis to show most important features at top\n",
    "#         ax.invert_yaxis()\n",
    "        \n",
    "#         # Add performance metrics as text box\n",
    "#         mae = cv_results_linear_optimized[model_name]['test_mae']\n",
    "#         r2 = cv_results_linear_optimized[model_name]['test_r2']\n",
    "#         overfitting = cv_results_linear_optimized[model_name]['overfitting']\n",
    "        \n",
    "#         textstr = f'MAE: {mae:.4f}\\nR¬≤: {r2:.4f}\\nOverfitting: {overfitting:.4f}'\n",
    "#         props = dict(boxstyle='round', facecolor=colors[idx], alpha=0.2)\n",
    "#         ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=10,\n",
    "#                 verticalalignment='bottom', horizontalalignment='right', bbox=props)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìà FEATURE IMPORTANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, importance_df in feature_importance_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  ‚Ä¢ Total features: {len(importance_df)}\")\n",
    "        print(f\"  ‚Ä¢ Max coefficient: {importance_df['abs_coefficient'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean coefficient: {importance_df['abs_coefficient'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Features with |coef| > 0.1: {(importance_df['abs_coefficient'] > 0.1).sum()}\")\n",
    "        print(f\"  ‚Ä¢ Features with |coef| > 0.01: {(importance_df['abs_coefficient'] > 0.01).sum()}\")\n",
    "    \n",
    "    print(f\"\\n‚ú® Feature importance analysis complete!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not extract feature importances from enough models\")\n",
    "    print(f\"   Successfully analyzed: {len(feature_importance_results)} models\")\n",
    "    print(f\"   Need at least 2 models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960d2a1",
   "metadata": {},
   "source": [
    "# Key Differences Between Weighted and Stacked Ensemble Approaches\n",
    "\n",
    "| Aspect | Weighted Ensemble | Stacked Ensemble |\n",
    "|--------|-------------------|------------------|\n",
    "| **Meta-learning** | ‚ùå No | ‚úÖ Yes |\n",
    "| **Learning Method** | Mathematical optimization | Machine learning model |\n",
    "| **Validation Strategy** | Cross-validation for evaluation | Cross-validation for meta-training |\n",
    "| **Weight Determination** | Optuna optimization (scipy.optimize) | Meta-learner learns automatically |\n",
    "| **Model Complexity** | Medium | High |\n",
    "| **Training Speed** | Fast | Slower |\n",
    "| **Interpretability** | ‚úÖ Clear weights | ‚ùå Black-box meta-model |\n",
    "| **Overfitting Risk** | Lower | Medium |\n",
    "| **Feature Engineering** | Uses base predictions only | Can learn complex interactions |\n",
    "| **Hyperparameter Tuning** | Base models + weights | Base models + meta-learner |\n",
    "| **Implementation Complexity** | Simpler | More complex |\n",
    "| **Performance Potential** | Good | Potentially higher |\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Weighted Ensemble**: Combines predictions using mathematically optimized weights\n",
    "- **Stacked Ensemble**: Uses a meta-learner to learn how to combine base model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4f5b",
   "metadata": {},
   "source": [
    "# Weighted Ensemble Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb6133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: WEIGHTED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\n",
      "======================================================================\n",
      "üõ°Ô∏è SMART SCALING STATUS:\n",
      "   Found 2 Archetype_ features to preserve\n",
      "   ‚úÖ Pre-trained Optuna models already use smart scaling\n",
      "   ‚úÖ Archetype features preserved as 0/1 values in base models\n",
      "\n",
      "1. SELECTING TOP OPTUNA-OPTIMIZED MODELS\n",
      "--------------------------------------------------\n",
      "Top Optuna-Optimized Linear Models (by CV MAE):\n",
      "  Lasso: MAE = 2.7272, R¬≤ = 0.9311\n",
      "  ElasticNet: MAE = 2.7278, R¬≤ = 0.9310\n",
      "  Huber: MAE = 2.7339, R¬≤ = 0.9305\n",
      "\n",
      "Top Optuna-Optimized Boosting Models (by CV MAE):\n",
      "  CatBoost: MAE = 3.0233, R¬≤ = 0.9146\n",
      "  XGBoost: MAE = 3.0536, R¬≤ = 0.9136\n",
      "\n",
      "2. OPTUNA-ENHANCED ENSEMBLE COMPOSITION\n",
      "--------------------------------------------------\n",
      "Selected 3 Optuna-optimized models for ensemble:\n",
      "  ‚úÖ Lasso (Optuna-optimized)\n",
      "  ‚úÖ ElasticNet (Optuna-optimized)\n",
      "  ‚úÖ CatBoost (Optuna-optimized)\n",
      "\n",
      "Total ensemble models: 3\n",
      "\n",
      "3. OPTUNA-OPTIMIZED MODEL PERFORMANCE SUMMARY\n",
      "--------------------------------------------------\n",
      "Lasso: MAE = 2.7272, R¬≤ = 0.9311\n",
      "ElasticNet: MAE = 2.7278, R¬≤ = 0.9310\n",
      "CatBoost: MAE = 3.0233, R¬≤ = 0.9146\n",
      "\n",
      "4. OPTUNA PARAMETERS IN USE\n",
      "--------------------------------------------------\n",
      "\n",
      "Lasso (Linear):\n",
      "  alpha: 0.0102\n",
      "  max_iter: 18944\n",
      "\n",
      "ElasticNet (Linear):\n",
      "  alpha: 0.0102\n",
      "  l1_ratio: 0.8994\n",
      "  max_iter: 16686\n",
      "\n",
      "CatBoost (Boosting):\n",
      "  iterations: 169\n",
      "  depth: 3\n",
      "  learning_rate: 0.1418\n",
      "  border_count: 74\n",
      "\n",
      "üöÄ Using Optuna-optimized models for superior ensemble performance!\n"
     ]
    }
   ],
   "source": [
    "print(\"PHASE 1: WEIGHTED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# üõ°Ô∏è SMART SCALING STATUS CHECK FOR WEIGHTED ENSEMBLE\n",
    "weighted_archetype_features = [col for col in X_full.columns if col.startswith('Archetype_')]\n",
    "weighted_continuous_features = [col for col in X_full.columns if col not in weighted_archetype_features]\n",
    "\n",
    "if weighted_archetype_features:\n",
    "    print(f\"üõ°Ô∏è SMART SCALING STATUS:\")\n",
    "    print(f\"   Found {len(weighted_archetype_features)} Archetype_ features to preserve\")\n",
    "    print(f\"   ‚úÖ Pre-trained Optuna models already use smart scaling\")\n",
    "    print(f\"   ‚úÖ Archetype features preserved as 0/1 values in base models\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  SCALING STATUS: No Archetype_ features found\")\n",
    "    print(f\"   ‚úÖ Pre-trained Optuna models use standard scaling\")\n",
    "\n",
    "# First, let's identify our top performing models from Optuna optimization\n",
    "print(\"\\n1. SELECTING TOP OPTUNA-OPTIMIZED MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Top 3 linear models (based on CV MAE from Optuna optimization)\n",
    "top_linear_models = dict(sorted(cv_results_linear_optimized.items(), key=lambda x: x[1]['test_mae'])[:3])\n",
    "print(\"Top Optuna-Optimized Linear Models (by CV MAE):\")\n",
    "for name, result in top_linear_models.items():\n",
    "    print(f\"  {name}: MAE = {result['test_mae']:.4f}, R¬≤ = {result['test_r2']:.4f}\")\n",
    "\n",
    "# Top 2 boosting models (from Optuna optimization)\n",
    "top_boosting_models = dict(sorted(cv_results_optimized.items(), key=lambda x: x[1]['test_mae'])[:2])\n",
    "print(\"\\nTop Optuna-Optimized Boosting Models (by CV MAE):\")\n",
    "for name, result in top_boosting_models.items():\n",
    "    print(f\"  {name}: MAE = {result['test_mae']:.4f}, R¬≤ = {result['test_r2']:.4f}\")\n",
    "\n",
    "# Select our ensemble candidates (top performers from each category)\n",
    "ensemble_models = {}\n",
    "\n",
    "# Add top 2 linear models from Optuna optimization\n",
    "linear_names = list(top_linear_models.keys())[:2]\n",
    "for name in linear_names:\n",
    "    ensemble_models[name] = optimized_linear_models[name]  # ‚úÖ Use Optuna-optimized models\n",
    "\n",
    "# Add the selected boosting model from intelligent selection (respecting overfitting concerns)\n",
    "boosting_name = best_model_name  # Use the intelligently selected model from earlier analysis\n",
    "ensemble_models[boosting_name] = optimized_models[boosting_name]  # ‚úÖ Use Optuna-optimized models\n",
    "\n",
    "print(f\"\\n2. OPTUNA-ENHANCED ENSEMBLE COMPOSITION\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected {len(ensemble_models)} Optuna-optimized models for ensemble:\")\n",
    "for name in ensemble_models.keys():\n",
    "    if name == boosting_name and name != list(top_boosting_models.keys())[0]:\n",
    "        print(f\"  ‚úÖ {name} (Optuna-optimized, intelligently selected over {list(top_boosting_models.keys())[0]} to avoid overfitting)\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ {name} (Optuna-optimized)\")\n",
    "    \n",
    "print(f\"\\nTotal ensemble models: {len(ensemble_models)}\")\n",
    "\n",
    "# Add note about intelligent boosting model selection\n",
    "if boosting_name != list(top_boosting_models.keys())[0]:\n",
    "    top_mae_boosting = list(top_boosting_models.keys())[0]\n",
    "    print(f\"\\n‚ÑπÔ∏è Model Selection Note:\")\n",
    "    print(f\"   Boosting model {boosting_name} was selected over {top_mae_boosting}\")\n",
    "    print(f\"   based on intelligent selection criteria (balancing performance & overfitting)\")\n",
    "    print(f\"   Selection reason: {selection_reason}\")\n",
    "\n",
    "# Store performance metrics for weight calculation using Optuna results\n",
    "model_performance = {}\n",
    "for name in ensemble_models.keys():\n",
    "    if name in cv_results_linear_optimized:  # Linear model\n",
    "        model_performance[name] = {\n",
    "            'mae': cv_results_linear_optimized[name]['test_mae'],\n",
    "            'r2': cv_results_linear_optimized[name]['test_r2']\n",
    "        }\n",
    "    elif name in cv_results_optimized:  # Boosting model\n",
    "        model_performance[name] = {\n",
    "            'mae': cv_results_optimized[name]['test_mae'], \n",
    "            'r2': cv_results_optimized[name]['test_r2']\n",
    "        }\n",
    "\n",
    "print(f\"\\n3. OPTUNA-OPTIMIZED MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "for name, perf in model_performance.items():\n",
    "    print(f\"{name}: MAE = {perf['mae']:.4f}, R¬≤ = {perf['r2']:.4f}\")\n",
    "\n",
    "# Display the Optuna-optimized parameters being used\n",
    "print(f\"\\n4. OPTUNA PARAMETERS IN USE\")\n",
    "print(\"-\" * 50)\n",
    "for name in ensemble_models.keys():\n",
    "    if name in optimized_linear_params and optimized_linear_params[name]:\n",
    "        print(f\"\\n{name} (Linear):\")\n",
    "        for param, value in optimized_linear_params[name].items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {param}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {param}: {value}\")\n",
    "    elif name in optimized_params:\n",
    "        print(f\"\\n{name} (Boosting):\")\n",
    "        for param, value in optimized_params[name].items():\n",
    "            if isinstance(value, float) and 'rate' in param:\n",
    "                print(f\"  {param}: {value:.4f}\")\n",
    "            elif isinstance(value, (int, bool, str)):\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüöÄ Using Optuna-optimized models for superior ensemble performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e2121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. GENERATING OUT-OF-FOLD PREDICTIONS (OPTUNA MODELS)\n",
      "------------------------------------------------------------\n",
      "Generating OOF predictions for Lasso (Optuna-optimized)...\n",
      "  OOF MAE: 2.7272\n",
      "Generating OOF predictions for ElasticNet (Optuna-optimized)...\n",
      "  OOF MAE: 2.7278\n",
      "Generating OOF predictions for CatBoost (Optuna-optimized)...\n",
      "  OOF MAE: 2.7278\n",
      "Generating OOF predictions for CatBoost (Optuna-optimized)...\n",
      "  OOF MAE: 3.0536\n",
      "\n",
      "OOF prediction matrix shape: (1812, 3)\n",
      "\n",
      "6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Initial weights (based on Optuna-optimized model performance):\n",
      "  Lasso: 0.345\n",
      "  ElasticNet: 0.345\n",
      "  CatBoost: 0.311\n",
      "\n",
      "Optimization successful: True\n",
      "Optimal Optuna ensemble OOF MAE: 2.7275\n",
      "\n",
      "Optimal weights for Optuna-optimized models:\n",
      "  Lasso: 0.503\n",
      "  ElasticNet: 0.497\n",
      "  CatBoost: 0.000\n",
      "\n",
      "Improvement over best individual Optuna model:\n",
      "  Best individual MAE: 2.7272\n",
      "  Ensemble MAE: 2.7275\n",
      "  Improvement: -0.0003 (-0.01%)\n",
      "\n",
      "‚ú® Ensemble optimization complete using Optuna-optimized base models!\n",
      "  OOF MAE: 3.0536\n",
      "\n",
      "OOF prediction matrix shape: (1812, 3)\n",
      "\n",
      "6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Initial weights (based on Optuna-optimized model performance):\n",
      "  Lasso: 0.345\n",
      "  ElasticNet: 0.345\n",
      "  CatBoost: 0.311\n",
      "\n",
      "Optimization successful: True\n",
      "Optimal Optuna ensemble OOF MAE: 2.7275\n",
      "\n",
      "Optimal weights for Optuna-optimized models:\n",
      "  Lasso: 0.503\n",
      "  ElasticNet: 0.497\n",
      "  CatBoost: 0.000\n",
      "\n",
      "Improvement over best individual Optuna model:\n",
      "  Best individual MAE: 2.7272\n",
      "  Ensemble MAE: 2.7275\n",
      "  Improvement: -0.0003 (-0.01%)\n",
      "\n",
      "‚ú® Ensemble optimization complete using Optuna-optimized base models!\n"
     ]
    }
   ],
   "source": [
    "# Generate out-of-fold predictions for weight optimization with Optuna-optimized models\n",
    "print(\"\\n5. GENERATING OUT-OF-FOLD PREDICTIONS (OPTUNA MODELS)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Generate OOF predictions for each Optuna-optimized model\n",
    "oof_predictions = {}\n",
    "model_names = list(ensemble_models.keys())\n",
    "\n",
    "# Use the same CV strategy as the original optimization\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"Generating OOF predictions for {name} (Optuna-optimized)...\")\n",
    "    \n",
    "    # Use the same CV strategy as before\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=cv, method='predict')\n",
    "    oof_predictions[name] = oof_pred\n",
    "    \n",
    "    # Calculate OOF MAE\n",
    "    oof_mae = mean_absolute_error(y_full, oof_pred)\n",
    "    print(f\"  OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "# Create OOF prediction matrix\n",
    "oof_matrix = np.column_stack([oof_predictions[name] for name in model_names])\n",
    "print(f\"\\nOOF prediction matrix shape: {oof_matrix.shape}\")\n",
    "\n",
    "print(\"\\n6. OPTIMIZING ENSEMBLE WEIGHTS FOR OPTUNA MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def ensemble_mae_objective(weights, predictions, targets):\n",
    "    \"\"\"Objective function to minimize: weighted ensemble MAE\"\"\"\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()  # Normalize to sum to 1\n",
    "    ensemble_pred = np.dot(predictions, weights)\n",
    "    return mean_absolute_error(targets, ensemble_pred)\n",
    "\n",
    "# Initial weights based on inverse MAE (better models get higher weights)\n",
    "initial_weights = []\n",
    "for name in model_names:\n",
    "    mae = model_performance[name]['mae']\n",
    "    # Inverse weight: lower MAE = higher weight\n",
    "    weight = 1.0 / mae if mae > 0 else 1.0\n",
    "    initial_weights.append(weight)\n",
    "\n",
    "# Normalize initial weights\n",
    "initial_weights = np.array(initial_weights)\n",
    "initial_weights = initial_weights / initial_weights.sum()\n",
    "\n",
    "print(\"Initial weights (based on Optuna-optimized model performance):\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {initial_weights[i]:.3f}\")\n",
    "\n",
    "# Constraint: weights must sum to 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})\n",
    "\n",
    "# Bounds: each weight between 0 and 1\n",
    "bounds = [(0.0, 1.0) for _ in range(len(model_names))]\n",
    "\n",
    "# Optimize weights\n",
    "result = minimize(\n",
    "    ensemble_mae_objective,\n",
    "    initial_weights,\n",
    "    args=(oof_matrix, y_full),\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=constraints\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "optimal_mae = result.fun\n",
    "\n",
    "print(f\"\\nOptimization successful: {result.success}\")\n",
    "print(f\"Optimal Optuna ensemble OOF MAE: {optimal_mae:.4f}\")\n",
    "print(\"\\nOptimal weights for Optuna-optimized models:\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {optimal_weights[i]:.3f}\")\n",
    "\n",
    "# Calculate improvement over best individual Optuna-optimized model\n",
    "best_individual_mae = min([model_performance[name]['mae'] for name in model_names])\n",
    "improvement = best_individual_mae - optimal_mae\n",
    "print(f\"\\nImprovement over best individual Optuna model:\")\n",
    "print(f\"  Best individual MAE: {best_individual_mae:.4f}\")\n",
    "print(f\"  Ensemble MAE: {optimal_mae:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.4f} ({improvement/best_individual_mae*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚ú® Ensemble optimization complete using Optuna-optimized base models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ad7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. TRAINING FINAL OPTUNA-OPTIMIZED ENSEMBLE MODELS\n",
      "------------------------------------------------------------\n",
      "Training Lasso (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.94 to 109.42\n",
      "Training ElasticNet (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 44.92 to 109.23\n",
      "Training CatBoost (Optuna-optimized) on full dataset...\n",
      "  Test predictions range: 47.74 to 103.20\n",
      "\n",
      "All 3 Optuna-optimized models trained successfully!\n",
      "Test prediction matrix shape: (453, 3)\n",
      "\n",
      "8. GENERATING OPTUNA-ENHANCED ENSEMBLE PREDICTIONS\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced ensemble test predictions:\n",
      "  Range: 44.93 to 109.33\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Comparison with individual Optuna-optimized models:\n",
      "  Lasso (weight=0.503): mean=79.07, std=12.05\n",
      "  ElasticNet (weight=0.497): mean=79.08, std=12.04\n",
      "  CatBoost (weight=0.000): mean=79.06, std=12.07\n",
      "\n",
      "9. CREATING OPTUNA-ENHANCED SUBMISSION FILE\n",
      "------------------------------------------------------------\n",
      "‚úÖ Optuna-enhanced submission saved: submission_optuna_weighted_ensemble_20251005_153749.csv\n",
      "üìÅ Path: /home/chrisfkh/sctp-ds-ai/mod3/kaggle_moneyball/submissions/submission_optuna_weighted_ensemble_20251005_153749.csv\n",
      "üìä Predictions shape: (453, 2)\n",
      "\n",
      "First 10 predictions:\n",
      "     ID          W\n",
      "0  1756  69.163215\n",
      "1  1282  74.530591\n",
      "2   351  84.376593\n",
      "3   421  87.104443\n",
      "4    57  93.270224\n",
      "5  1557  97.582178\n",
      "6   846  79.289275\n",
      "7  1658  84.141581\n",
      "8   112  72.770915\n",
      "9  2075  83.667249\n",
      "\n",
      "10. OPTUNA-ENHANCED WEIGHTED ENSEMBLE SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ensemble Composition (Optuna-optimized models):\n",
      "  Lasso: 50.3%\n",
      "  ElasticNet: 49.7%\n",
      "  CatBoost: 0.0%\n",
      "\n",
      "Expected Performance:\n",
      "  Cross-validation MAE: 2.7275\n",
      "  Expected Kaggle score: ~2.73\n",
      "  Improvement vs best individual Optuna model: -0.0003\n",
      "\n",
      "üöÄ Phase 1 complete with Optuna-optimized models!\n",
      "  Test predictions range: 47.74 to 103.20\n",
      "\n",
      "All 3 Optuna-optimized models trained successfully!\n",
      "Test prediction matrix shape: (453, 3)\n",
      "\n",
      "8. GENERATING OPTUNA-ENHANCED ENSEMBLE PREDICTIONS\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced ensemble test predictions:\n",
      "  Range: 44.93 to 109.33\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Comparison with individual Optuna-optimized models:\n",
      "  Lasso (weight=0.503): mean=79.07, std=12.05\n",
      "  ElasticNet (weight=0.497): mean=79.08, std=12.04\n",
      "  CatBoost (weight=0.000): mean=79.06, std=12.07\n",
      "\n",
      "9. CREATING OPTUNA-ENHANCED SUBMISSION FILE\n",
      "------------------------------------------------------------\n",
      "‚úÖ Optuna-enhanced submission saved: submission_optuna_weighted_ensemble_20251005_153749.csv\n",
      "üìÅ Path: /home/chrisfkh/sctp-ds-ai/mod3/kaggle_moneyball/submissions/submission_optuna_weighted_ensemble_20251005_153749.csv\n",
      "üìä Predictions shape: (453, 2)\n",
      "\n",
      "First 10 predictions:\n",
      "     ID          W\n",
      "0  1756  69.163215\n",
      "1  1282  74.530591\n",
      "2   351  84.376593\n",
      "3   421  87.104443\n",
      "4    57  93.270224\n",
      "5  1557  97.582178\n",
      "6   846  79.289275\n",
      "7  1658  84.141581\n",
      "8   112  72.770915\n",
      "9  2075  83.667249\n",
      "\n",
      "10. OPTUNA-ENHANCED WEIGHTED ENSEMBLE SUMMARY\n",
      "------------------------------------------------------------\n",
      "Ensemble Composition (Optuna-optimized models):\n",
      "  Lasso: 50.3%\n",
      "  ElasticNet: 49.7%\n",
      "  CatBoost: 0.0%\n",
      "\n",
      "Expected Performance:\n",
      "  Cross-validation MAE: 2.7275\n",
      "  Expected Kaggle score: ~2.73\n",
      "  Improvement vs best individual Optuna model: -0.0003\n",
      "\n",
      "üöÄ Phase 1 complete with Optuna-optimized models!\n"
     ]
    }
   ],
   "source": [
    "# Train final Optuna-optimized models and generate test predictions\n",
    "print(\"\\n7. TRAINING FINAL OPTUNA-OPTIMIZED ENSEMBLE MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train each Optuna-optimized model on the full training dataset\n",
    "final_models = {}\n",
    "test_predictions = {}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"Training {name} (Optuna-optimized) on full dataset...\")\n",
    "    \n",
    "    # Clone and train the Optuna-optimized model\n",
    "    final_model = model  # Already configured with Optuna parameters\n",
    "    final_model.fit(X_full, y_full)\n",
    "    final_models[name] = final_model\n",
    "    \n",
    "    # Generate test predictions\n",
    "    test_pred = final_model.predict(X_test_final)\n",
    "    test_predictions[name] = test_pred\n",
    "    \n",
    "    print(f\"  Test predictions range: {test_pred.min():.2f} to {test_pred.max():.2f}\")\n",
    "\n",
    "print(f\"\\nAll {len(final_models)} Optuna-optimized models trained successfully!\")\n",
    "\n",
    "# Create test prediction matrix\n",
    "test_matrix = np.column_stack([test_predictions[name] for name in model_names])\n",
    "print(f\"Test prediction matrix shape: {test_matrix.shape}\")\n",
    "\n",
    "print(\"\\n8. GENERATING OPTUNA-ENHANCED ENSEMBLE PREDICTIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Generate weighted ensemble predictions using Optuna-optimized models\n",
    "ensemble_test_pred = np.dot(test_matrix, optimal_weights)\n",
    "\n",
    "print(f\"Optuna-enhanced ensemble test predictions:\")\n",
    "print(f\"  Range: {ensemble_test_pred.min():.2f} to {ensemble_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {ensemble_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {ensemble_test_pred.std():.2f}\")\n",
    "\n",
    "# Compare with individual Optuna-optimized model predictions\n",
    "print(f\"\\nComparison with individual Optuna-optimized models:\")\n",
    "for i, name in enumerate(model_names):\n",
    "    individual_pred = test_predictions[name]\n",
    "    weight = optimal_weights[i]\n",
    "    print(f\"  {name} (weight={weight:.3f}): mean={individual_pred.mean():.2f}, std={individual_pred.std():.2f}\")\n",
    "\n",
    "print(f\"\\n9. CREATING OPTUNA-ENHANCED SUBMISSION FILE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],  # Use the actual ID column from test.csv\n",
    "    'W': ensemble_test_pred\n",
    "})\n",
    "\n",
    "# Generate timestamp for unique filename\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f\"submission_optuna_weighted_ensemble_{timestamp}.csv\"\n",
    "submission_path = SUB_DIR / submission_filename\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Optuna-enhanced submission saved: {submission_filename}\")\n",
    "print(f\"üìÅ Path: {submission_path}\")\n",
    "print(f\"üìä Predictions shape: {submission_df.shape}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(f\"\\n10. OPTUNA-ENHANCED WEIGHTED ENSEMBLE SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Ensemble Composition (Optuna-optimized models):\")\n",
    "for i, name in enumerate(model_names):\n",
    "    print(f\"  {name}: {optimal_weights[i]:.1%}\")\n",
    "print(f\"\\nExpected Performance:\")\n",
    "print(f\"  Cross-validation MAE: {optimal_mae:.4f}\")\n",
    "print(f\"  Expected Kaggle score: ~{optimal_mae:.2f}\")\n",
    "print(f\"  Improvement vs best individual Optuna model: {improvement:.4f}\")\n",
    "print(f\"\\nüöÄ Phase 1 complete with Optuna-optimized models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889453b4",
   "metadata": {},
   "source": [
    "# Stacked Ensemble Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6aaa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "1. CREATING DIVERSE OPTUNA-OPTIMIZED BASE MODELS\n",
      "------------------------------------------------------------\n",
      "üõ°Ô∏è SMART SCALING DETECTED FOR STACKING:\n",
      "   Archetype features to preserve: 2\n",
      "      ‚Ä¢ Archetype_1\n",
      "      ‚Ä¢ Archetype_2\n",
      "   Continuous features to scale: 56\n",
      "   Smart scaling: ‚úÖ ACTIVE for all linear stacking models\n",
      "üõ°Ô∏è SMART SCALING DETECTED FOR STACKING:\n",
      "   Archetype features to preserve: 2\n",
      "      ‚Ä¢ Archetype_1\n",
      "      ‚Ä¢ Archetype_2\n",
      "   Continuous features to scale: 56\n",
      "   Smart scaling: ‚úÖ ACTIVE for all linear stacking models\n",
      "Base models for stacking (Optuna-optimized): 11\n",
      "  ‚úÖ Ridge_optuna\n",
      "  ‚úÖ Ridge_conservative\n",
      "  ‚úÖ Ridge_aggressive\n",
      "  ‚úÖ Lasso_optuna\n",
      "  ‚úÖ Lasso_variation\n",
      "  ‚úÖ ElasticNet_optuna\n",
      "  ‚úÖ XGBoost_optuna\n",
      "  ‚úÖ XGBoost_conservative\n",
      "  ‚úÖ CatBoost_optuna\n",
      "  ‚úÖ Best_Linear\n",
      "  ‚úÖ Best_Boosting\n",
      "\n",
      "2. IMPLEMENTING STACKED ENSEMBLE WITH OPTUNA MODELS\n",
      "------------------------------------------------------------\n",
      "Generating Level 1 out-of-fold predictions with Optuna-optimized models...\n",
      "  Processing Ridge_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7343\n",
      "  Processing Ridge_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7545\n",
      "  Processing Ridge_aggressive (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7384\n",
      "  Processing Lasso_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7273\n",
      "  Processing Lasso_variation (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7336\n",
      "  Processing ElasticNet_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7270\n",
      "  Processing XGBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7336\n",
      "  Processing ElasticNet_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7270\n",
      "  Processing XGBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.1201\n",
      "  Processing XGBoost_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 3.1201\n",
      "  Processing XGBoost_conservative (Optuna-enhanced)...\n",
      "    OOF MAE: 3.3608\n",
      "  Processing CatBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.3608\n",
      "  Processing CatBoost_optuna (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0302\n",
      "  Processing Best_Linear (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7272\n",
      "  Processing Best_Boosting (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0302\n",
      "  Processing Best_Linear (Optuna-enhanced)...\n",
      "    OOF MAE: 2.7272\n",
      "  Processing Best_Boosting (Optuna-enhanced)...\n",
      "    OOF MAE: 3.0536\n",
      "\n",
      "Level 1 OOF predictions shape: (1812, 11)\n",
      "Level 1 test predictions shape: (453, 11)\n",
      "üöÄ All base models use Optuna-optimized parameters!\n",
      "    OOF MAE: 3.0536\n",
      "\n",
      "Level 1 OOF predictions shape: (1812, 11)\n",
      "Level 1 test predictions shape: (453, 11)\n",
      "üöÄ All base models use Optuna-optimized parameters!\n"
     ]
    }
   ],
   "source": [
    "print(\"STACKED ENSEMBLE IMPLEMENTATION (OPTUNA-OPTIMIZED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"\\n1. CREATING DIVERSE OPTUNA-OPTIMIZED BASE MODELS\") \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# üõ°Ô∏è Check for smart scaling requirements\n",
    "stacking_archetype_features = [col for col in X_full.columns if col.startswith('Archetype_')]\n",
    "stacking_continuous_features = [col for col in X_full.columns if col not in stacking_archetype_features]\n",
    "\n",
    "if stacking_archetype_features:\n",
    "    print(f\"üõ°Ô∏è SMART SCALING DETECTED FOR STACKING:\")\n",
    "    print(f\"   Archetype features to preserve: {len(stacking_archetype_features)}\")\n",
    "    for feat in stacking_archetype_features:\n",
    "        print(f\"      ‚Ä¢ {feat}\")\n",
    "    print(f\"   Continuous features to scale: {len(stacking_continuous_features)}\")\n",
    "    print(f\"   Smart scaling: ‚úÖ ACTIVE for all linear stacking models\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  SCALING INFO: No Archetype_ features found\")\n",
    "    print(f\"   All {len(stacking_continuous_features)} features will be scaled uniformly\")\n",
    "    print(f\"   Standard scaling: ‚úÖ ACTIVE\")\n",
    "\n",
    "if stacking_archetype_features:\n",
    "    print(f\"üõ°Ô∏è SMART SCALING DETECTED FOR STACKING:\")\n",
    "    print(f\"   Archetype features to preserve: {len(stacking_archetype_features)}\")\n",
    "    for feat in stacking_archetype_features:\n",
    "        print(f\"      ‚Ä¢ {feat}\")\n",
    "    print(f\"   Continuous features to scale: {len(stacking_continuous_features)}\")\n",
    "    print(f\"   Smart scaling: ‚úÖ ACTIVE for all linear stacking models\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  SCALING INFO: No Archetype_ features found\")\n",
    "    print(f\"   All {len(stacking_continuous_features)} features will be scaled uniformly\")\n",
    "    print(f\"   Standard scaling: ‚úÖ ACTIVE\")\n",
    "\n",
    "# Create diverse base models using Optuna-optimized parameters for better generalization\n",
    "def create_optuna_linear_model(model_type, params, suffix=\"\"):\n",
    "    \"\"\"Create linear model with Optuna-optimized parameters and smart scaling\"\"\"\n",
    "    \n",
    "    # üõ°Ô∏è SMART SCALING: Check for Archetype_ features in the current dataset\n",
    "    current_archetype_features = [col for col in X_full.columns if col.startswith('Archetype_')]\n",
    "    current_continuous_features = [col for col in X_full.columns if col not in current_archetype_features]\n",
    "    \n",
    "    # Create smart preprocessor for stacking models\n",
    "    if current_archetype_features:\n",
    "        # Use ColumnTransformer to preserve Archetype_ features\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('scaler', StandardScaler(), current_continuous_features),\n",
    "                ('passthrough', 'passthrough', current_archetype_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "    else:\n",
    "        # No Archetype_ features - use standard scaling\n",
    "        preprocessor = StandardScaler()\n",
    "    \n",
    "    if model_type == 'Ridge':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', Ridge(alpha=params.get('alpha', 1.0), random_state=42))\n",
    "        ])\n",
    "    elif model_type == 'Lasso':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', Lasso(\n",
    "                alpha=params.get('alpha', 0.01), \n",
    "                max_iter=params.get('max_iter', 10000),\n",
    "                tol=1e-3,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_type == 'ElasticNet':\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', ElasticNet(\n",
    "                alpha=params.get('alpha', 0.1),\n",
    "                l1_ratio=params.get('l1_ratio', 0.5),\n",
    "                max_iter=params.get('max_iter', 10000),\n",
    "                tol=1e-3,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "# Build stacking models using Optuna-optimized parameters\n",
    "stacking_models = {}\n",
    "\n",
    "# Linear models with Optuna-optimized parameters + variations for diversity\n",
    "if 'Ridge' in optimized_linear_params:\n",
    "    ridge_params = optimized_linear_params['Ridge']\n",
    "    # Use optimized alpha and create variations\n",
    "    base_alpha = ridge_params.get('alpha', 1.0)\n",
    "    stacking_models['Ridge_optuna'] = create_optuna_linear_model('Ridge', ridge_params)\n",
    "    stacking_models['Ridge_conservative'] = create_optuna_linear_model('Ridge', {'alpha': base_alpha * 5})\n",
    "    stacking_models['Ridge_aggressive'] = create_optuna_linear_model('Ridge', {'alpha': base_alpha * 0.2})\n",
    "\n",
    "if 'Lasso' in optimized_linear_params:\n",
    "    lasso_params = optimized_linear_params['Lasso']\n",
    "    stacking_models['Lasso_optuna'] = create_optuna_linear_model('Lasso', lasso_params)\n",
    "    # Create variation\n",
    "    base_alpha = lasso_params.get('alpha', 0.01)\n",
    "    stacking_models['Lasso_variation'] = create_optuna_linear_model('Lasso', {\n",
    "        'alpha': base_alpha * 2,\n",
    "        'max_iter': lasso_params.get('max_iter', 10000)\n",
    "    })\n",
    "\n",
    "if 'ElasticNet' in optimized_linear_params:\n",
    "    elasticnet_params = optimized_linear_params['ElasticNet']\n",
    "    stacking_models['ElasticNet_optuna'] = create_optuna_linear_model('ElasticNet', elasticnet_params)\n",
    "\n",
    "# Tree-based models with Optuna-optimized parameters\n",
    "if 'XGBoost' in optimized_params:\n",
    "    xgb_params = optimized_params['XGBoost'].copy()\n",
    "    # Use Optuna parameters but make conservative for stacking\n",
    "    xgb_params['n_estimators'] = min(xgb_params.get('n_estimators', 150), 150)  # Cap for speed\n",
    "    xgb_params['verbosity'] = 0\n",
    "    xgb_params['random_state'] = 42\n",
    "    \n",
    "    # Create XGBoost with Optuna parameters\n",
    "    stacking_models['XGBoost_optuna'] = XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Create conservative variation\n",
    "    conservative_params = xgb_params.copy()\n",
    "    conservative_params['max_depth'] = max(3, xgb_params.get('max_depth', 6) - 1)  # Shallower\n",
    "    conservative_params['learning_rate'] = xgb_params.get('learning_rate', 0.1) * 0.8  # Slower\n",
    "    stacking_models['XGBoost_conservative'] = XGBRegressor(**conservative_params)\n",
    "\n",
    "if 'CatBoost' in optimized_params:\n",
    "    cat_params = optimized_params['CatBoost'].copy()\n",
    "    # Use Optuna parameters but make conservative for stacking\n",
    "    cat_params['iterations'] = min(cat_params.get('iterations', 150), 150)  # Cap for speed\n",
    "    cat_params['verbose'] = False\n",
    "    cat_params['random_seed'] = 42\n",
    "    \n",
    "    stacking_models['CatBoost_optuna'] = CatBoostRegressor(**cat_params)\n",
    "\n",
    "# Add the best individual Optuna-optimized models\n",
    "stacking_models['Best_Linear'] = best_linear_model  # From Optuna optimization\n",
    "stacking_models['Best_Boosting'] = best_model       # From Optuna optimization\n",
    "\n",
    "print(f\"Base models for stacking (Optuna-optimized): {len(stacking_models)}\")\n",
    "for name in stacking_models.keys():\n",
    "    print(f\"  ‚úÖ {name}\")\n",
    "\n",
    "print(f\"\\n2. IMPLEMENTING STACKED ENSEMBLE WITH OPTUNA MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use the same CV folds for all models to ensure consistency  \n",
    "stacking_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Level 1: Generate out-of-fold predictions from Optuna-optimized base models\n",
    "print(\"Generating Level 1 out-of-fold predictions with Optuna-optimized models...\")\n",
    "\n",
    "level1_oof_preds = np.zeros((len(X_full), len(stacking_models)))\n",
    "level1_test_preds = np.zeros((len(X_test_final), len(stacking_models)))\n",
    "\n",
    "model_names_stack = list(stacking_models.keys())\n",
    "\n",
    "for i, (name, model) in enumerate(stacking_models.items()):\n",
    "    print(f\"  Processing {name} (Optuna-enhanced)...\")\n",
    "    \n",
    "    # Generate OOF predictions\n",
    "    oof_pred = cross_val_predict(model, X_full, y_full, cv=stacking_cv, method='predict')\n",
    "    level1_oof_preds[:, i] = oof_pred\n",
    "    \n",
    "    # Train on full dataset and predict test set\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_full, y_full)\n",
    "    test_pred = model_clone.predict(X_test_final)\n",
    "    level1_test_preds[:, i] = test_pred\n",
    "    \n",
    "    # Calculate individual model OOF MAE\n",
    "    oof_mae = mean_absolute_error(y_full, oof_pred)\n",
    "    print(f\"    OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nLevel 1 OOF predictions shape: {level1_oof_preds.shape}\")\n",
    "print(f\"Level 1 test predictions shape: {level1_test_preds.shape}\")\n",
    "print(f\"üöÄ All base models use Optuna-optimized parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54168822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. TRAINING LEVEL 2 META-LEARNER (OPTUNA-ENHANCED)\n",
      "------------------------------------------------------------\n",
      "Evaluating Optuna-enhanced meta-learners:\n",
      "  üöÄ Ridge_meta_optuna: MAE = 2.7297 (¬±0.0622)\n",
      "  üìä Ridge_meta_conservative: MAE = 2.7328 (¬±0.0574)\n",
      "  üìä Ridge_meta_aggressive: MAE = 2.7289 (¬±0.0647)\n",
      "  üöÄ Lasso_meta_optuna: MAE = 2.7341 (¬±0.0576)\n",
      "  üöÄ Lasso_meta_optuna: MAE = 2.7341 (¬±0.0576)\n",
      "  üöÄ ElasticNet_meta_optuna: MAE = 2.7341 (¬±0.0580)\n",
      "  üìä Ridge_meta_standard: MAE = 2.7290 (¬±0.0640)\n",
      "  üìä LinearRegression_meta: MAE = 2.7289 (¬±0.0656)\n",
      "\n",
      "üèÜ Best meta-learner: Ridge_meta_aggressive\n",
      "Best meta-learner CV MAE: 2.7289\n",
      "Uses Optuna optimization: ‚ùå No\n",
      "\n",
      "4. TRAINING FINAL OPTUNA-ENHANCED STACKED MODEL\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced stacked ensemble test predictions:\n",
      "  Range: 45.87 to 108.86\n",
      "  Mean: 79.07\n",
      "  Std: 11.90\n",
      "\n",
      "5. COMPARISON WITH PHASE 1 OPTUNA ENSEMBLE\n",
      "------------------------------------------------------------\n",
      "Phase 1 Optuna ensemble predictions:\n",
      "  Range: 44.93 to 109.33\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Phase 2 Optuna stacked predictions:\n",
      "  Range: 45.87 to 108.86\n",
      "  Mean: 79.07\n",
      "  Std: 11.90\n",
      "\n",
      "Correlation between Phase 1 and Phase 2 Optuna ensembles: 0.9991\n",
      "\n",
      "Phase 2 (Optuna-enhanced stacked ensemble):\n",
      "  CV MAE: 2.7289\n",
      "  Expected improvement vs Phase 1: -0.0014\n",
      "  ‚ö†Ô∏è Phase 2 CV did not improve Phase 1\n",
      "  üìä Both benefit from Optuna optimization\n",
      "\n",
      "üéØ Both ensembles now use Optuna-optimized models!\n",
      "  üöÄ ElasticNet_meta_optuna: MAE = 2.7341 (¬±0.0580)\n",
      "  üìä Ridge_meta_standard: MAE = 2.7290 (¬±0.0640)\n",
      "  üìä LinearRegression_meta: MAE = 2.7289 (¬±0.0656)\n",
      "\n",
      "üèÜ Best meta-learner: Ridge_meta_aggressive\n",
      "Best meta-learner CV MAE: 2.7289\n",
      "Uses Optuna optimization: ‚ùå No\n",
      "\n",
      "4. TRAINING FINAL OPTUNA-ENHANCED STACKED MODEL\n",
      "------------------------------------------------------------\n",
      "Optuna-enhanced stacked ensemble test predictions:\n",
      "  Range: 45.87 to 108.86\n",
      "  Mean: 79.07\n",
      "  Std: 11.90\n",
      "\n",
      "5. COMPARISON WITH PHASE 1 OPTUNA ENSEMBLE\n",
      "------------------------------------------------------------\n",
      "Phase 1 Optuna ensemble predictions:\n",
      "  Range: 44.93 to 109.33\n",
      "  Mean: 79.08\n",
      "  Std: 12.05\n",
      "\n",
      "Phase 2 Optuna stacked predictions:\n",
      "  Range: 45.87 to 108.86\n",
      "  Mean: 79.07\n",
      "  Std: 11.90\n",
      "\n",
      "Correlation between Phase 1 and Phase 2 Optuna ensembles: 0.9991\n",
      "\n",
      "Phase 2 (Optuna-enhanced stacked ensemble):\n",
      "  CV MAE: 2.7289\n",
      "  Expected improvement vs Phase 1: -0.0014\n",
      "  ‚ö†Ô∏è Phase 2 CV did not improve Phase 1\n",
      "  üìä Both benefit from Optuna optimization\n",
      "\n",
      "üéØ Both ensembles now use Optuna-optimized models!\n"
     ]
    }
   ],
   "source": [
    "# Level 2: Train meta-learner with Optuna-enhanced parameters\n",
    "print(f\"\\n3. TRAINING LEVEL 2 META-LEARNER (OPTUNA-ENHANCED)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use Optuna-optimized parameters for meta-learners too\n",
    "meta_learners = {}\n",
    "\n",
    "# Create meta-learners using Optuna-optimized parameters when available\n",
    "if 'Ridge' in optimized_linear_params:\n",
    "    ridge_alpha = optimized_linear_params['Ridge'].get('alpha', 1.0)\n",
    "    meta_learners['Ridge_meta_optuna'] = Ridge(alpha=ridge_alpha)\n",
    "    meta_learners['Ridge_meta_conservative'] = Ridge(alpha=ridge_alpha * 10)  # More regularized\n",
    "    meta_learners['Ridge_meta_aggressive'] = Ridge(alpha=ridge_alpha * 0.1)   # Less regularized\n",
    "\n",
    "if 'Lasso' in optimized_linear_params:\n",
    "    lasso_alpha = optimized_linear_params['Lasso'].get('alpha', 0.01)\n",
    "    lasso_max_iter = optimized_linear_params['Lasso'].get('max_iter', 10000)\n",
    "    meta_learners['Lasso_meta_optuna'] = Lasso(alpha=lasso_alpha, max_iter=lasso_max_iter, tol=1e-3)\n",
    "\n",
    "if 'ElasticNet' in optimized_linear_params:\n",
    "    en_alpha = optimized_linear_params['ElasticNet'].get('alpha', 0.1)\n",
    "    en_l1_ratio = optimized_linear_params['ElasticNet'].get('l1_ratio', 0.5)\n",
    "    en_max_iter = optimized_linear_params['ElasticNet'].get('max_iter', 10000)\n",
    "    meta_learners['ElasticNet_meta_optuna'] = ElasticNet(\n",
    "        alpha=en_alpha, l1_ratio=en_l1_ratio, max_iter=en_max_iter, tol=1e-3\n",
    "    )\n",
    "\n",
    "# Add some standard options for comparison\n",
    "meta_learners['Ridge_meta_standard'] = Ridge(alpha=1.0)\n",
    "meta_learners['LinearRegression_meta'] = LinearRegression()\n",
    "\n",
    "best_meta_mae = float('inf')\n",
    "best_meta_name = None\n",
    "best_meta_model = None\n",
    "\n",
    "print(\"Evaluating Optuna-enhanced meta-learners:\")\n",
    "for name, meta_model in meta_learners.items():\n",
    "    # Cross-validate the meta-learner on OOF predictions\n",
    "    meta_cv_scores = cross_val_score(\n",
    "        meta_model, level1_oof_preds, y_full,\n",
    "        cv=5, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    meta_mae = -meta_cv_scores.mean()\n",
    "    meta_mae_std = meta_cv_scores.std()\n",
    "    \n",
    "    optuna_flag = \"üöÄ\" if \"optuna\" in name else \"üìä\"\n",
    "    print(f\"  {optuna_flag} {name}: MAE = {meta_mae:.4f} (¬±{meta_mae_std:.4f})\")\n",
    "    \n",
    "    if meta_mae < best_meta_mae:\n",
    "        best_meta_mae = meta_mae\n",
    "        best_meta_name = name\n",
    "        best_meta_model = meta_model\n",
    "\n",
    "print(f\"\\nüèÜ Best meta-learner: {best_meta_name}\")\n",
    "print(f\"Best meta-learner CV MAE: {best_meta_mae:.4f}\")\n",
    "is_optuna_meta = \"optuna\" in best_meta_name\n",
    "print(f\"Uses Optuna optimization: {'‚úÖ Yes' if is_optuna_meta else '‚ùå No'}\")\n",
    "\n",
    "# Train the best meta-learner on all OOF predictions\n",
    "print(f\"\\n4. TRAINING FINAL OPTUNA-ENHANCED STACKED MODEL\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "final_meta_model = clone(best_meta_model)\n",
    "final_meta_model.fit(level1_oof_preds, y_full)\n",
    "\n",
    "# Generate final stacked predictions\n",
    "stacked_test_pred = final_meta_model.predict(level1_test_preds)\n",
    "\n",
    "print(f\"Optuna-enhanced stacked ensemble test predictions:\")\n",
    "print(f\"  Range: {stacked_test_pred.min():.2f} to {stacked_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {stacked_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {stacked_test_pred.std():.2f}\")\n",
    "\n",
    "# Compare with Phase 1 Optuna ensemble\n",
    "print(f\"\\n5. COMPARISON WITH PHASE 1 OPTUNA ENSEMBLE\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Phase 1 Optuna ensemble predictions:\")\n",
    "print(f\"  Range: {ensemble_test_pred.min():.2f} to {ensemble_test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {ensemble_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {ensemble_test_pred.std():.2f}\")\n",
    "\n",
    "print(f\"\\nPhase 2 Optuna stacked predictions:\")\n",
    "print(f\"  Range: {stacked_test_pred.min():.2f} to {stacked_test_pred.max():.2f}\")  \n",
    "print(f\"  Mean: {stacked_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {stacked_test_pred.std():.2f}\")\n",
    "\n",
    "# Calculate correlation between Phase 1 and Phase 2 predictions\n",
    "correlation = np.corrcoef(ensemble_test_pred, stacked_test_pred)[0, 1]\n",
    "print(f\"\\nCorrelation between Phase 1 and Phase 2 Optuna ensembles: {correlation:.4f}\")\n",
    "\n",
    "print(f\"\\nPhase 2 (Optuna-enhanced stacked ensemble):\")\n",
    "print(f\"  CV MAE: {best_meta_mae:.4f}\")\n",
    "improvement_vs_phase1 = optimal_mae - best_meta_mae\n",
    "print(f\"  Expected improvement vs Phase 1: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "if best_meta_mae < optimal_mae:\n",
    "    print(f\"  ‚úÖ Phase 2 shows improvement over Phase 1!\")\n",
    "    print(f\"  üöÄ Optuna optimization helped both phases!\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Phase 2 CV did not improve Phase 1\")\n",
    "    print(f\"  üìä Both benefit from Optuna optimization\")\n",
    "    \n",
    "print(f\"\\nüéØ Both ensembles now use Optuna-optimized models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb2464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. CREATING OPTUNA-ENHANCED STACKED ENSEMBLE SUBMISSION\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Optuna-enhanced stacked ensemble submission saved: submission_optuna_stacked_ensemble_20251005_154449.csv\n",
      "üìÅ Path: /home/chrisfkh/sctp-ds-ai/mod3/kaggle_moneyball/submissions/submission_optuna_stacked_ensemble_20251005_154449.csv\n",
      "üìä Predictions shape: (453, 2)\n",
      "\n",
      "First 10 predictions:\n",
      "     ID          W\n",
      "0  1756  69.620038\n",
      "1  1282  74.213446\n",
      "2   351  83.893124\n",
      "3   421  86.209213\n",
      "4    57  92.357637\n",
      "5  1557  97.794090\n",
      "6   846  79.108836\n",
      "7  1658  83.538823\n",
      "8   112  73.248867\n",
      "9  2075  83.550110\n",
      "\n",
      "7. OPTUNA-ENHANCED STACKED ENSEMBLE SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "Base Models (all Optuna-optimized): 11\n",
      "  üöÄ Ridge_optuna\n",
      "  üìä Ridge_conservative\n",
      "  üìä Ridge_aggressive\n",
      "  üöÄ Lasso_optuna\n",
      "  üìä Lasso_variation\n",
      "  üöÄ ElasticNet_optuna\n",
      "  üöÄ XGBoost_optuna\n",
      "  üìä XGBoost_conservative\n",
      "  üöÄ CatBoost_optuna\n",
      "  üöÄ Best_Linear\n",
      "  üöÄ Best_Boosting\n",
      "\n",
      "Meta-learner: Ridge_meta_aggressive\n",
      "Meta-learner status: üìä Standard parameters\n",
      "\n",
      "Expected Performance:\n",
      "  CV MAE: 2.7289\n",
      "  Expected Kaggle improvement vs Phase 1: -0.0014\n",
      "\n",
      "üéØ PHASE COMPARISON SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "Phase 1 (Weighted): MAE = 2.7275 | Correlation: 0.999\n",
      "Phase 2 (Stacked):  MAE = 2.7289 | Improvement: -0.0014\n",
      "üèÜ Current leader: Phase 1 (Weighted)\n",
      "\n",
      "‚ú® Both phases now leverage full Optuna optimization!\n",
      "üöÄ Ready to submit the best performing ensemble to Kaggle!\n",
      "\n",
      "üìà OPTIMIZATION JOURNEY\n",
      "----------------------------------------------------------------------\n",
      "1. ‚úÖ Boosting models optimized with Optuna\n",
      "2. ‚úÖ Linear models optimized with Optuna\n",
      "3. ‚úÖ Phase 1 ensemble uses Optuna models\n",
      "4. ‚úÖ Phase 2 ensemble uses Optuna models + meta-learner\n",
      "5. üéØ Ready for Kaggle submission with optimized ensembles!\n"
     ]
    }
   ],
   "source": [
    "# Create Optuna-enhanced stacked ensemble submission\n",
    "print(f\"\\n6. CREATING OPTUNA-ENHANCED STACKED ENSEMBLE SUBMISSION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create submission DataFrame for Optuna-enhanced stacked ensemble\n",
    "stacked_submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'W': stacked_test_pred\n",
    "})\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp_stacked = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "stacked_submission_filename = f\"submission_optuna_stacked_ensemble_{timestamp_stacked}.csv\"\n",
    "stacked_submission_path = SUB_DIR / stacked_submission_filename\n",
    "\n",
    "# Save submission\n",
    "stacked_submission_df.to_csv(stacked_submission_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Optuna-enhanced stacked ensemble submission saved: {stacked_submission_filename}\")\n",
    "print(f\"üìÅ Path: {stacked_submission_path}\")\n",
    "print(f\"üìä Predictions shape: {stacked_submission_df.shape}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(stacked_submission_df.head(10))\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(f\"\\n7. OPTUNA-ENHANCED STACKED ENSEMBLE SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Base Models (all Optuna-optimized): {len(stacking_models)}\")\n",
    "for name in model_names_stack:\n",
    "    optuna_flag = \"üöÄ\" if any(x in name.lower() for x in ['optuna', 'best']) else \"üìä\"\n",
    "    print(f\"  {optuna_flag} {name}\")\n",
    "\n",
    "print(f\"\\nMeta-learner: {best_meta_name}\")\n",
    "meta_optuna_status = \"üöÄ Uses Optuna optimization\" if \"optuna\" in best_meta_name else \"üìä Standard parameters\"\n",
    "print(f\"Meta-learner status: {meta_optuna_status}\")\n",
    "\n",
    "print(f\"\\nExpected Performance:\")\n",
    "print(f\"  CV MAE: {best_meta_mae:.4f}\")\n",
    "print(f\"  Expected Kaggle improvement vs Phase 1: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ PHASE COMPARISON SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Phase 1 (Weighted): MAE = {optimal_mae:.4f} | Correlation: {correlation:.3f}\")\n",
    "print(f\"Phase 2 (Stacked):  MAE = {best_meta_mae:.4f} | Improvement: {improvement_vs_phase1:.4f}\")\n",
    "\n",
    "winner = \"Phase 2 (Stacked)\" if best_meta_mae < optimal_mae else \"Phase 1 (Weighted)\"\n",
    "print(f\"üèÜ Current leader: {winner}\")\n",
    "\n",
    "print(f\"\\n‚ú® Both phases now leverage full Optuna optimization!\")\n",
    "print(f\"üöÄ Ready to submit the best performing ensemble to Kaggle!\")\n",
    "\n",
    "# Show the optimization journey\n",
    "print(f\"\\nüìà OPTIMIZATION JOURNEY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"1. ‚úÖ Boosting models optimized with Optuna\")\n",
    "print(f\"2. ‚úÖ Linear models optimized with Optuna\")  \n",
    "print(f\"3. ‚úÖ Phase 1 ensemble uses Optuna models\")\n",
    "print(f\"4. ‚úÖ Phase 2 ensemble uses Optuna models + meta-learner\")\n",
    "print(f\"5. üéØ Ready for Kaggle submission with optimized ensembles!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
